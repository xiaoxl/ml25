<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.10">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Intro to Pytorch – Machine Learning Fall 2024</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/6/intro.html" rel="next">
<link href="../../contents/4/intro.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2d9718c933debafcce942f9b212640bc.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c2be7f5231965d65f9256c776f13bc01.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Fall 2024</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/5/intro.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Intro to Pytorch</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/1/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/2/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">k-Nearest Neighbors algorithm (k-NN)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/3/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/4/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ensemble methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/5/intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Intro to Pytorch</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/6/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/7/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Netural networks</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-linearregression_math" id="toc-sec-linearregression_math" class="nav-link active" data-scroll-target="#sec-linearregression_math"><span class="header-section-number">5.1</span> Linear regression (math)</a>
  <ul class="collapse">
  <li><a href="#parameter-space" id="toc-parameter-space" class="nav-link" data-scroll-target="#parameter-space"><span class="header-section-number">5.1.1</span> Parameter space</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="header-section-number">5.1.2</span> Loss function</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="header-section-number">5.1.3</span> Gradient Descent</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">5.1.4</span> Summary</a></li>
  </ul></li>
  <li><a href="#linear-regression-numpy" id="toc-linear-regression-numpy" class="nav-link" data-scroll-target="#linear-regression-numpy"><span class="header-section-number">5.2</span> Linear regression (<code>numpy</code>)</a>
  <ul class="collapse">
  <li><a href="#prepare-the-dataset" id="toc-prepare-the-dataset" class="nav-link" data-scroll-target="#prepare-the-dataset"><span class="header-section-number">5.2.1</span> Prepare the dataset</a></li>
  <li><a href="#compute-gradient" id="toc-compute-gradient" class="nav-link" data-scroll-target="#compute-gradient"><span class="header-section-number">5.2.2</span> Compute gradient</a></li>
  <li><a href="#sec-gradeientdescent_numpy_example" id="toc-sec-gradeientdescent_numpy_example" class="nav-link" data-scroll-target="#sec-gradeientdescent_numpy_example"><span class="header-section-number">5.2.3</span> Gradient descent</a></li>
  <li><a href="#sec-minibatch_numpy" id="toc-sec-minibatch_numpy" class="nav-link" data-scroll-target="#sec-minibatch_numpy"><span class="header-section-number">5.2.4</span> Mini-batch and optimizers</a></li>
  </ul></li>
  <li><a href="#linear-regression-pytorch" id="toc-linear-regression-pytorch" class="nav-link" data-scroll-target="#linear-regression-pytorch"><span class="header-section-number">5.3</span> Linear regression (<code>PyTorch</code>)</a>
  <ul class="collapse">
  <li><a href="#construct-torch.tensor" id="toc-construct-torch.tensor" class="nav-link" data-scroll-target="#construct-torch.tensor"><span class="header-section-number">5.3.1</span> Construct <code>torch.Tensor</code></a></li>
  <li><a href="#devices" id="toc-devices" class="nav-link" data-scroll-target="#devices"><span class="header-section-number">5.3.2</span> devices</a></li>
  <li><a href="#sec-gradientdescent_pytorch_example" id="toc-sec-gradientdescent_pytorch_example" class="nav-link" data-scroll-target="#sec-gradientdescent_pytorch_example"><span class="header-section-number">5.3.3</span> Gradient</a></li>
  <li><a href="#optimizers" id="toc-optimizers" class="nav-link" data-scroll-target="#optimizers"><span class="header-section-number">5.3.4</span> Optimizers</a></li>
  <li><a href="#use-class-to-describe-the-model" id="toc-use-class-to-describe-the-model" class="nav-link" data-scroll-target="#use-class-to-describe-the-model"><span class="header-section-number">5.3.5</span> Use class to describe the model</a></li>
  <li><a href="#using-standard-modules" id="toc-using-standard-modules" class="nav-link" data-scroll-target="#using-standard-modules"><span class="header-section-number">5.3.6</span> Using standard modules</a></li>
  </ul></li>
  <li><a href="#sec-dataloader" id="toc-sec-dataloader" class="nav-link" data-scroll-target="#sec-dataloader"><span class="header-section-number">5.4</span> Dataloader</a>
  <ul class="collapse">
  <li><a href="#convert-the-previous-dataset-using-dataloader" id="toc-convert-the-previous-dataset-using-dataloader" class="nav-link" data-scroll-target="#convert-the-previous-dataset-using-dataloader"><span class="header-section-number">5.4.1</span> Convert the previous dataset using DataLoader</a></li>
  <li><a href="#rewrite-using-random_split" id="toc-rewrite-using-random_split" class="nav-link" data-scroll-target="#rewrite-using-random_split"><span class="header-section-number">5.4.2</span> Rewrite using <code>random_split</code></a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.5</span> Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Intro to Pytorch</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Most materials are based on <span class="citation" data-cites="Godoy2022">[<a href="../../index.html#ref-Godoy2022" role="doc-biblioref">1</a>]</span>.</p>
<section id="sec-linearregression_math" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-linearregression_math"><span class="header-section-number">5.1</span> Linear regression (math)</h2>
<div class="hidden">
<!-- Constants and basic symbols -->
<p><span class="math display">\[
\require{physics}
\require{braket}
\]</span></p>
<p><span class="math display">\[
\newcommand{\dl}[1]{{\hspace{#1mu}\mathrm d}}
\newcommand{\me}{{\mathrm e}}
\]</span></p>
<!-- Probability -->
<p><span class="math display">\[
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Mode}{\operatorname{mode}}
\]</span></p>
<!-- Distributions pdf -->
<p><span class="math display">\[
\newcommand{\pdfbinom}{{\tt binom}}
\newcommand{\pdfbeta}{{\tt beta}}
\newcommand{\pdfpois}{{\tt poisson}}
\newcommand{\pdfgamma}{{\tt gamma}}
\newcommand{\pdfnormal}{{\tt norm}}
  \newcommand{\pdfexp}{{\tt expon}}
\]</span></p>
<!-- Distributions -->
<p><span class="math display">\[
\newcommand{\distbinom}{\operatorname{B}}
\newcommand{\distbeta}{\operatorname{Beta}}
\newcommand{\distgamma}{\operatorname{Gamma}}
\newcommand{\distexp}{\operatorname{Exp}}
\newcommand{\distpois}{\operatorname{Poisson}}
\newcommand{\distnormal}{\operatorname{\mathcal N}}
\]</span></p>
</div>
<p>We only consider the simplest case: simple linear regression (SLR). The idea is very simple. The dataset contains two variables (the independent variable <span class="math inline">\(x\)</span> and the response variable <span class="math inline">\(y\)</span>.) The goal is to find the relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with the given dataset. We assume their relation is <span class="math inline">\(y=b+wx\)</span>. How do we find <span class="math inline">\(b\)</span> and <span class="math inline">\(w\)</span>?</p>
<p>Let us first see an example. We would like to find the red line (which is the best fitted curve) shown below.</p>
<div id="23c966a7" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-2-output-1.png" width="573" height="421" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="parameter-space" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="parameter-space"><span class="header-section-number">5.1.1</span> Parameter space</h3>
<p>The key here is to understand the idea of “parameter space”. Since we already know that the function we are looking for has a formula <span class="math inline">\(y=b+wx\)</span>, we could use the pair <span class="math inline">\((b, w)\)</span> to denote different candidates of our answer. For example, the following plot show some possibilities in green dashed lines, while each possiblity is denoted by <span class="math inline">\((b, w)\)</span>. Then the problem is reworded as to find the best pair <span class="math inline">\((b, w)\)</span>.</p>
<div id="52e5d591" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-3-output-1.png" width="573" height="421" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="loss-function" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">5.1.2</span> Loss function</h3>
<p>The “best” is defined in the following way. The dataset is given <span class="math inline">\(\{(x_i, y_i)\}\)</span>. If we choose a pair of parameters <span class="math inline">\((b,w)\)</span>, we will have an estimated regression line, as well as a set of estimated <span class="math inline">\(\hat{y_i}\)</span>. The idea is to let the difference between <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hat{y_i}\)</span> is as small as possible. In other words, a <strong>loss function</strong> <span class="math inline">\(J\)</span> is defined as follows:</p>
<p><span id="eq-cost_lr"><span class="math display">\[
J_{\{(x_i,y_i)\}}(b,w)=\frac1N\sum_{i=1}^N(y_i-\hat{y_i})^2=\frac1N\sum_{i=1}^N(y_i-b-wx_i)^2
\tag{5.1}\]</span></span> and we are expected to find the <span class="math inline">\((b,w)\)</span> such that the loss function is minimized. The contour map of <span class="math inline">\(J\)</span> is shown below.</p>
<div id="61f173bc" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-4-output-1.png" width="574" height="441" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gradient-descent" class="level3" data-number="5.1.3">
<h3 data-number="5.1.3" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">5.1.3</span> Gradient Descent</h3>
<p>We use a technique called “gradient descent” to find the global minimal of <span class="math inline">\(J\)</span>. We start from a random point. For example <span class="math inline">\((1.0, 1.5)\)</span>. Then we find a direction where the cost <span class="math inline">\(J\)</span> reduces the most, and move in that direction. This direction is computed by the gradient of the cost <span class="math inline">\(J\)</span>, and this is the reason why the algorithm is called “gradient descent”. After we get to a new point, we evaluate the new gradient and move in the new direction. The process is repeated and we are expected to get close to the minimal point after several iterations. Just like shown in the following plot.</p>
<div id="71872cd7" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-5-output-1.png" width="574" height="441" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The parameter updating rule is shown below. The <span class="math inline">\(\eta\)</span> is called the <strong>learning rate</strong>. It is a hyperparameter that is used to control the learning process.</p>
<p><span id="eq-gd_updating"><span class="math display">\[
\begin{aligned}
&amp;\pdv{J}{b}=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-1),\quad &amp;b_{new} = b_{old}-\eta*\pdv{J}{b},\\
&amp;\pdv{J}{w}=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-x_i),\quad &amp;w_{new} = w_{old}-\eta*\pdv{J}{w},
\end{aligned}
\tag{5.2}\]</span></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning rate <span class="math inline">\(\eta\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Generally speaking, larger <span class="math inline">\(\eta\)</span> will move faster to the global minimal, but might be jumpy which cause it harder to converge. On the other side, smaller <span class="math inline">\(\eta\)</span> moves in a more stable fashion, but may take a long time to converge. See the following examples.</p>
<div id="6f6bef97" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-6-output-1.png" width="577" height="436" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In the first example, <span class="math inline">\(\eta\)</span> is too small, that after 200 iterations it is not very close to the minimal. In the second example, <span class="math inline">\(\eta\)</span> becomes large. Although it gets to somewhere near the minimal, the path is very jumpy. It is able to converge only because the problem is indeed an easy one.</p>
</div>
</div>
</div>
<p>We may record the curve of the cost function.</p>
<div id="49c7bbd7" class="cell" data-execution_count="6">
<div class="cell-output cell-output-stdout">
<pre><code>After 200 iterations, the parameters are (2.291241352364798, 1.203587494484257).</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-7-output-2.png" width="593" height="436" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The cost is close to <span class="math inline">\(0\)</span> after 200 iterations and seems to be convergent. Therefore we believe that we are close to the minimal point. The point we get is (2.291241352364798, 1.203587494484257).</p>
</section>
<section id="summary" class="level3" data-number="5.1.4">
<h3 data-number="5.1.4" class="anchored" data-anchor-id="summary"><span class="header-section-number">5.1.4</span> Summary</h3>
<p>Let us summarize the example above and generalize it to the general case.</p>
<ol type="1">
<li>Let <span class="math inline">\(\{(X_i, y_i)\}\)</span> be a given dataset. Assume that <span class="math inline">\(y=f_{\Theta}(X)\)</span> where <span class="math inline">\(\Theta\)</span> is the set of all parameters.</li>
<li>The cost function <span class="math inline">\(J_{\Theta, \{(X_i, y_i)\}}\)</span> is defined.</li>
<li>To find the minimal point of the cost function, the gradient descent is applied:
<ul>
<li>Start from a random initial point <span class="math inline">\(\theta_0\)</span>.</li>
<li>Compute the gradient <span class="math inline">\(\nabla J\)</span> and update <span class="math inline">\(\theta_i=\theta_{i-1}- \eta \nabla J\)</span> and repeat the process multiple times.</li>
<li>Draw the learning curve and determine when to stop. Then we get the estimated best parameters <span class="math inline">\(\hat{\Theta}\)</span>.</li>
</ul></li>
<li>Our model under this setting is sovled. We then turn to evaluation phase.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The above process can be further developped. We will discuss many of them in later sections.</p>
<ol type="1">
<li>The cost function is related to each concerte problem.</li>
<li>To compute the gradient of the cost function, chain rule is usually used. In the setting of MLP which we will discuss later, the gradient computations with chain rule are summarized as the so-called <strong>Back propagation</strong>.</li>
<li>We go through the data points to compute the graident. How many points do we use? What is the frenqucy to update the gradient? This belongs to the topic of <strong>mini-batch</strong>.</li>
<li>Even when we know that the graident gives the best direction, sometimes we don’t really want to go in that direction, but make some modifications for some reason. To modify the direction, as well as choosing the learning rate <span class="math inline">\(\eta\)</span>, is the subject of <strong>optimizers</strong>.</li>
</ol>
</div>
</div>
</section>
</section>
<section id="linear-regression-numpy" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="linear-regression-numpy"><span class="header-section-number">5.2</span> Linear regression (<code>numpy</code>)</h2>
<div class="hidden">
<!-- Constants and basic symbols -->
<p><span class="math display">\[
\require{physics}
\require{braket}
\]</span></p>
<p><span class="math display">\[
\newcommand{\dl}[1]{{\hspace{#1mu}\mathrm d}}
\newcommand{\me}{{\mathrm e}}
\]</span></p>
<!-- Probability -->
<p><span class="math display">\[
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Mode}{\operatorname{mode}}
\]</span></p>
<!-- Distributions pdf -->
<p><span class="math display">\[
\newcommand{\pdfbinom}{{\tt binom}}
\newcommand{\pdfbeta}{{\tt beta}}
\newcommand{\pdfpois}{{\tt poisson}}
\newcommand{\pdfgamma}{{\tt gamma}}
\newcommand{\pdfnormal}{{\tt norm}}
  \newcommand{\pdfexp}{{\tt expon}}
\]</span></p>
<!-- Distributions -->
<p><span class="math display">\[
\newcommand{\distbinom}{\operatorname{B}}
\newcommand{\distbeta}{\operatorname{Beta}}
\newcommand{\distgamma}{\operatorname{Gamma}}
\newcommand{\distexp}{\operatorname{Exp}}
\newcommand{\distpois}{\operatorname{Poisson}}
\newcommand{\distnormal}{\operatorname{\mathcal N}}
\]</span></p>
</div>
<p>We will translate everything from the previous sections into codes.</p>
<section id="prepare-the-dataset" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="prepare-the-dataset"><span class="header-section-number">5.2.1</span> Prepare the dataset</h3>
<p>We first randomly generate a dataset <code>(X, y)</code> for the linear regression problem.</p>
<div id="c5d07aed" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a>y <span class="op">=</span> <span class="fl">2.3</span> <span class="op">+</span> <span class="fl">1.2</span> <span class="op">*</span> X <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We set the seed to be 42 for reproducing the results. We will also split the dataset into training and test sets.</p>
<div id="ce6a20f0" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">.15</span>,</span>
<span id="cb3-4"><a href="#cb3-4"></a>                                                    random_state<span class="op">=</span><span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will only focus only on the training set in this Chapter.</p>
</section>
<section id="compute-gradient" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="compute-gradient"><span class="header-section-number">5.2.2</span> Compute gradient</h3>
<p>Recall <a href="#eq-cost_lr" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> and <a href="#eq-gd_updating" class="quarto-xref">Equation&nbsp;<span>5.2</span></a></p>
<p><span class="math display">\[
\begin{aligned}
J(b,w)&amp;=\frac1N\sum_{i=1}^N(y_i-b-wx_i)^2,\\
\pdv{J}{b}&amp;=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-1),\\
\pdv{J}{w}&amp;=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-x_i).
\end{aligned}
\]</span></p>
<div id="4da41db9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> J(parameters, X, y):    </span>
<span id="cb4-2"><a href="#cb4-2"></a>    b <span class="op">=</span> parameters[<span class="dv">0</span>]</span>
<span id="cb4-3"><a href="#cb4-3"></a>    w <span class="op">=</span> parameters[<span class="dv">1</span>]</span>
<span id="cb4-4"><a href="#cb4-4"></a>    <span class="cf">return</span> ((y<span class="op">-</span>b<span class="op">-</span>w<span class="op">*</span>X)<span class="op">**</span><span class="dv">2</span>).mean().item()</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="kw">def</span> dJ(parameters, X, y):</span>
<span id="cb4-7"><a href="#cb4-7"></a>    b <span class="op">=</span> parameters[<span class="dv">0</span>]</span>
<span id="cb4-8"><a href="#cb4-8"></a>    w <span class="op">=</span> parameters[<span class="dv">1</span>]</span>
<span id="cb4-9"><a href="#cb4-9"></a>    db <span class="op">=</span> (<span class="dv">2</span><span class="op">*</span>(y<span class="op">-</span>b<span class="op">-</span>w<span class="op">*</span>X)<span class="op">*</span>(<span class="op">-</span><span class="dv">1</span>)).mean()</span>
<span id="cb4-10"><a href="#cb4-10"></a>    dw <span class="op">=</span> (<span class="dv">2</span><span class="op">*</span>(y<span class="op">-</span>b<span class="op">-</span>w<span class="op">*</span>X)<span class="op">*</span>(<span class="op">-</span>X)).mean()</span>
<span id="cb4-11"><a href="#cb4-11"></a>    <span class="cf">return</span> np.array([db, dw])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-gradeientdescent_numpy_example" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="sec-gradeientdescent_numpy_example"><span class="header-section-number">5.2.3</span> Gradient descent</h3>
<p>In general we need to random select a starting point. Here for the purpose of comparing to what we get from previous section, we will use a manual selected starting point <span class="math inline">\((1, 1.5)\)</span>. We then follow the path and move for a few steps. Here we will use <span class="math inline">\(\eta=0.2\)</span> as the learning rate.</p>
<div id="61a56d41" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>p <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">1.5</span>])</span>
<span id="cb5-2"><a href="#cb5-2"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a>plist <span class="op">=</span> []</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb5-6"><a href="#cb5-6"></a>    J_i <span class="op">=</span> J(p, X_train, y_train)</span>
<span id="cb5-7"><a href="#cb5-7"></a>    dJ_i <span class="op">=</span> dJ(p, X_train, y_train)</span>
<span id="cb5-8"><a href="#cb5-8"></a>    p <span class="op">=</span> p <span class="op">-</span> lr <span class="op">*</span> dJ_i</span>
<span id="cb5-9"><a href="#cb5-9"></a>    plist.append([p[<span class="dv">0</span>], p[<span class="dv">1</span>]])</span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>[[1.462128976503838, 1.70260448274778],
 [1.701791352506138, 1.7949449226054148],
 [1.8284450974134463, 1.8316393022111805],
 [1.8976247633456744, 1.840388291132826],
 [1.937508248869905, 1.8352370440485253],
 [1.96239470703006, 1.8233031967656035],
 [1.9795421883017092, 1.8081901205764057],
 [1.9926365306310478, 1.7917185352872653],
 [2.0035512067846226, 1.7748049887103947],
 [2.013240136591026, 1.7579075228763736]]</code></pre>
</div>
</div>
<p>You may compare the answer with the <code>PyTorch</code> implementation in <a href="#sec-gradientdescent_pytorch_example" class="quarto-xref"><span>Section 5.3.3</span></a>.</p>
</section>
<section id="sec-minibatch_numpy" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="sec-minibatch_numpy"><span class="header-section-number">5.2.4</span> Mini-batch and optimizers</h3>
<p>Review the gradient formula <a href="#eq-gd_updating" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>, the gradient is computed by looking at each given data point and putting the results together. Therefore it is possible to get the partial information of the gradient by just looking at part of the data. In other words, the updating process can be modify in the following way: divide the original dataset into several groups, run through each group to compute the gradient with the data in only one group and then update the parameters. In general there are three types:</p>
<ul>
<li>There is only 1 group: we update the parameters only once when we finish looking at all data points. This is the way we mentioned previously. It is called <strong>batch gradient descent</strong>.</li>
<li>Every single point forms a group: we update the parameters eachtime we look at one data point. This method is called <strong>stocastic gradient descent</strong> (SGD). Since we compute the gradient with only one data point, it is expected that the direction is far from perfect, and the descent process is expected to be more “random”.</li>
<li>Multiple groups of the same size are formed, with a reasonable group size and group number. This is called <strong>mini-batch gradient descent</strong>. It is the middle point between the above two methods. The <strong>batch size</strong>, which is the size of each group, is a very important hyperparameter for trainning.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Epochs
</div>
</div>
<div class="callout-body-container callout-body">
<p>One <strong>epoch</strong> is the process that you see each data point exactly once, no matter what the batch size is.</p>
</div>
</div>
<p>Usually batch gradient descent is expected to have a more smooth trajection but move slowly, while SGD is expected to move faster to the minimal point but may never really get to it since the trajection is too jumpy. Mini-batch is meant to strike a balanced point by finding a good batch size. In the example below, we show the mini-batch gradient descent in the first 10 epochs.</p>
<div id="15337ed2" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>p <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">1.5</span>])</span>
<span id="cb7-2"><a href="#cb7-2"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>batchsize <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>RANDOMSEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb7-5"><a href="#cb7-5"></a></span>
<span id="cb7-6"><a href="#cb7-6"></a>N <span class="op">=</span> X_train.shape[<span class="dv">0</span>]</span>
<span id="cb7-7"><a href="#cb7-7"></a>indx <span class="op">=</span> np.arange(N)</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>np.random.seed(RANDOMSEED)</span>
<span id="cb7-10"><a href="#cb7-10"></a>np.random.shuffle(indx)</span>
<span id="cb7-11"><a href="#cb7-11"></a>batches <span class="op">=</span> []</span>
<span id="cb7-12"><a href="#cb7-12"></a></span>
<span id="cb7-13"><a href="#cb7-13"></a>batch_num <span class="op">=</span> <span class="bu">int</span>(np.ceil(N <span class="op">/</span> batchsize))</span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_num):</span>
<span id="cb7-15"><a href="#cb7-15"></a>    last <span class="op">=</span> np.minimum((i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>batchsize, N)</span>
<span id="cb7-16"><a href="#cb7-16"></a>    batches.append(indx[i<span class="op">*</span>batchsize: last])</span>
<span id="cb7-17"><a href="#cb7-17"></a></span>
<span id="cb7-18"><a href="#cb7-18"></a>plist <span class="op">=</span> []</span>
<span id="cb7-19"><a href="#cb7-19"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb7-20"><a href="#cb7-20"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_num):</span>
<span id="cb7-21"><a href="#cb7-21"></a>        dJ_i <span class="op">=</span> dJ(p, X_train[batches[i]], y_train[batches[i]])</span>
<span id="cb7-22"><a href="#cb7-22"></a>        p <span class="op">=</span> p <span class="op">-</span> lr <span class="op">*</span> dJ_i</span>
<span id="cb7-23"><a href="#cb7-23"></a>    plist.append([p[<span class="dv">0</span>], p[<span class="dv">1</span>]])</span>
<span id="cb7-24"><a href="#cb7-24"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>[[1.8396487079358765, 1.8350571390420554],
 [1.9759220547433842, 1.826737782139383],
 [2.0168483670845423, 1.7772907765564647],
 [2.043224019190362, 1.726425817587749],
 [2.065921614604292, 1.6790714304374827],
 [2.086528794877608, 1.635572387082582],
 [2.1053898608038017, 1.595691439736766],
 [2.1226730826892424, 1.559137786653576],
 [2.1385131544625695, 1.5256351733493791],
 [2.1530309370105214, 1.494929115540467]]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Non-shuffle version
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here is the result for the non-shuffle version. You could compare the results with what we do later.</p>
<div id="9a3ecb35" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>p <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">1.5</span>])</span>
<span id="cb9-2"><a href="#cb9-2"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>batchsize <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a>N <span class="op">=</span> X_train.shape[<span class="dv">0</span>]</span>
<span id="cb9-6"><a href="#cb9-6"></a>indx <span class="op">=</span> np.arange(N)</span>
<span id="cb9-7"><a href="#cb9-7"></a></span>
<span id="cb9-8"><a href="#cb9-8"></a>batches <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9"></a>batch_num <span class="op">=</span> <span class="bu">int</span>(np.ceil(N <span class="op">/</span> batchsize))</span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_num):</span>
<span id="cb9-11"><a href="#cb9-11"></a>    last <span class="op">=</span> np.minimum((i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>batchsize, N)</span>
<span id="cb9-12"><a href="#cb9-12"></a>    batches.append(indx[i<span class="op">*</span>batchsize: last])</span>
<span id="cb9-13"><a href="#cb9-13"></a></span>
<span id="cb9-14"><a href="#cb9-14"></a>plist <span class="op">=</span> []</span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb9-16"><a href="#cb9-16"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_num):</span>
<span id="cb9-17"><a href="#cb9-17"></a>        dJ_i <span class="op">=</span> dJ(p, X_train[batches[i]], y_train[batches[i]])</span>
<span id="cb9-18"><a href="#cb9-18"></a>        p <span class="op">=</span> p <span class="op">-</span> lr <span class="op">*</span> dJ_i</span>
<span id="cb9-19"><a href="#cb9-19"></a>    plist.append([p[<span class="dv">0</span>], p[<span class="dv">1</span>]])</span>
<span id="cb9-20"><a href="#cb9-20"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>[[1.8277028504755573, 1.8368193572906044],
 [1.9607104449826838, 1.8293981130981023],
 [2.001626059409397, 1.7815077539441087],
 [2.0286935704191, 1.7321715193480243],
 [2.0522055690695757, 1.686138097785071],
 [2.0736381185747943, 1.6437403254745735],
 [2.0933134526016604, 1.6047617600958677],
 [2.111393711486754, 1.5689357968513453],
 [2.1280105514943686, 1.5360086358936902],
 [2.143282725696795, 1.505745878510758]]</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="linear-regression-pytorch" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="linear-regression-pytorch"><span class="header-section-number">5.3</span> Linear regression (<code>PyTorch</code>)</h2>
<section id="construct-torch.tensor" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="construct-torch.tensor"><span class="header-section-number">5.3.1</span> Construct <code>torch.Tensor</code></h3>
<p>There are multiple ways to construct a tensor. I just discuss those confusing ones.</p>
<ul>
<li><code>torch.Tensor</code> is the <code>PyTorch</code> tensor data structure. Itself serves as the constructor of the class, therefore you may use <code>torch.Tensor(data)</code> to construct a tensor. This is relative basic, and will have a default <code>float</code> type.</li>
<li><code>torch.tensor</code> is the recommendated function to construct a tensor from data. It has two benefits over <code>torch.Tensor</code>: it will automatically induce the datatype from data instead of always using <code>float</code>; and it is easier to change datatype with the argument <code>dtype</code>.</li>
<li><code>torch.as_tensor</code> is a function to construct a tensor from data. If the original data is numpy array, this tensor shares data with it. This means that if one is changed, the other is changed as well.</li>
</ul>
<div id="08a13f30" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">import</span> torch</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a>example <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb11-5"><a href="#cb11-5"></a>example_tensor0 <span class="op">=</span> torch.Tensor(example)</span>
<span id="cb11-6"><a href="#cb11-6"></a>example_tensor1 <span class="op">=</span> torch.tensor(example)</span>
<span id="cb11-7"><a href="#cb11-7"></a>example_tensor2 <span class="op">=</span> torch.as_tensor(example)</span>
<span id="cb11-8"><a href="#cb11-8"></a></span>
<span id="cb11-9"><a href="#cb11-9"></a><span class="bu">print</span>(<span class="ss">f'Tensor: dtype: </span><span class="sc">{</span>example_tensor0<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">, tensor: dtype: </span><span class="sc">{</span>example_tensor1<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-10"><a href="#cb11-10"></a></span>
<span id="cb11-11"><a href="#cb11-11"></a><span class="bu">print</span>(<span class="ss">f'tensor: </span><span class="sc">{</span>example_tensor1<span class="sc">}</span><span class="ss">, as_tensor: </span><span class="sc">{</span>example_tensor2<span class="sc">}</span><span class="ss">, original: </span><span class="sc">{</span>example<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-12"><a href="#cb11-12"></a></span>
<span id="cb11-13"><a href="#cb11-13"></a>example[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-14"><a href="#cb11-14"></a><span class="bu">print</span>(<span class="ss">f'tensor: </span><span class="sc">{</span>example_tensor1<span class="sc">}</span><span class="ss">, as_tensor: </span><span class="sc">{</span>example_tensor2<span class="sc">}</span><span class="ss">, original: </span><span class="sc">{</span>example<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Tensor: dtype: torch.float32, tensor: dtype: torch.int32
tensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([1, 2], dtype=torch.int32), original: [1 2]
tensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([0, 2], dtype=torch.int32), original: [0 2]</code></pre>
</div>
</div>
<p>In general, it is recommended to use <code>torch.as_tensor</code> over <code>torch.tensor</code> (since for large data to create a view is much faster than to create a copy) and to use <code>torch.tensor</code> over <code>torch.Tensor</code> (due to the benefits mentioned above).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Scalar
</div>
</div>
<div class="callout-body-container callout-body">
<p>A tensor with only one element is still a tensor in <code>PyTorch</code>. To use it as a scalar, you need to use <code>itme()</code> method.</p>
<div id="d265e5f5" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>a <span class="op">=</span> torch.tensor(<span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor(1)</code></pre>
</div>
</div>
<div id="520ccd2a" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>a.item()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>1</code></pre>
</div>
</div>
<p>Note that for <code>numpy</code>, before 2.0 version an array with one element is considered as scalar. However after 2.0, it behaves very similar to <code>PyTorch</code>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
datatype
</div>
</div>
<div class="callout-body-container callout-body">
<p>The datatype in <code>PyTorch</code> is very strict. Many functions can work with only some of the datatypes. In most cases <code>float</code> and <code>double</code> are used. Other types may or may not be supported by a specific function.</p>
<p>However, there are a lot of ways to play with types. For example, you may use <code>torch.tensor([1], dtype=torch.double)</code> to directly construct a <code>double</code> tensor, or use <code>torch.tensor([1]).double()</code> to first construct an <code>int</code> tensor and then cast it into a <code>double</code> tensor.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
another datatype note
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>numpy</code> also has <code>dtype</code> setting but since it is not strict on it, we ignored it previous. Here is the case: the default setting for <code>numpy</code> is <code>double</code> type, or <code>float64</code>, while in <code>PyTorch</code> <code>float</code>, or <code>float32</code>, is commonly used. Since the precision is different, when cast from <code>double</code> to <code>float</code>, the number might be changed a little bit.</p>
<div id="b955a9da" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>a <span class="op">=</span> np.random.random(<span class="dv">1</span>)</span>
<span id="cb17-2"><a href="#cb17-2"></a>b <span class="op">=</span> torch.as_tensor(a)</span>
<span id="cb17-3"><a href="#cb17-3"></a>c <span class="op">=</span> torch.as_tensor(a, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb17-4"><a href="#cb17-4"></a>d <span class="op">=</span> torch.as_tensor(a, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb17-5"><a href="#cb17-5"></a>e <span class="op">=</span> torch.as_tensor(a, dtype<span class="op">=</span>torch.float64)</span>
<span id="cb17-6"><a href="#cb17-6"></a>f <span class="op">=</span> b.<span class="bu">float</span>()</span>
<span id="cb17-7"><a href="#cb17-7"></a>g <span class="op">=</span> f.double()</span>
<span id="cb17-8"><a href="#cb17-8"></a><span class="bu">print</span>(<span class="ss">f'a: </span><span class="sc">{</span>a[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">, type of a: </span><span class="sc">{</span>a<span class="sc">.</span>dtype<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb17-9"><a href="#cb17-9"></a>      <span class="ss">f'b: </span><span class="sc">{</span>b<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, type of b: </span><span class="sc">{</span>b<span class="sc">.</span>dtype<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb17-10"><a href="#cb17-10"></a>      <span class="ss">f'c: </span><span class="sc">{</span>c<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, type of c: </span><span class="sc">{</span>c<span class="sc">.</span>dtype<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb17-11"><a href="#cb17-11"></a>      <span class="ss">f'd: </span><span class="sc">{</span>d<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, type of d: </span><span class="sc">{</span>d<span class="sc">.</span>dtype<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb17-12"><a href="#cb17-12"></a>      <span class="ss">f'e: </span><span class="sc">{</span>e<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, type of e: </span><span class="sc">{</span>e<span class="sc">.</span>dtype<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb17-13"><a href="#cb17-13"></a>      <span class="ss">f'f: </span><span class="sc">{</span>f<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, type of e: </span><span class="sc">{</span>f<span class="sc">.</span>dtype<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span></span>
<span id="cb17-14"><a href="#cb17-14"></a>      <span class="ss">f'g: </span><span class="sc">{</span>g<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, type of e: </span><span class="sc">{</span>g<span class="sc">.</span>dtype<span class="sc">}</span><span class="ch">\n</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>a: 0.28093450968738076, type of a: float64
b: 0.28093450968738076, type of b: torch.float64
c: 0.28093451261520386, type of c: torch.float32
d: 0.28093450968738076, type of d: torch.float64
e: 0.28093450968738076, type of e: torch.float64
f: 0.28093451261520386, type of e: torch.float32
g: 0.28093451261520386, type of e: torch.float64
</code></pre>
</div>
</div>
<p>You may notice the difference from the example, and also take notes about the convetion of which setting is corresponding to which type. Note that <code>dtype=float</code> actually create <code>double</code> type.</p>
<p>In this notes we will use <code>double</code> type by setting <code>dtype=float</code> or <code>torch.float64</code> to reduce the possibility of that small differences.</p>
</div>
</div>
<p>We now construct a <code>PyTorch</code> tensor version of the dataset we used in previous sections. The <code>device</code> part will be introduced later.</p>
<div id="d0bef568" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="im">import</span> torch</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>RANDOMSEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>np.random.seed(RANDOMSEED)</span>
<span id="cb19-7"><a href="#cb19-7"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb19-8"><a href="#cb19-8"></a></span>
<span id="cb19-9"><a href="#cb19-9"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>)</span>
<span id="cb19-10"><a href="#cb19-10"></a>y <span class="op">=</span> <span class="fl">2.3</span> <span class="op">+</span> <span class="fl">1.2</span> <span class="op">*</span> X <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb19-11"><a href="#cb19-11"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">.15</span>,</span>
<span id="cb19-12"><a href="#cb19-12"></a>                                                    random_state<span class="op">=</span>RANDOMSEED)</span>
<span id="cb19-13"><a href="#cb19-13"></a>X_tensor_train <span class="op">=</span> torch.as_tensor(X_train, device<span class="op">=</span>device, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb19-14"><a href="#cb19-14"></a>y_tensor_train <span class="op">=</span> torch.as_tensor(y_train, device<span class="op">=</span>device, dtype<span class="op">=</span><span class="bu">float</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Back to <code>numpy</code>
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we would like to turn a tensor back to <code>numpy</code>, usually we would like to remove it from related computation graphs and just keep the data. Therefore usually we would like to apply <code>detach()</code> method to the tensor before converting. Also later we will talk about devices. When taking GPU into consideration, we also want to send the tensor back to CPU before converting. Therefore the code to turn a <code>PyTorch</code> tensor to a <code>numpy</code> array is <code>x.detach().cpu().numpy()</code>.</p>
</div>
</div>
</section>
<section id="devices" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="devices"><span class="header-section-number">5.3.2</span> devices</h3>
<p>We coulde use <code>torch.cuda.is_available()</code> to check whether we have GPU/CUDA supported devices. If the answer is no, we don’t need to change any codes and everything works fine but slow.</p>
<p>If we have GPU/CUDA supported devices, we could send our tensors to them and do computations there. Google Colab is a good place to play with it if we don’t have our own hardware.</p>
<p>In most cases we use <code>to(device)</code> method to send a tensor to a device. Sometimes some function has <code>device=device</code> argument to automatically construct tensors in a device. Note that if one needs to compute the gradient of a tensor and send the tensor to a device, we need to manually set <code>requires_grad_(True)</code> or create the tensor with <code>device</code> argument.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<code>to(device)</code>
</div>
</div>
<div class="callout-body-container callout-body">
<p>When sending tensors to other devices by <code>to</code>, gradient info might be lost. Therefore if we need to send trainable tensors to GPU some special methods should be used (e.g.&nbsp;setting <code>device</code> when creating the tensor). However for the dataset we don’t need to worry about it.</p>
</div>
</div>
<p>Here are some examples, although they only makes sense in a GPU environment.</p>
<div id="7ea0b073" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb20-2"><a href="#cb20-2"></a></span>
<span id="cb20-3"><a href="#cb20-3"></a>t1 <span class="op">=</span> torch.tensor(<span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">float</span>, device<span class="op">=</span>device)</span>
<span id="cb20-4"><a href="#cb20-4"></a>t2 <span class="op">=</span> torch.tensor(<span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb20-5"><a href="#cb20-5"></a><span class="bu">print</span>(<span class="ss">f't1: </span><span class="sc">{</span>t1<span class="sc">.</span><span class="bu">type</span>()<span class="sc">}</span><span class="ss">, t2: </span><span class="sc">{</span>t2<span class="sc">.</span><span class="bu">type</span>()<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you can see <code>cuda</code> in the output of <code>type</code>, it is a GPU tensor. Otherwise it is a CPU tensor. We may use <code>to</code> to convert a CPU tensor to be a GPU tensor. If this tensor requires gradient, we should set it manually.</p>
<div id="2ebae6ad" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>t3 <span class="op">=</span> t2.to(device)</span>
<span id="cb21-2"><a href="#cb21-2"></a>t3 <span class="op">=</span> t3.requires_grad_(<span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It is usually recommended to write codes with <code>device</code> in mind like above, since the codes work for both CPU and GPU machines.</p>
</section>
<section id="sec-gradientdescent_pytorch_example" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="sec-gradientdescent_pytorch_example"><span class="header-section-number">5.3.3</span> Gradient</h3>
<p><code>PyTorch</code> can use <code>autograd</code> to automatically compute the gradient of given formula. All computations are done within the context of tensors. The biggest difference between <code>PyTorch</code> tensor and <code>numpy</code> array is that <code>PyTorch</code> tensor carries gradient infomation on its own.</p>
<p>The step is very easy: first use <code>PyTorch</code> tensor to write a formula, enable gradients on correct tensors, and then use the <code>backward()</code> method.</p>
<div id="bcfa2776" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>b <span class="op">=</span> torch.tensor(<span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">float</span>, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-2"><a href="#cb22-2"></a>w <span class="op">=</span> torch.tensor(<span class="fl">1.5</span>, dtype<span class="op">=</span><span class="bu">float</span>, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-3"><a href="#cb22-3"></a></span>
<span id="cb22-4"><a href="#cb22-4"></a>loss <span class="op">=</span> ((y_tensor_train <span class="op">-</span> b <span class="op">-</span> w <span class="op">*</span> X_tensor_train)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb22-5"><a href="#cb22-5"></a>loss.backward()</span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="bu">print</span>(<span class="ss">f'db: </span><span class="sc">{</span>b<span class="sc">.</span>grad<span class="sc">}</span><span class="ss">, dw: </span><span class="sc">{</span>w<span class="sc">.</span>grad<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>db: -2.310644882519191, dw: -1.0130224137389</code></pre>
</div>
</div>
<p>We could manually compute the first few iterations and record the results. You may compare it with the <code>numpy</code> implementation in <a href="#sec-gradeientdescent_numpy_example" class="quarto-xref"><span>Section 5.2.3</span></a>. The answer is exactly the same.</p>
<div id="4d53c4a8" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>b <span class="op">=</span> torch.tensor(<span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">float</span>, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-2"><a href="#cb24-2"></a>w <span class="op">=</span> torch.tensor(<span class="fl">1.5</span>, dtype<span class="op">=</span><span class="bu">float</span>, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-3"><a href="#cb24-3"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>plist <span class="op">=</span> []</span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb24-6"><a href="#cb24-6"></a>    loss <span class="op">=</span> ((y_tensor_train <span class="op">-</span> b <span class="op">-</span> w <span class="op">*</span> X_tensor_train)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb24-7"><a href="#cb24-7"></a>    loss.backward()</span>
<span id="cb24-8"><a href="#cb24-8"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb24-9"><a href="#cb24-9"></a>        b <span class="op">-=</span> lr <span class="op">*</span> b.grad</span>
<span id="cb24-10"><a href="#cb24-10"></a>        w <span class="op">-=</span> lr <span class="op">*</span> w.grad</span>
<span id="cb24-11"><a href="#cb24-11"></a>    b.grad.zero_()</span>
<span id="cb24-12"><a href="#cb24-12"></a>    w.grad.zero_()</span>
<span id="cb24-13"><a href="#cb24-13"></a>    plist.append([b.item(), w.item()])</span>
<span id="cb24-14"><a href="#cb24-14"></a>    </span>
<span id="cb24-15"><a href="#cb24-15"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>[[1.462128976503838, 1.70260448274778],
 [1.701791352506138, 1.7949449226054148],
 [1.8284450974134463, 1.8316393022111805],
 [1.8976247633456746, 1.840388291132826],
 [1.9375082488699051, 1.8352370440485253],
 [1.9623947070300602, 1.8233031967656035],
 [1.9795421883017095, 1.8081901205764057],
 [1.992636530631048, 1.7917185352872653],
 [2.003551206784623, 1.7748049887103945],
 [2.013240136591026, 1.7579075228763734]]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The code has some tricky parts. The main issue is to let <code>PyTorch</code> know which gradient infomation should be kept, which should not. In this code, to make it run correctly, we need to pay attention to the following three things:</p>
<ul>
<li>Before updating <code>b</code> and <code>w</code>, <code>with torch.no_grad()</code> should be used, to tell <code>PyTorch</code> don’t compute gradient here.</li>
<li>When updating <code>b</code> and <code>w</code>, we should use the in-place syntax <code>b -= db</code> instead of <code>b = b - db</code>. Again, the reason is related to updating gradient: the out-of-place syntax <code>b = b - db</code> will lose the grad info.</li>
<li>After updating <code>b</code> and <code>w</code>, we need to zero out the grad info by applying <code>b.grad.zero_()</code> and <code>w.grad.zero_()</code>.</li>
</ul>
</div>
</div>
<p>We will skip mini-batch gradient descent here, and leave it to the next section with a more systematic treatment.</p>
</section>
<section id="optimizers" class="level3" data-number="5.3.4">
<h3 data-number="5.3.4" class="anchored" data-anchor-id="optimizers"><span class="header-section-number">5.3.4</span> Optimizers</h3>
<p>After we get the gradient, there are still many tricks to move one step further. We already talked about the learning rate before. It is not the only case. Another example is that sometimes we don’t really want to move in the direction given by the gradient, but we want to modify it a little bit. All these tricks are combined together and are called optimizers.</p>
<p>An optimizer is a set of rules to update parameters after the gradient is computed. We already talked about <code>SGD</code> (stochastic gradient descent). Other common ones include <code>RMSprop</code> and <code>Adam</code>. In general, <code>Adam</code> is the generic best optimizer. If you don’t know which optimizer to use, <code>Adam</code> is always the go-to choice.</p>
<p>Here we rewrite our previous code by optimizers. We use <code>SGD</code> in this example. Again, we may compare the results to <a href="#sec-gradeientdescent_numpy_example" class="quarto-xref"><span>Section 5.2.3</span></a> and <a href="#sec-gradientdescent_pytorch_example" class="quarto-xref"><span>Section 5.3.3</span></a>.</p>
<div id="f9932498" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> SGD</span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="im">import</span> torch</span>
<span id="cb26-3"><a href="#cb26-3"></a></span>
<span id="cb26-4"><a href="#cb26-4"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb26-5"><a href="#cb26-5"></a>b <span class="op">=</span> torch.tensor(<span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">float</span>, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-6"><a href="#cb26-6"></a>w <span class="op">=</span> torch.tensor(<span class="fl">1.5</span>, dtype<span class="op">=</span><span class="bu">float</span>, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-7"><a href="#cb26-7"></a></span>
<span id="cb26-8"><a href="#cb26-8"></a>optimizer <span class="op">=</span> SGD([b, w], lr<span class="op">=</span>lr)</span>
<span id="cb26-9"><a href="#cb26-9"></a>plist <span class="op">=</span> []</span>
<span id="cb26-10"><a href="#cb26-10"></a></span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb26-12"><a href="#cb26-12"></a>    loss <span class="op">=</span> ((y_tensor_train <span class="op">-</span> b <span class="op">-</span> w<span class="op">*</span>X_tensor_train)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb26-13"><a href="#cb26-13"></a>    loss.backward()</span>
<span id="cb26-14"><a href="#cb26-14"></a>    optimizer.step()</span>
<span id="cb26-15"><a href="#cb26-15"></a>    optimizer.zero_grad()</span>
<span id="cb26-16"><a href="#cb26-16"></a>    plist.append([b.item(), w.item()])</span>
<span id="cb26-17"><a href="#cb26-17"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>[[1.462128976503838, 1.70260448274778],
 [1.701791352506138, 1.7949449226054148],
 [1.8284450974134463, 1.8316393022111805],
 [1.8976247633456746, 1.840388291132826],
 [1.9375082488699051, 1.8352370440485253],
 [1.9623947070300602, 1.8233031967656035],
 [1.9795421883017095, 1.8081901205764057],
 [1.992636530631048, 1.7917185352872653],
 [2.003551206784623, 1.7748049887103945],
 [2.013240136591026, 1.7579075228763734]]</code></pre>
</div>
</div>
</section>
<section id="use-class-to-describe-the-model" class="level3" data-number="5.3.5">
<h3 data-number="5.3.5" class="anchored" data-anchor-id="use-class-to-describe-the-model"><span class="header-section-number">5.3.5</span> Use class to describe the model</h3>
<p>We now want to upgrade the code we wrote in previous sections in terms of classes, since it is a good way to wrap up our own code.</p>
<div id="71cedd9c" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb28-2"><a href="#cb28-2"></a></span>
<span id="cb28-3"><a href="#cb28-3"></a><span class="kw">class</span> LR(nn.Module):</span>
<span id="cb28-4"><a href="#cb28-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb28-5"><a href="#cb28-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb28-6"><a href="#cb28-6"></a></span>
<span id="cb28-7"><a href="#cb28-7"></a>        <span class="va">self</span>.b <span class="op">=</span> nn.Parameter(torch.tensor(<span class="dv">1</span>, requires_grad<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span><span class="bu">float</span>))</span>
<span id="cb28-8"><a href="#cb28-8"></a>        <span class="va">self</span>.w <span class="op">=</span> nn.Parameter(torch.tensor(<span class="fl">1.5</span>, requires_grad<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span><span class="bu">float</span>))</span>
<span id="cb28-9"><a href="#cb28-9"></a></span>
<span id="cb28-10"><a href="#cb28-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb28-11"><a href="#cb28-11"></a>        <span class="cf">return</span> <span class="va">self</span>.b <span class="op">+</span> <span class="va">self</span>.w <span class="op">*</span> x</span>
<span id="cb28-12"><a href="#cb28-12"></a></span>
<span id="cb28-13"><a href="#cb28-13"></a>RANDOMSEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb28-14"><a href="#cb28-14"></a>torch.manual_seed(RANDOMSEED)</span>
<span id="cb28-15"><a href="#cb28-15"></a></span>
<span id="cb28-16"><a href="#cb28-16"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb28-17"><a href="#cb28-17"></a>model <span class="op">=</span> LR().to(device)</span>
<span id="cb28-18"><a href="#cb28-18"></a>model.state_dict()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>OrderedDict([('b', tensor(1., dtype=torch.float64)),
             ('w', tensor(1.5000, dtype=torch.float64))])</code></pre>
</div>
</div>
<p>We could use <code>model.state_dict()</code> to look at the parameters of the model. Another way to see the parameters is to use <code>model.parameters()</code> method. The latter will return an iterator that help you go through all parameters.</p>
<div id="2b82ccc2" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="cf">for</span> item <span class="kw">in</span> model.parameters():</span>
<span id="cb30-2"><a href="#cb30-2"></a>    <span class="bu">print</span>(item)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor(1., dtype=torch.float64, requires_grad=True)
Parameter containing:
tensor(1.5000, dtype=torch.float64, requires_grad=True)</code></pre>
</div>
</div>
<p>Now we reproduce the training code for <code>LR</code> class.</p>
<div id="508ab116" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> SGD</span>
<span id="cb32-2"><a href="#cb32-2"></a></span>
<span id="cb32-3"><a href="#cb32-3"></a><span class="kw">def</span> loss_fn(yhat, y):</span>
<span id="cb32-4"><a href="#cb32-4"></a>    <span class="cf">return</span> ((yhat<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span>).mean()</span>
<span id="cb32-5"><a href="#cb32-5"></a></span>
<span id="cb32-6"><a href="#cb32-6"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb32-7"><a href="#cb32-7"></a>optimizer <span class="op">=</span> SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb32-8"><a href="#cb32-8"></a></span>
<span id="cb32-9"><a href="#cb32-9"></a>epoch_num <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb32-10"><a href="#cb32-10"></a></span>
<span id="cb32-11"><a href="#cb32-11"></a>plist <span class="op">=</span> []</span>
<span id="cb32-12"><a href="#cb32-12"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epoch_num):</span>
<span id="cb32-13"><a href="#cb32-13"></a>    model.train()</span>
<span id="cb32-14"><a href="#cb32-14"></a></span>
<span id="cb32-15"><a href="#cb32-15"></a>    yhat <span class="op">=</span> model(X_tensor_train)</span>
<span id="cb32-16"><a href="#cb32-16"></a>    loss <span class="op">=</span> loss_fn(yhat, y_tensor_train)</span>
<span id="cb32-17"><a href="#cb32-17"></a>    loss.backward()</span>
<span id="cb32-18"><a href="#cb32-18"></a>    optimizer.step()</span>
<span id="cb32-19"><a href="#cb32-19"></a>    optimizer.zero_grad()</span>
<span id="cb32-20"><a href="#cb32-20"></a>    p <span class="op">=</span> model.state_dict()</span>
<span id="cb32-21"><a href="#cb32-21"></a>    plist.append([p[<span class="st">'b'</span>].item(), p[<span class="st">'w'</span>].item()])</span>
<span id="cb32-22"><a href="#cb32-22"></a></span>
<span id="cb32-23"><a href="#cb32-23"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>[[1.462128976503838, 1.70260448274778],
 [1.701791352506138, 1.7949449226054148],
 [1.8284450974134463, 1.8316393022111805],
 [1.8976247633456746, 1.840388291132826],
 [1.9375082488699051, 1.8352370440485253],
 [1.9623947070300602, 1.8233031967656035],
 [1.9795421883017095, 1.8081901205764057],
 [1.992636530631048, 1.7917185352872653],
 [2.003551206784623, 1.7748049887103945],
 [2.013240136591026, 1.7579075228763734]]</code></pre>
</div>
</div>
</section>
<section id="using-standard-modules" class="level3" data-number="5.3.6">
<h3 data-number="5.3.6" class="anchored" data-anchor-id="using-standard-modules"><span class="header-section-number">5.3.6</span> Using standard modules</h3>
<p>We hand write our models and set parameters in our previous versions. <code>PyTorch</code> provides many standard modules that we can use directly. For example, the linear regression model can be found in <code>nn.modules</code> as <code>Linear</code>, while our loss function is the mean square differene function which is <code>MSELoss</code> from <code>nn</code>.</p>
<div id="87d64bdf" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="im">from</span> torch.nn.modules <span class="im">import</span> Linear</span>
<span id="cb34-2"><a href="#cb34-2"></a><span class="im">from</span> torch.nn <span class="im">import</span> MSELoss</span>
<span id="cb34-3"><a href="#cb34-3"></a></span>
<span id="cb34-4"><a href="#cb34-4"></a><span class="kw">class</span> BetterLR(nn.Module):</span>
<span id="cb34-5"><a href="#cb34-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb34-6"><a href="#cb34-6"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb34-7"><a href="#cb34-7"></a></span>
<span id="cb34-8"><a href="#cb34-8"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(in_features<span class="op">=</span><span class="dv">1</span>, out_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-9"><a href="#cb34-9"></a>        <span class="va">self</span>.linear.bias <span class="op">=</span> torch.nn.Parameter(torch.tensor([<span class="fl">1.0</span>], dtype<span class="op">=</span><span class="bu">float</span>))</span>
<span id="cb34-10"><a href="#cb34-10"></a>        <span class="va">self</span>.linear.weight <span class="op">=</span> torch.nn.Parameter(torch.tensor([[<span class="fl">1.5</span>]], dtype<span class="op">=</span><span class="bu">float</span>))</span>
<span id="cb34-11"><a href="#cb34-11"></a></span>
<span id="cb34-12"><a href="#cb34-12"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-13"><a href="#cb34-13"></a>        <span class="cf">return</span> <span class="va">self</span>.linear(x)</span>
<span id="cb34-14"><a href="#cb34-14"></a></span>
<span id="cb34-15"><a href="#cb34-15"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb34-16"><a href="#cb34-16"></a></span>
<span id="cb34-17"><a href="#cb34-17"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb34-18"><a href="#cb34-18"></a>model2 <span class="op">=</span> BetterLR().to(device)</span>
<span id="cb34-19"><a href="#cb34-19"></a>optimizer2 <span class="op">=</span> SGD(model2.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb34-20"><a href="#cb34-20"></a></span>
<span id="cb34-21"><a href="#cb34-21"></a>epoch_num <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb34-22"><a href="#cb34-22"></a>plist <span class="op">=</span> []</span>
<span id="cb34-23"><a href="#cb34-23"></a></span>
<span id="cb34-24"><a href="#cb34-24"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epoch_num):</span>
<span id="cb34-25"><a href="#cb34-25"></a>    model2.train()</span>
<span id="cb34-26"><a href="#cb34-26"></a></span>
<span id="cb34-27"><a href="#cb34-27"></a>    yhat <span class="op">=</span> model2(X_tensor_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb34-28"><a href="#cb34-28"></a>    loss2 <span class="op">=</span> MSELoss(reduction<span class="op">=</span><span class="st">'mean'</span>)(yhat, y_tensor_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb34-29"><a href="#cb34-29"></a>    loss2.backward()</span>
<span id="cb34-30"><a href="#cb34-30"></a>    optimizer2.step()</span>
<span id="cb34-31"><a href="#cb34-31"></a>    optimizer2.zero_grad()</span>
<span id="cb34-32"><a href="#cb34-32"></a>    p <span class="op">=</span> model2.state_dict()</span>
<span id="cb34-33"><a href="#cb34-33"></a>    plist.append([p[<span class="st">'linear.bias'</span>].item(), p[<span class="st">'linear.weight'</span>].item()])</span>
<span id="cb34-34"><a href="#cb34-34"></a></span>
<span id="cb34-35"><a href="#cb34-35"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>[[1.462128976503838, 1.70260448274778],
 [1.701791352506138, 1.7949449226054148],
 [1.8284450974134463, 1.8316393022111805],
 [1.8976247633456746, 1.840388291132826],
 [1.9375082488699051, 1.8352370440485253],
 [1.9623947070300602, 1.8233031967656035],
 [1.9795421883017095, 1.8081901205764057],
 [1.992636530631048, 1.7917185352872653],
 [2.003551206784623, 1.7748049887103945],
 [2.013240136591026, 1.7579075228763734]]</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Initialize the parameters
</div>
</div>
<div class="callout-body-container callout-body">
<p>In all our examples we initialize the parameters to be <span class="math inline">\((1, 1.5)\)</span> for the purpose of comparision. In most cases, we don’t manually set the intial values, but use random numbers. In this case, we simply delete the manual codes.</p>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that if we directly change our function to the standard one we will encounter some issues. The main reason is that our previous code is an oversimplifed version that we treat <code>b</code> and <code>w</code> as two scalars. They are scalars in our particular problem, but it is better to treat them as a special case of tensors for the purpose of better generalization. Actually based on the standard functions from <code>PyTorch</code> (as well as many others like <code>sklearn</code>) <code>X</code> and <code>y</code> are expected to be 2D tensors. This is the reason why there are some strange <code>reshape(-1, 1 )</code> in the codes.</p>
<p>We will reconstruct it in the later sections.</p>
</div>
</div>
</section>
</section>
<section id="sec-dataloader" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec-dataloader"><span class="header-section-number">5.4</span> Dataloader</h2>
<section id="convert-the-previous-dataset-using-dataloader" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="convert-the-previous-dataset-using-dataloader"><span class="header-section-number">5.4.1</span> Convert the previous dataset using DataLoader</h3>
<p>Usually we use a class to provide data. The class is based on <code>Dataset</code> class, and need to implement the constructor, <code>__getitem__</code> method and <code>__len__</code> method. Here is an example.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that we directly change <code>X</code> and <code>y</code> to be 2D tensors when we create the dataset.</p>
</div>
</div>
<div id="b23a9694" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb36-2"><a href="#cb36-2"></a></span>
<span id="cb36-3"><a href="#cb36-3"></a><span class="kw">class</span> MyData(Dataset):</span>
<span id="cb36-4"><a href="#cb36-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, x, y):</span>
<span id="cb36-5"><a href="#cb36-5"></a>        <span class="va">self</span>.x <span class="op">=</span> torch.tensor(x, dtype<span class="op">=</span><span class="bu">float</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb36-6"><a href="#cb36-6"></a>        <span class="va">self</span>.y <span class="op">=</span> torch.tensor(y, dtype<span class="op">=</span><span class="bu">float</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb36-7"><a href="#cb36-7"></a></span>
<span id="cb36-8"><a href="#cb36-8"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb36-9"><a href="#cb36-9"></a>        <span class="cf">return</span> (<span class="va">self</span>.x[index], <span class="va">self</span>.y[index])</span>
<span id="cb36-10"><a href="#cb36-10"></a></span>
<span id="cb36-11"><a href="#cb36-11"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb36-12"><a href="#cb36-12"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.y)</span>
<span id="cb36-13"><a href="#cb36-13"></a></span>
<span id="cb36-14"><a href="#cb36-14"></a>train_data <span class="op">=</span> MyData(X_train, y_train)</span>
<span id="cb36-15"><a href="#cb36-15"></a>train_data[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>(tensor([0.7713], dtype=torch.float64), tensor([3.1575], dtype=torch.float64))</code></pre>
</div>
</div>
<p>Then we use <code>Dataloader</code> to feed the data into our model.</p>
<div id="9fccd07b" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader </span>
<span id="cb38-2"><a href="#cb38-2"></a>train_loader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It is used in the following way.</p>
<div id="f07b8716" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb39-2"><a href="#cb39-2"></a>epoch_num <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb39-3"><a href="#cb39-3"></a></span>
<span id="cb39-4"><a href="#cb39-4"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb39-5"><a href="#cb39-5"></a>model <span class="op">=</span> BetterLR().to(device)</span>
<span id="cb39-6"><a href="#cb39-6"></a>optimizer <span class="op">=</span> SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb39-7"><a href="#cb39-7"></a></span>
<span id="cb39-8"><a href="#cb39-8"></a>plist <span class="op">=</span> []</span>
<span id="cb39-9"><a href="#cb39-9"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epoch_num):</span>
<span id="cb39-10"><a href="#cb39-10"></a>    <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> train_loader:</span>
<span id="cb39-11"><a href="#cb39-11"></a>        yhat <span class="op">=</span> model(X_batch)</span>
<span id="cb39-12"><a href="#cb39-12"></a>        loss <span class="op">=</span> MSELoss(reduction<span class="op">=</span><span class="st">'mean'</span>)(yhat, y_batch)</span>
<span id="cb39-13"><a href="#cb39-13"></a>        loss.backward()</span>
<span id="cb39-14"><a href="#cb39-14"></a>        optimizer.step()</span>
<span id="cb39-15"><a href="#cb39-15"></a>        optimizer.zero_grad()</span>
<span id="cb39-16"><a href="#cb39-16"></a>    p <span class="op">=</span> model.state_dict()</span>
<span id="cb39-17"><a href="#cb39-17"></a>    plist.append([p[<span class="st">'linear.bias'</span>].item(), p[<span class="st">'linear.weight'</span>].item()])</span>
<span id="cb39-18"><a href="#cb39-18"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>[[1.823910885110298, 1.8337838971747065],
 [1.943046588179278, 1.8159156063329505],
 [2.008997965681829, 1.7804701154492042],
 [2.0285249013384035, 1.7270125310793967],
 [2.0563058412652087, 1.6825156563053996],
 [2.0699681006441395, 1.6356185362954465],
 [2.105228875384819, 1.6040273694037541],
 [2.0967822112836743, 1.552514187091518],
 [2.124683201405048, 1.5249842249986072],
 [2.1368486965881486, 1.493812615037102]]</code></pre>
</div>
</div>
<p>When applying mini-batch, usually we will shuffle the dataset. If we disable the shuffle here as well as the shuffle in <code>numpy</code> case, you will see that we get exactly the same answer.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Non-shuffle version
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here is the result for non-shuffle version.</p>
<div id="a0ac7fbb" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader </span>
<span id="cb41-2"><a href="#cb41-2"></a></span>
<span id="cb41-3"><a href="#cb41-3"></a>lr <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb41-4"><a href="#cb41-4"></a>epoch_num <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb41-5"><a href="#cb41-5"></a></span>
<span id="cb41-6"><a href="#cb41-6"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb41-7"><a href="#cb41-7"></a>model <span class="op">=</span> BetterLR().to(device)</span>
<span id="cb41-8"><a href="#cb41-8"></a>optimizer <span class="op">=</span> SGD(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb41-9"><a href="#cb41-9"></a></span>
<span id="cb41-10"><a href="#cb41-10"></a>train_loader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb41-11"><a href="#cb41-11"></a></span>
<span id="cb41-12"><a href="#cb41-12"></a>plist <span class="op">=</span> []</span>
<span id="cb41-13"><a href="#cb41-13"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epoch_num):</span>
<span id="cb41-14"><a href="#cb41-14"></a>    <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> train_loader:</span>
<span id="cb41-15"><a href="#cb41-15"></a>        yhat <span class="op">=</span> model(X_batch)</span>
<span id="cb41-16"><a href="#cb41-16"></a>        loss <span class="op">=</span> MSELoss(reduction<span class="op">=</span><span class="st">'mean'</span>)(yhat, y_batch)</span>
<span id="cb41-17"><a href="#cb41-17"></a>        loss.backward()</span>
<span id="cb41-18"><a href="#cb41-18"></a>        optimizer.step()</span>
<span id="cb41-19"><a href="#cb41-19"></a>        optimizer.zero_grad()</span>
<span id="cb41-20"><a href="#cb41-20"></a>    p <span class="op">=</span> model.state_dict()</span>
<span id="cb41-21"><a href="#cb41-21"></a>    plist.append([p[<span class="st">'linear.bias'</span>].item(), p[<span class="st">'linear.weight'</span>].item()])</span>
<span id="cb41-22"><a href="#cb41-22"></a>plist</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>[[1.8277028504755573, 1.8368193572906044],
 [1.9607104449826838, 1.8293981130981023],
 [2.001626059409397, 1.7815077539441087],
 [2.0286935704191, 1.7321715193480243],
 [2.0522055690695757, 1.686138097785071],
 [2.0736381185747943, 1.6437403254745735],
 [2.0933134526016604, 1.6047617600958677],
 [2.111393711486754, 1.5689357968513453],
 [2.1280105514943686, 1.5360086358936902],
 [2.143282725696795, 1.505745878510758]]</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>You may notice that we use some not-very-elegent way to display the result. Don’t worry about it. We will work on a better solution in the next section.</p>
</section>
<section id="rewrite-using-random_split" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="rewrite-using-random_split"><span class="header-section-number">5.4.2</span> Rewrite using <code>random_split</code></h3>
<p>It is possible to purely use <code>PyTorch</code> instead of going through <code>sklearn</code>. After we get the Dataset, we could use <code>random_split</code> to create training set and testing set.</p>
<div id="edf88408" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split</span>
<span id="cb43-2"><a href="#cb43-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-3"><a href="#cb43-3"></a></span>
<span id="cb43-4"><a href="#cb43-4"></a>dataset <span class="op">=</span> MyData(X, y)</span>
<span id="cb43-5"><a href="#cb43-5"></a>train_data, val_data <span class="op">=</span> random_split(dataset, [<span class="fl">.85</span>, <span class="fl">.15</span>], generator<span class="op">=</span>torch.Generator().manual_seed(<span class="dv">42</span>))</span>
<span id="cb43-6"><a href="#cb43-6"></a></span>
<span id="cb43-7"><a href="#cb43-7"></a>train_loader <span class="op">=</span> DataLoader(train_data, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-8"><a href="#cb43-8"></a>val_loader <span class="op">=</span> DataLoader(val_data, batch_size<span class="op">=</span><span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<!-- {{< include output.qmd >}} -->
</section>
</section>
<section id="exercises" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="exercises"><span class="header-section-number">5.5</span> Exercises</h2>
<div id="exr-" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.1</strong></span> Try to reconstruct all the plots in Section <a href="#sec-linearregression_math" class="quarto-xref"><span>Section 5.1</span></a>.</p>
</div>


<div id="refs" class="references csl-bib-body" role="list" style="display: none">
<div id="ref-Godoy2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline"><span class="smallcaps">Godoy</span>, D. V. (2022). <em><a href="https://leanpub.com/pytorch">Deep learning with PyTorch step-by-step: A beginner’s guide</a></em>. https://leanpub.com/pytorch.</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/4/intro.html" class="pagination-link" aria-label="Ensemble methods">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ensemble methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/6/intro.html" class="pagination-link" aria-label="Logistic regression">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>