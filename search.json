[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning 2025 Fall",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT 4803/5803 Machine Learning 2025 Fall at ATU. If you have any comments/suggetions/concers about the notes please contact us at xxiao@atu.edu.\n\n\nReferences\n\n\n[1] Godoy, D. V.\n(2022). Deep learning with\nPyTorch step-by-step: A beginner’s guide.\nhttps://leanpub.com/pytorch.\n\n\n[2] Chollet, F.\n(2021). Deep\nlearning with python, second edition. MANNING PUBN.\n\n\n[3] Géron, A.\n(2019). Hands-on machine learning with scikit-learn, keras, and\nTensorFlow concepts, tools, and techniques to build intelligent systems:\nConcepts, tools, and techniques to build intelligent systems.\nO’Reilly Media.\n\n\n[4] Harrington, P.\n(2012). Machine\nlearning in action. Manning Publications.\n\n\n[5] Klosterman, S.\n(2021). Data\nscience projects with python: A case study approach to gaining valuable\ninsights from real data with machine learning. Packt\nPublishing, Limited.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "contents/1/intro.html",
    "href": "contents/1/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Machine Learning?\nIn this Chapter we will discuss\nMachine Learning is the science (and art) of programming computers so they can learn from data [1].\nHere is a slightly more general definition:\nThis “without being explicitly programmed to do so” is the essential difference between Machine Learning and usual computing tasks. The usual way to make a computer do useful work is to have a human programmer write down rules — a computer program — to be followed to turn input data into appropriate answers. Machine Learning turns this around: the machine looks at the input data and the expected task outcome, and figures out what the rules should be. A Machine Learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task [2].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#what-is-machine-learning",
    "href": "contents/1/intro.html#what-is-machine-learning",
    "title": "1  Introduction",
    "section": "",
    "text": "[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\n                                           -- Arthur Samuel, 1959",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#types-of-machine-learning-systems",
    "href": "contents/1/intro.html#types-of-machine-learning-systems",
    "title": "1  Introduction",
    "section": "1.2 Types of Machine Learning Systems",
    "text": "1.2 Types of Machine Learning Systems\nThere are many different types of Machine Learning systems that it is useful to classify them in braod categories, based on different criteria. These criteria are not exclusive, and you can combine them in any way you like.\nThe most popular criterion for Machine Learning classification is the amount and type of supervision they get during training. In this case there are four major types.\nSupervised Learning The training set you feed to the algorithm includes the desired solutions. The machines learn from the data to alter the model to get the desired output. The main task for Supervised Learning is classification and regression.\nUnsupervised Learning In Unsupervised Learning, the data provided doesn’t have class information or desired solutions. We just want to dig some information directly from those data themselves. Usually Unsupervised Learning is used for clustering and dimension reduction.\nReinforcement Learning In Reinforcement Learning, there is a reward system to measure how well the machine performs the task, and the machine is learning to find the strategy to maximize the rewards. Typical examples here include gaming AI and walking robots.\nSemisupervised Learning This is actually a combination of Supervised Learning and Unsupervised Learning, that it is usually used to deal with data that are half labelled.\n\n1.2.1 Tasks for Supervised Learning\nAs mentioned above, for Supervised Learning, there are two typical types of tasks:\nClassification It is the task of predicting a discrete class labels. A typical classification problem is to see an handwritten digit image and recognize it.\nRegression It is the task of predicting a continuous quantity. A typical regression problem is to predict the house price based on various features of the house.\nThere are a lot of other tasks that are not directly covered by these two, but these two are the most classical Supervised Learning tasks.\n\n\n\n\n\n\nNote\n\n\n\nIn this course we will mainly focus on Supervised Classification problems.\n\n\n\n\n1.2.2 Classification based on complexity\nAlong with the popularity boost of deep neural network, there comes another classificaiton: shallow learning vs. deep learning. Basically all but deep neural network belongs to shallow learning. Although deep learning can do a lot of fancy stuffs, shallow learning is still very good in many cases. When the performance of a shallow learning model is good enough comparing to that of a deep learning model, people tend to use the shallow learning since it is usually faster, easier to understand and easier to modify.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "href": "contents/1/intro.html#basic-setting-for-machine-learning-problems",
    "title": "1  Introduction",
    "section": "1.3 Basic setting for Machine learning problems",
    "text": "1.3 Basic setting for Machine learning problems\n\n\n\n\n\n\nNote\n\n\n\nWe by default assume that we are dealing with a Supervised Classification problem.\n\n\n\n1.3.1 Input and output data structure\nSince we are dealing with Supervised Classification problems, the desired solutions are given. These desired solutions in Classification problems are also called labels. The properties that the data are used to describe are called features. Both features and labels are usually organized as row vectors.\n\nExample 1.1 The example is extracted from [3]. There are some sample data shown in the following table. We would like to use these information to classify bird species.\n\n\n\n\nTable 1.1: Bird species classification based on four features\n\n\n\n\n\n\n\n\nWeight (g)\nWingspan (cm)\nWebbed feet?\nBack color\nSpecies\n\n\n\n\n1000.100000\n125.000000\nNo\nBrown\nButeo jamaicensis\n\n\n3000.700000\n200.000000\nNo\nGray\nSagittarius serpentarius\n\n\n3300.000000\n220.300000\nNo\nGray\nSagittarius serpentarius\n\n\n4100.000000\n136.000000\nYes\nBlack\nGavia immer\n\n\n3.000000\n11.000000\nNo\nGreen\nCalothorax lucifer\n\n\n570.000000\n75.000000\nNo\nBlack\nCampephilus principalis\n\n\n\n\n\n\n\n\nThe first four columns are features, and the last column is the label. The first two features are numeric and can take on decimal values. The third feature is binary that can only be \\(1\\) (Yes) or \\(0\\) (No). The fourth feature is an enumeration over the color palette. You may either treat it as categorical data or numeric data, depending on how you want to build the model and what you want to get out of the data. In this example we will use it as categorical data that we only choose it from a list of colors (\\(1\\) — Brown, \\(2\\) — Gray, \\(3\\) — Black, \\(4\\) — Green).\nThen we are able to transform the above data into the following form:\n\n\n\nTable 1.2: Vectorized Bird species data\n\n\n\n\n\nFeatures\nLabels\n\n\n\n\n\\(\\begin{bmatrix}1001.1 & 125.0 & 0 & 1 \\end{bmatrix}\\)\n\\(1\\)\n\n\n\\(\\begin{bmatrix}3000.7 & 200.0 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}3300.0 & 220.3 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}4100.0 & 136.0 & 1 & 3 \\end{bmatrix}\\)\n\\(3\\)\n\n\n\\(\\begin{bmatrix}3.0 & 11.0 & 0 & 4 \\end{bmatrix}\\)\n\\(4\\)\n\n\n\\(\\begin{bmatrix}570.0 & 75.0 & 0 & 3 \\end{bmatrix}\\)\n\\(5\\)\n\n\n\n\n\n\nThen the Supervised Learning problem is stated as follows: Given the features and the labels, we would like to find a model that can classify future data.\n\n\n\n1.3.2 Parameters and hyperparameters\nA model parameter is internal to the model and its value is learned from the data.\nA model hyperparameter is external to the model and its value is set by people.\nFor example, assume that we would like to use Logistic regression to fit the data. We set the learning rate is 0.1 and the maximal iteration is 100. After the computations are done, we get a the model\n\\[\ny = \\sigma(0.8+0.7x).\n\\] The two cofficients \\(0.8\\) and \\(0.7\\) are the parameters of the model. The model Logistic regression, the learning rate 0.1 and the maximal iteration 100 are all hyperparametrs. If we change to a different set of hyperparameters, we may get a different model, with a different set of parameters.\nThe details of Logistic regression will be discussed later.\n\n\n1.3.3 Evaluate a Machine Learning model\nOnce the model is built, how do we know that it is good or not? The naive idea is to test the model on some brand new data and check whether it is able to get the desired results. The usual way to achieve it is to split the input dataset into three pieces: training set, validation set and test set.\nThe model is initially fit on the training set, with some arbitrary selections of hyperparameters. Then hyperparameters will be changed, and new model is fitted over the training set. Which set of hyperparameters is better? We then test their performance over the validation set. We could run through a lot of different combinations of hyperparameters, and find the best performance over the validation set. After we get the best hyperparameters, the model is selcted, and we fit it over the training set to get our model to use.\nTo compare our model with our models, either our own model using other algorithms, or models built by others, we need some new data. We can no longer use the training set and the validation set since all data in them are used, either for training or for hyperparameters tuning. We need to use the test set to evaluate the “real performance” of our data.\nTo summarize:\n\nTraining set: used to fit the model;\nValidation set: used to tune the hyperparameters;\nTest set: used to check the overall performance of the model.\n\nThe validation set is not always required. If we use cross-validation technique for hyperparameters tuning, like sklearn.model_selection.GridSearchCV(), we don’t need a separated validation set. In this case, we will only need the training set and the test set, and run GridSearchCV over the training set. The cross-validation will be discussed in Section 2.2.5.\nThe sizes and strategies for dataset division depends on the problem and data available. It is often recommanded that more training data should be used. The typical distribution of training, validation and test is \\((6:3:1)\\), \\((7:2:1)\\) or \\((8:1:1)\\). Sometimes validation set is discarded and only training set and test set are used. In this case the distribution of training and test set is usually \\((7:3)\\), \\((8:2)\\) or \\((9:1)\\).\n\n\n1.3.4 Workflow in developing a machine learning application\nThe workflow described below is from [3].\n\nCollect data.\nPrepare the input data.\nAnalyze the input data.\nTrain the algorithm.\nTest the algorithm.\nUse it.\n\nIn this course, we will mainly focus on Step 4 as well Step 5. These two steps are where the “core” algorithms lie, depending on the algorithm. We will start from the next Chapter to talk about various Machine Learning algorithms and examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#python-quick-guide",
    "href": "contents/1/intro.html#python-quick-guide",
    "title": "1  Introduction",
    "section": "1.4 Python quick guide",
    "text": "1.4 Python quick guide\n\n1.4.1 Python Notebook\nWe mainly use Jupyter Notebook (.ipynb) to write documents for this course. Currently most main stream Python IDEs support Jupyter Notebook. They are not entirely identical but the differences are not huge and you may choose any you like. In this course we recommand using VS Code. The setup can be found in Appendix Section A.1.\n\n\n1.4.2 Python fundamentals\nWe will put some very basic Python commands here for you to warm up. More advanced Python knowledge will be covered during the rest of the semester. The main reference for this part is [3]. Another referenece is My notes.\n\n1.4.2.1 Indentation\nPython is using indentation to denote code blocks. It is not convienent to write in the first place, but it forces you to write clean, readable code.\nBy the way, the if and for block are actually straightforward.\nif jj &lt; 3:\n    jj = jj \n    print(\"It is smaller than 3.\")\nif jj &lt; 3:\n    jj = jj\nprint(\"It is smaller than 3.\")\nfor i in range(3):\n    i = i + 1\n    print(i)\nfor i in range(3):\n    i = i + 1\nprint(i)\n\n\n\n\n\n\n\n\nPlease tell the differences between the above codes.\n\n\n1.4.2.2 list and dict\nHere are some very basic usage of lists of dictionaries in Python.\n\nnewlist = list()\nnewlist.append(1)\nnewlist.append('hello')\nnewlist\n\n[1, 'hello']\n\n\n\nnewlisttwo = [1, 'hello']\nnewlisttwo\n\n[1, 'hello']\n\n\n\nnewdict = dict()\nnewdict['one'] = 'good'\nnewdict[1] = 'yes'\nnewdict\n\n{'one': 'good', 1: 'yes'}\n\n\n\nnewdicttwo = {'one': 'good', 1: 'yes'}\nnewdicttwo\n\n{'one': 'good', 1: 'yes'}\n\n\n\n\n1.4.2.3 Loop through lists\nWhen creating for loops we may let Python directly loop through lists. Here is an example. The code is almost self-explained.\n\nalist = ['one', 2, 'three', 4]\n\nfor item in alist:\n    print(item)\n\none\n2\nthree\n4\n\n\n\n\n1.4.2.4 Reading files\nThere are a lot of functions that can read files. The basic one is to read any files as a big string. After we get the string, we may parse it based on the structure of the data.\nThe above process sounds complicated. That’s why we have so many different functions reading files. Usually they focus on a certain types of files (e.g. spreadsheets, images, etc..), parse the data into a particular data structure for us to use later.\nI will mention a few examples.\n\n\n\n\n\n\nNotecsv files and excel files\n\n\n\n\n\nBoth of them are spreadsheets format. Usually we use pandas.read_csv and pandas.read_excel both of which are from the package pandas to read these two types of files. When reading excel, package openpyxl is needed.\n\n\n\n\n\n\n\n\n\nNoteimages\n\n\n\n\n\nImages can be treated as matrices, that each entry represents one pixel. If the image is black/white, it is represented by one matrix where each entry represents the gray value. If the image is colored, it is represented by three matrices where each entry represents one color. To use which three colors depends on the color map. rgb is a popular choice.\nIn this course when we need to read images, we usually use matplotlib.pyplot.imread from the package matplotlib or cv.imread from the package opencv.\n\n\n\n\n\n\n1.4.2.5 Writing files\n\npandas.DataFrame.to_csv\npandas.DataFrame.to_excel\nmatplotlib.figure.Figure.savefig()  \n\n\n\n1.4.2.6 Relative paths\nIn this course, when reading and writing files, please keep all the files using relative paths. That is, only write the path starting from the working directory.\n\nExample 1.2 Consider the following tasks:\n\nYour working directory is C:/Users/Xinli/projects/.\nWant to read a file D:/Files/example.csv.\nWant to generate a file whose name is result.csv and put it in a subfoler named foldername.\n\nTo do the tasks, don’t directly run the code pd.read_csv('D:/Files/example.csv'). Instead you should first copy the file to your working directory C:/Users/Xinli/projects/, and then run the following code.\n\nimport pandas as pd\n\ndf = pd.read_csv('example.csv')\ndf.to_csv('foldername/result.csv')\n\nPlease pay attention to how the paths are written.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#data-visualization",
    "href": "contents/1/intro.html#data-visualization",
    "title": "1  Introduction",
    "section": "1.5 Data visualization",
    "text": "1.5 Data visualization\nWe need to visualize the data during toying with models, especialy when we want to keep track of metrcis to evaluate the performance of our model. We will introduce two ways for the visualization.\n\n1.5.1 Naive way\nThe idea is to record the data we want to visualize, and then direct display the data with a certain visualization library. The most popular and default choice is matplotlib. Although there are many customizations, the basic usage is very simple.\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [1, 0, 4, -1, 3]\nz = [-1, 0, 3, -2, 3]\n\nplt.plot(x, y, label='y')\nplt.plot(x, z, label='z')\nplt.legend()\n\n\n\n\n\n\n\n\nSince in most cases in this course we would like to see line plot, this piece of code shows the main usage.\n\nWe need to specify two series of data as x and y respectively.\nWe could show two lines in the same plot.\nIf we add labels, and show legend, legends and labels in the plot will be automatically generated.\n\n\nx = [1, 2, 3, 4, 5]\ny = [1, 0, 4, -1, 3]\nz = [-1, 0, 3, -2, 3]\n\nfig, ax = plt.subplots()\nax.plot(x, y, label='y')\nax.plot(x, z, label='z')\nax.legend()\n\nax.set_xlabel('x')\nax.set_ylabel('y and z')\nax.set_title('Example')\n\nfig.set_size_inches((6, 4))\nfig.set_dpi(100)\nfig.savefig('example.png')\n\n\n\n\n\n\n\n\n\n\n1.5.2 Tensorboard\n\n\nThis section is optional.\n\nInstead of manually recording data and show plots, we could use logging tools for visualization. Similar to libraries, there are many tools of choice. tensorboard is one of simpler tools. tensorboard originally is a tool for tensorflow. It later joins PyTorch and becomes a (relatively independent) tool. Here we will use it with PyTorch since in the second half of the semester we will talk about PyTorch. To install tensorboard after you install PyTorch you could use the following command. More details can be found here. Note that tensorboard depends on matplotlib therefore they have to be installed simutanously.\npip install matplotlib tensorboard\nThe basic idea of tensorboard is that it is logger that record the information you send to it. It can then retrieve the information and show some plots later. Whether the plots are good or not depends on how you structure the data sent to it. There are many customizations. Here we will only discuss one way to use it.\nThe mindset of using tensorboard is as follows. 1. A logger should be initialized to accept your data. 2. For organization, we put the logs in a foler structure. 3. During runs, we send data to the logger. 4. After the run, the logger is closed. 5. We could later run tensorboard to load these logs to see the data we generated during runs.\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter('runs/ex1')\nx = [1, 2, 3, 4, 5]\ny = [1, 0, 4, -1, 3]\nz = [-1, 0, 3, -2, 3]\n\nfor i in range(5):\n    writer.add_scalar('y', y[i], x[i])\n    writer.add_scalar('z', z[i], x[i])\n\nwriter.close()\n\nThis piece of code shows the basic usage of tensorboard.\n\nWe first initialize the logger and put it in the folder runs/ex1. Note that if this folder is not assigned, a folder with random name will be generated. But if the name is assigned like what we do here, the logs will be recorded in this folder everytime we run the code. This is NOT recommended.\nThen we use .add_scalar to send the data we need to the logger one by one. Note that for these scalar data the three arguments are label, value and independent variable.\nAt the end, the logger will be closed and saved to disk.\n\nTo visualize the result, you may type the command in comma line, and tensorboard will then let you know where you can visualize the data.\ntensorboard --logdir=runs/ex1 \nTensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\nClick the link, you will see tensorboard as follows.\n\nHow tensorboard show the data depends on how you structure your data. Here I suggest a way to organize the data you send to tensorboard.\n\nUse date/time and other indicators as the name of each run.\nGroup similar metrics together and record them in a dictionary.\nWhen using a dictionary to send many scalars, use add_scalars instead of add_scalar.\n\n\nfrom torch.utils.tensorboard import SummaryWriter\nfrom datetime import datetime\n\nrunfolder = datetime.now().strftime('%Y%d%m-%H%M%S')\nwriter = SummaryWriter(f'runs/{runfolder}')\nx = [1, 2, 3, 4, 5]\ny = [1, 0, 4, -1, 3]\nz = [-1, 0, 3, -2, 3]\n\nfor i in range(5):\n    scalars = {\n        'y': y[i],\n        'z': z[i]\n    }\n    writer.add_scalars('scalars', scalars, x[i])\n\nwriter.close()\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the plots from tensorboard and matplotlib are a little different. The reason is that tensorboard automatically smooth the curve. You can use the cmoothing factor to control the effect. When you change it to be 0, you will get exactly the same plot.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#datasets",
    "href": "contents/1/intro.html#datasets",
    "title": "1  Introduction",
    "section": "1.6 Datasets",
    "text": "1.6 Datasets\nYou should have a dataset to build a machine learning model. The datasets usually come in different formats.\n\nYou are provided with the dataset files.\nYou load the dataset from public repository.\n\nAfter you get the dataset, there should be some preprocessing steps to change the dataset into the format we need.\n\nhuggingface\nfrom packages",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/1/intro.html#exercises",
    "href": "contents/1/intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.7 Exercises",
    "text": "1.7 Exercises\nThese exercises are from [4], [1] and [3].\n\n1.7.1 Python Notebook\n\nExercise 1.1 (Hello World!) Please set up a Python Notebook environment and type print('Hello World!').\n\n\nExercise 1.2 Please set up a Python Notebook and start a new virtual environment and type print('Hello World!').\n\n\n\n1.7.2 Basic Python\n\nExercise 1.3 (Play with lists) Please complete the following tasks.\n\nWrite a for loop to print values from 0 to 4.\nCombine two lists ['apple', 'orange'] and ['banana'] using +.\nSort the list ['apple', 'orange', 'banana'] using sorted().\n\n\n\n\nExercise 1.4 (Play with list, dict and pandas.) Please complete the following tasks.\n\nCreate a new dictionary people with two keys name and age. The values are all empty list.\nAdd Tony to the name list in people.\nAdd Harry to the name list in people.\nAdd number 100 to the age list in people.\nAdd number 10 to the age list in people.\nFind all the keys of people and save them into a list namelist.\nConvert the dictionary people to a Pandas DataFrame df.\n\n\n\n\nExercise 1.5 (The dataset iris)  \n\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nPlease explore this dataset.\n\nPlease get the features for iris and save it into X as an numpy array.\nWhat is the meaning of these features?\nPlease get the labels for iris and save it into y as an numpy array.\nWhat is the meaning of labels?\n\n\n\n\nExercise 1.6 (Play with Pandas) Please download the Titanic data file from here. Then follow the instructions to perform the required tasks.\n\nUse pandas.read_csv to read the dataset and save it as a dataframe object df.\nChange the values of the Sex column that male is 0 and female is 1.\nPick the columns Pclass, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard and Fare and transform them into a 2-dimensional numpy.ndarray, and save it as X.\nPick the column Survived and transform it into a 1-dimensional numpy.ndarray and save it as y.\n\n\n\n\n\n\n\n[1] Géron, A. (2019). Hands-on machine learning with scikit-learn, keras, and TensorFlow concepts, tools, and techniques to build intelligent systems: Concepts, tools, and techniques to build intelligent systems. O’Reilly Media.\n\n\n[2] Chollet, F. (2021). Deep learning with python, second edition. MANNING PUBN.\n\n\n[3] Harrington, P. (2012). Machine learning in action. Manning Publications.\n\n\n[4] Klosterman, S. (2021). Data science projects with python: A case study approach to gaining valuable insights from real data with machine learning. Packt Publishing, Limited.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html",
    "href": "contents/2/intro.html",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1 k-Nearest Neighbors Algorithm (k-NN)\nThis algorithm is different from other algorithms covered in this course, that it doesn’t really extract features from the data. However, since its idea is easy to understand, we use it as our first step towards machine learning world.\nSimilar to other algorithms, we will only cover the beginning part of the algorithm. All later upgrades of the algorithms are left for yourselves to learn.\nReferences: [1].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "href": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "2.1.1 Ideas\nAssume that we have a set of labeled data \\(\\{(X_i, y_i)\\}\\) where \\(y_i\\) denotes the label. Given a new data \\(X\\), how do we determine the label of it?\nk-NN algorithm starts from a very straightforward idea. We use the distances from the new data point \\(X\\) to the known data points to identify the label. If \\(X\\) is closer to \\(y_i\\) points, then we will label \\(X\\) as \\(y_i\\).\nLet us take cities and countries as an example. New York and Los Angeles are U.S cities, and Beijing and Shanghai are Chinese cities. Now we would like to consider Tianjin and Russellville. Do they belong to China or U.S? We calculate the distances from Tianjin (resp. Russellville) to all four known cities. Since Tianjin is closer to Beijing and Shanghai comparing to New York and Los Angeles, we classify Tianjin as a Chinese city. Similarly, since Russellville is closer to New York and Los Angeles comparing to Beijing and Shanghai, we classify it as a U.S. city.\n\n\n\n\n\n\n\nG\n\n\n\nBeijing\n\nBeijing\n\n\n\nShanghai\n\nShanghai\n\n\n\nTianjin\n\nTianjin\n\n\n\nTianjin-&gt;Beijing\n\n\ncloser\n\n\n\nTianjin-&gt;Shanghai\n\n\ncloser   \n\n\n\nNew York\n\nNew York\n\n\n\nTianjin-&gt;New York\n\n\n far away\n\n\n\nLos Angelis\n\nLos Angelis\n\n\n\nTianjin-&gt;Los Angelis\n\n\n far away\n\n\n\nRussellville\n\nRussellville\n\n\n\nRussellville-&gt;Beijing\n\n\nfar away \n\n\n\nRussellville-&gt;Shanghai\n\n\nfar away\n\n\n\nRussellville-&gt;New York\n\n\ncloser  \n\n\n\nRussellville-&gt;Los Angelis\n\n\ncloser\n\n\n\n\n\n\n\n\nThis naive example explains the idea of k-NN. Here is a more detailed description of the algorithm.\n\n\n2.1.2 The Algorithm\n\n\n\n\n\n\nNotek-NN Classifier\n\n\n\nInputs: Given the training data set \\(\\{(X_i, y_i)\\}\\) where \\(X_i=(x_i^1,x_i^2,\\ldots,x_i^n)\\) represents \\(n\\) features and \\(y_i\\) represents labels. Given a new data point \\(\\tilde{X}=(\\tilde{x}^1,\\tilde{x}^2,\\ldots,\\tilde{x}^n)\\).\nOutputs: Want to find the best label for \\(\\tilde{X}\\).\n\nCompute the distance from \\(\\tilde{X}\\) to each \\(X_i\\).\nSort all these distances from the nearest to the furthest.\nFind the nearest \\(k\\) data points.\nDetermine the labels for each of these \\(k\\) nearest points, and compute the frenqucy of each labels.\nThe most frequent label is considered to be the label of \\(\\tilde{X}\\).\n\n\n\n\n\n2.1.3 Details\n\nThe distance between two data points are defined by the Euclidean distance:\n\n\\[\ndist\\left((x^j_i)_{j=1}^n, (\\tilde{x}^j)_{j=1}^n\\right)=\\sqrt{\\sum_{j=1}^n(x^j_i-\\tilde{x}^j)^2}.\n\\]\n\nUsing linear algebra notations:\n\n\\[\ndist(X_i,\\tilde{X})=\\sqrt{(X_i-\\tilde{X})\\cdot(X_i-\\tilde{X})}.\n\\]\n\nAll the distances are stored in a \\(1\\)-dim numpy array, and we will combine it together with another \\(1\\)-dim array that store the labels of each point.\n\n\n\n2.1.4 The codes\n\n\nThis part is optional.\n\n\nargsort\nunique\nargmax\n\n\nimport numpy as np\n\ndef classify_kNN(inX, X, y, k=5):\n    # compute the distance between each row of X and Xmat\n    Dmat = np.sqrt(((inX - X)**2).sum(axis=1))\n    # sort by distance\n    k = min(k, Dmat.shape[0])\n    argsorted = Dmat.argsort()[:k]\n    relatedy = y[argsorted]\n    # count the freq. of the first k labels\n    labelcounts = np.unique(relatedy, return_counts=True)\n    # find the label with the most counts\n    label = labelcounts[0][labelcounts[1].argmax()]\n    return label\n\n\n\n\n2.1.5 sklearn packages\nYou may also directly use the kNN function KNeighborsClassifier packaged in sklearn.neighbors. You may check the description of the function online from here.\nThere are many ways to modify the kNN algorithm. What we just mentioned is the simplest idea. It is correspondent to the argument weights='uniform', algorithm='brute and metric='euclidean'. However due to the implementation details, the results we got from sklearn are still a little bit different from the results produced by our naive codes.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=10, weights='uniform',\n                           algorithm='brute', metric='euclidean')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n\n\n2.1.6 Normalization\nDifferent features may have different scales. It might be unfair for those features that have small scales. Therefore usually it is better to rescale all the features to make them have similar scales. After examining all the data, we find the minimal value minVal and the range ranges for each column. The normalization formula is:\n\\[\nX_{norm} = \\frac{X_{original}-minVal}{ranges}.\n\\]\nWe could also convert the normalized number back to the original value by\n\\[\nX_{original} = X_{norm} \\times ranges + minVal.\n\\]\n\n\nThe sample codes are listed below. This part is optional.\n\n\nimport numpy as np\n\ndef encodeNorm(X, parameters=None):\n    # parameters contains minVals and ranges\n    if parameters is None:\n        minVals = np.min(X, axis=0)\n        maxVals = np.max(X, axis=0)\n        ranges = np.maximum(maxVals - minVals, np.ones(minVals.size))\n        parameters = {'ranges': ranges, 'minVals': minVals}\n    else:\n        minVals = parameters['minVals']\n        ranges = parameters['ranges']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xnorm = (X - Nmat)/ranges\n    return (Xnorm, parameters)\n\n\ndef decodeNorm(X, parameters):\n    # parameters contains minVals and ranges\n    ranges = parameters['ranges']\n    minVals = parameters['minVals']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xoriginal = X * ranges + Nmat\n    return Xoriginal\n\n\nYou could use MinMaxScaler from sklearn.preprocessing to achive the goal. The API is very similar to an estimator.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler()\nmm.fit(X)\nX_norm = mm.transform(X)\n\nThe last two lines can be combined into\n\nX_norm = mm.fit_transform(X)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "href": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.2 k-NN Project 1: iris Classification",
    "text": "2.2 k-NN Project 1: iris Classification\nThis data is from sklearn.datasets. This dataset consists of 3 different types of irises’ petal / sepal length / width, stored in a \\(150\\times4\\) numpy.ndarray. We already explored the dataset briefly in the previous chapter. This time we will try to use the feature provided to predict the type of the irises. For the purpose of plotting, we will only use the first two features: sepal length and sepal width.\n\n2.2.1 Explore the dataset\nWe first load the dataset.\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target\n\nThen we would like to split the dataset into trainning data and test data. Here we are going to use sklearn.model_selection.train_test_split function. Besides the dataset, we should also provide the propotion of the test set comparing to the whole dataset. We will choose test_size=0.1 here, which means that the size of the test set is 0.1 times the size of the whole dataset. stratify=y means that when split the dataset we want to split respects the distribution of labels in y.\nThe split will be randomly. You may set the argument random_state to be a certain number to control the random process. If you set a random_state, the result of the random process will stay the same. This is for reproducible output across multiple function calls.\nAfter we get the training set, we should also normalize it. All our normalization should be based on the training set. When we want to use our model on some new data points, we will use the same normalization parameters to normalize the data points in interests right before we apply the model. Here since we mainly care about the test set, we could normalize the test set at this stage.\nNote that in the following code, we mainly use the implementation from sklearn.  \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n\nmm = MinMaxScaler()\nX_train_norm = mm.fit_transform(X_train)\nX_test_norm = mm.transform(X_test)\n\nBefore we start to play with k-NN, let us look at the data first. Since we only choose two features, it is able to plot these data points on a 2D plane, with different colors representing different classes.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot the scatter plot.\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\n# Generate legends.\nlabels = ['setosa', 'versicolor', 'virginica']\n_ = fig.legend(handles=scatter.legend_elements()[0], labels=labels,\n               loc=\"right\", title=\"Labels\")\n\n\n\n\n\n\n\n\n\n\n2.2.2 Apply our k-NN model\nNow let us apply k-NN to this dataset. Since our data is prepared, what we need to do is directly call the functions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 10\nclf = KNeighborsClassifier(n_neighbors, weights=\"uniform\", metric=\"euclidean\",\n                           algorithm='brute')\nclf.fit(X_train_norm, y_train)\ny_pred_sk = clf.predict(X_test_norm)\n\nacc = np.mean(y_pred_sk == y_test)\nacc\n\nnp.float64(0.7333333333333333)\n\n\n\n\n2.2.3 Using data pipeline\nWe may organize the above process in a neater way. After we get a data, the usual process is to apply several transforms to the data before we really get to the model part. Using terminolgies from sklearn, the former are called transforms, and the latter is called an estimator. In this example, we have exactly one tranform which is the normalization. The estimator here we use is the k-NN classifier.\nsklearn provides a standard way to write these codes, which is called pipeline. We may chain the transforms and estimators in a sequence and let the data go through the pipeline. In this example, the pipeline contains two steps: 1. The normalization transform sklearn.preprocessing.MinMaxScaler. 2. The k-NN classifier sklearn.neighbors.KNeighborsClassifier. This is the same one as we use previously.\nThe code is as follows. It is a straightforward code. Note that the () after the class in each step of steps is very important. The codes cannot run if you miss it.\nAfter we setup the pipeline, we may use it as other estimators since it is an estimator. Here we may also use the accuracy function provided by sklearn to perform the computation. It is essentially the same as our acc computation.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\nn_neighbors = 10\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.7333333333333333\n\n\nOnce a pipeline is set, you may use step name with TWO underscores __ with parameter name to get access to a specific parameter. Please check the following code.\n\npipe.get_params()['knn__n_neighbors']\n\n10\n\n\n\npipe.set_params(knn__n_neighbors=5)\npipe.get_params()['knn__n_neighbors']\n\n5\n\n\n\n\n2.2.4 Visualize the Decision boundary\n\n\nThis section is optional.\n\nUsing the classifier we get above, we are able to classify every points on the plane. This enables us to draw the following plot, which is called the Decision boundary. It helps us to visualize the relations between features and the classes.\nWe use DecisionBoundaryDisplay from sklearn.inspection to plot the decision boundary. The function requires us to have a fitted classifier. We may use the classifier pipe we got above. Note that this classifier should have some build-in structures that our classify_kNN function doesn’t have. We may rewrite our codes to make it work, but this goes out of the scope of this section. This is supposed to be Python programming exercise. We will talk about it in the future if we have enough time.\nWe first plot the dicision boundary using DecisionBoundaryDisplay.from_estimator. Then we plot the points from X_test. From the plot it is very clear which points are misclassified.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n            pipe, \n            X_train,\n            response_method=\"predict\",\n            plot_method=\"pcolormesh\",\n            xlabel=iris.feature_names[0],\n            ylabel=iris.feature_names[1],\n            alpha=0.5)\ndisp.ax_.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor=\"k\")\ndisp.figure_.set_size_inches((10,7))\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 k-Fold Cross-Validation\nPreviously we perform a random split and test our model in this case. What would happen if we fit our model on another split? We might get a different accuracy score. So in order to evaluate the performance of our model, it is natual to consider several different split and compute the accuracy socre for each case, and combine all these socres together to generate an index to indicate whehter our model is good or bad. This naive idea is called k-Fold Cross-Validation.\nThe algorithm is described as follows. We first randomly split the dataset into k groups of the same size. We use one of them as the test set, and the rest together forming the training set, and use this setting to get an accuracy score. We did this for each group to be chosen as the test set. Then the final score is the mean.\n\n\n\n\n\n\nNoteKFold\n\n\n\n\n\nKFold from sklearn.model.selection is used to split the dataset into k groups and in each iteration to chooose one as the validation set.\n\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5)\n\nfor train_idx, val_idx in kf.split(range(10)):\n    print(train_idx, val_idx)\n\n[2 3 4 5 6 7 8 9] [0 1]\n[0 1 4 5 6 7 8 9] [2 3]\n[0 1 2 3 6 7 8 9] [4 5]\n[0 1 2 3 4 5 8 9] [6 7]\n[0 1 2 3 4 5 6 7] [8 9]\n\n\n\nI only put range(10) in kf.split since it only needs to work with the index. If a dataset is put there, the output is still the index of which data is in the training set and which is in the validation set.\nIf you want to randomize the selection, when set up KFold we could add an argument shuffle=True. In this case, we may use random_state to control the outcome provide reproducing ability.\n\nLet us see an example for our data.\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.base import clone\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\ncv_scores = []\nfor train_idx, val_idx in kf.split(X):\n    pipe_tmp = clone(pipe)\n    pipe_tmp.fit(X[train_idx], y[train_idx])\n    cv_scores.append(pipe_tmp.score(X[val_idx], y[val_idx]))\ncv_scores\n\n[0.8333333333333334,\n 0.7333333333333333,\n 0.7333333333333333,\n 0.7,\n 0.7666666666666667]\n\n\n\nnp.mean(cv_scores)\n\nnp.float64(0.7533333333333333)\n\n\nNote that here sklearn.base.clone is used to initialize an unfitted model which has the same hyperpamaters as pipe.\n\n\n\n\n\n\n\n\n\nNotecross_validate\n\n\n\n\n\nKFold is too “manual”. We may use cross_validate to autmate the above process. Note that depending on the arguments given cross_validate may be implemented by KFold.\n\nfrom sklearn.model_selection import cross_validate\n\ncv_result = cross_validate(pipe, X, y, cv=5, scoring='accuracy')\ncv_result\n\n{'fit_time': array([0.0030055 , 0.00227547, 0.00099826, 0.00199318, 0.00199986]),\n 'score_time': array([0.00800037, 0.0065515 , 0.0082345 , 0.00600076, 0.0079999 ]),\n 'test_score': array([0.73333333, 0.8       , 0.76666667, 0.9       , 0.73333333])}\n\n\nAnd you may only see the scores if this is the only thing that interests you.\n\ncv_result['test_score']\n\narray([0.73333333, 0.8       , 0.76666667, 0.9       , 0.73333333])\n\n\n\nYou may choose different scoring methods. More info can be found in the document.\nIf cv=5, KFold(5, shuffle=False) is applied here. If you prefer random split, you may directly use KFold here.\n\n\ncv_result = cross_validate(pipe, X, y, scoring='accuracy',\n                           cv=KFold(5, shuffle=True, random_state=1))\ncv_result['test_score']\n\narray([0.83333333, 0.73333333, 0.73333333, 0.7       , 0.76666667])\n\n\nYou may compare this result with the previous one and the one in KFold section. Of course, the cv score is usually the mean of all the scores.\n\ncv_result['test_score'].mean()\n\nnp.float64(0.7533333333333333)\n\n\n\n\n\n\n\n\n\n\n\nNotecross_val_score\n\n\n\n\n\nThis is a faster way to directly get cv_result['test_score'] in cross_validate section. The argument about cv and scoring are the same as cross_validate.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(pipe, X, y, cv=KFold(5, shuffle=True, random_state=1))\ncv_scores\n\narray([0.83333333, 0.73333333, 0.73333333, 0.7       , 0.76666667])\n\n\n\ncv_scores.mean()\n\nnp.float64(0.7533333333333333)\n\n\n\n\n\n\n\n2.2.6 Choosing a k value\nIn the previous example we choose k to be 10 as an example. To choose a k value we usually run some test by trying different k and choose the one with the best performance. In this case, best performance means the highest cross-validation score.\n\n\n\n\n\n\nNoteGrid search\n\n\n\n\n\nsklearn.model_selection.GridSearchCV provides a way to do this directly. We only need to setup the esitimator, the metric (which is the cross-validation score in this case), and the hyperparameters to be searched through, and GridSearchCV will run the search automatically.\nWe let k go from 1 to 100. The code is as follows.\nNote that parameters is where we set the search space. It is a dictionary. The key is the name of the estimator plus double _ and then plus the name of the parameter.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n35\n\n\nAfter we fit the data, the best_estimator_.get_params() can be printed. It tells us that it is best to use 31 neibhours for our model. We can directly use the best estimator by calling clf.best_estimator_.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nnp.mean(cv_scores)\n\nnp.float64(0.82)\n\n\nThe cross-validation score using k=31 is calculated. This serves as a benchmark score and we may come back to dataset using other methods and compare the scores.\n\n\n\n\n\n\n\n\n\nNotePlot the curve\n\n\n\n\n\nGrid search can only give us a single number that has the best cross validation score. However there are many cases that the number might not be really the best. So usually we also want to see the result for all k. The best way to display all results simutanously is to plot the curve.\n\nimport matplotlib.pyplot as plt\n\nn_list = list(range(1, 101))\ncv_scores = []\nfor k in n_list:\n    pipe_tmp = clone(pipe)\n    pipe_tmp.set_params(knn__n_neighbors=k)\n    cv_scores.append(cross_val_score(pipe_tmp, X, y, cv=5).mean())\n\nplt.plot(cv_scores)\n\n\n\n\n\n\n\n\nFrom this plot, combining with the best cv score happens at k=31, we could make our final decision about which k to choose.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "href": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.3 k-NN Project 2: Dating Classification",
    "text": "2.3 k-NN Project 2: Dating Classification\nThe data can be downloaded from here.\n\n2.3.1 Background\nHelen dated several people and rated them using a three-point scale: 3 is best and 1 is worst. She also collected data from all her dates and recorded them in the file attached. These data contains 3 features:\n\nNumber of frequent flyer miles earned per year\nPercentage of time spent playing video games\nLiters of ice cream consumed per week\n\nWe would like to predict her ratings of new dates when we are given the three features.\nThe data contains four columns, while the first column refers to Mileage, the second Gamingtime, the third Icecream and the fourth Rating.\n\n\n2.3.2 Look at Data\nWe first load the data and store it into a DataFrame.\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('datingTestSet2.txt', sep='\\t', header=None)\ndf.head()\n\nTo make it easier to read, we would like to change the name of the columns.\n\ndf = df.rename(columns={0: \"Mileage\", 1: \"Gamingtime\", 2: 'Icecream', 3: 'Rating'})\ndf.head()\n\n\n\n\n\n\n\n\nMileage\nGamingtime\nIcecream\nRating\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\nSince now we have more than 2 features, it is not suitable to directly draw scatter plots. We use seaborn.pairplot to look at the pairplot. From the below plots, before we apply any tricks, it seems that Milegae and Gamingtime are better than Icecream to classify the data points.\n\nimport seaborn as sns\nsns.pairplot(data=df, hue='Rating')\n\n\n\n\n\n\n\n\n\n\n2.3.3 Applying kNN\nSimilar to the previous example, we will apply both methods for comparisons.\n\nfrom sklearn.model_selection import train_test_split\nX = np.array(df[['Mileage', 'Gamingtime', 'Icecream']])\ny = np.array(df['Rating'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40, stratify=y)\n\n\n# Using sklearn.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\naccuracy_score(y_pipe, y_test)\n\n0.93\n\n\n\n\n2.3.4 Choosing k Value\nSimilar to the previous section, we can run tests on k value to choose one to be used in our model using GridSearchCV.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters, cv=5)\nclf.fit(X_train, y_train)\nclf.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n12\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\n\nn_list = list(range(1, 101))\ncv_scores = []\nfor k in n_list:\n    pipe_tmp = clone(pipe)\n    pipe_tmp.set_params(knn__n_neighbors=k)\n    cv_scores.append(cross_val_score(pipe_tmp, X_train, y_train, cv=5).mean())\nplt.plot(cv_scores)\n\n\n\n\n\n\n\n\nFrom this result, in this case the best k is 12. The corresponding test score is\n\nclf.score(X_test, y_test)\n\n0.93",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "href": "contents/2/intro.html#k-nn-project-3-handwritten-recognition",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.4 k-NN Project 3: Handwritten recognition",
    "text": "2.4 k-NN Project 3: Handwritten recognition\nWe would like to let the machine recognize handwritten digits. The dataset is MNIST comeing from the MNIST database. Now we apply kNN algrotithm to it.\n\n2.4.1 Dataset description\nEvery digit is stored as a \\(28\\times28\\) picture. This is a \\(28\\times28\\) matrix. Every entry represents a gray value of the corresponding pixel, whose value is from 0 to 255. The label of each matrix is the digit it represents. Note that the dataset provided is already splitted into a training set and a test set.\nThe dataset can be loaded following the instruction.\n\nfrom datasets import load_dataset\nimport numpy as np\nimport itertools\n\ndef pil_to_array(data):\n    data['image'] = np.array(data['image'])\n    return data\n\nmnist_train = load_dataset(\"ylecun/mnist\", split='train').take(600)\nmnist_test = load_dataset(\"ylecun/mnist\", split='test').take(100)\n\nmnist_train_processed = mnist_train.map(pil_to_array)\nmnist_test_processed = mnist_test.map(pil_to_array)\n\nX_train = np.array(mnist_train_processed['image']).reshape(-1, 784)\ny_train = np.array(mnist_train_processed['label']).reshape(-1)\nX_test = np.array(mnist_test_processed['image']).reshape(-1, 784)\ny_test = np.array(mnist_test_processed['label']).reshape(-1)\n\nNote that one of the purpose to load the data in streaming mode is that the dataset is big and it is not wise to load everything all together. However this is the only way to train a KNN model since all it does is to memorize everything. In the future with other models we may want to load the image one by one with the streaming mode.\nAlso due to the issue of large dataset, I only choose the first 600/100 images from the original dataset. This is implemented by the .take method when loading the dataset.\n\nnp.unique(y_train, return_counts=True)\n\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n array([58, 79, 64, 59, 59, 51, 54, 62, 49, 65]))\n\n\nAlthough not optimal, all digits are presented, and the distributions are relatively equal. So we will use this slice of the original dataset. In reality, if possible it is always better to use all data provided to you.\n\n\n2.4.2 Apply k-NN\nLike the previous two examples, we now try to apply the k-NN algorithm to classify these handwritten digits. Note that the original dataset is huge and the processing time is very slow. However since we only choose 600/100 images, we could still run all our tricks.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors=5))]\npipe = Pipeline(steps=steps)\nn_list = list(range(1, 11))\n\ncv_score = []\nfor k in n_list:\n    pipe_tmp = clone(pipe)\n    pipe_tmp.set_params(knn__n_neighbors=k)\n    cv_score.append(cross_val_score(pipe_tmp, X_train, y_train, cv=5).mean())\nplt.plot(n_list, cv_score)\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n\ngs = GridSearchCV(pipe, param_grid=dict(knn__n_neighbors=n_list), cv=5)\ngs.fit(X_train, y_train)\ngs.best_params_\n\n{'knn__n_neighbors': 3}\n\n\nThe best k is 3 for this degenerated dataset. The corresponding test score is\n\ngs.score(X_test, y_test)\n\n0.82",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/2/intro.html#exercises-and-projects",
    "href": "contents/2/intro.html#exercises-and-projects",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.5 Exercises and Projects",
    "text": "2.5 Exercises and Projects\n\nExercise 2.1 Handwritten example :label: ex2handwritten Consider the 1-dimensional data set shown below.\n\n\n\n\n\n\n\nx\n1.5\n2.5\n3.5\n4.5\n5.0\n5.5\n5.75\n6.5\n7.5\n10.5\n\n\n\n\ny\n+\n+\n-\n-\n-\n+\n+\n-\n+\n+\n\n\n\n\n\nPlease use the data to compute the class of \\(x=5.5\\) according to \\(k=1\\), \\(3\\), \\(6\\) and \\(9\\). Please compute everything by hand.\n\n\nExercise 2.2 (Titanic) Please download the titanic dataset from here. This is the same dataset from what you dealt with in Chapter 1 Exercises. Therefore you may use the same way to prepare the data.\nPlease analyze the dataset and build a k-NN model to predict whether someone is survived or not. Note that you have to pick k at the end.\n\n\n\n\n\n[1] Harrington, P. (2012). Machine learning in action. Manning Publications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>k-Nearest Neighbors algorithm (k-NN)</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html",
    "href": "contents/3/intro.html",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1 Gini impurity\nGiven a dataset with labels, the decision tree algorithm firstly trys to split the whole dataset into two different groups, based on some speicific features. Choose which feature to use and set the threshold for the split are done.\nTo split a dataset, we need a metric to tell whether the split is good or not. The two most popular metrics that are used are Gini impurity and Entropy. The two metrics don’t have essential differences, that the results obtained by applying each metric are very similar to each other. Therefore we will only focus on Gini impurity since it is slightly easier to compute and slightly easier to explain.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#gini-impurity",
    "href": "contents/3/intro.html#gini-impurity",
    "title": "3  Decision Trees",
    "section": "",
    "text": "3.1.1 Motivation and Definition\nAssume that we have a dataset of totally \\(n\\) objects, and these objects are divided into \\(k\\) classes. The \\(i\\)-th class has \\(n_i\\) objects. Then if we randomly pick an object, the probability to get an object belonging to the \\(i\\)-th class is\n\\[\np_i=\\frac{n_i}{n}\n\\]\nIf we then guess the class of the object purely based on the distribution of each class, the probability that our guess is incorrect is\n\\[\n1-p_i = 1-\\frac{n_i}{n}.\n\\]\nTherefore, if we randomly pick an object that belongs to the \\(i\\)-th class and randomly guess its class purely based on the distribution but our guess is wrong, the probability that such a thing happens is\n\\[\np_i(1-p_i).\n\\]\nConsider all classes. The probability at which any object of the dataset will be mislabelled when it is randomly labeled is the sum of the probability described above:\n\\[\n\\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2.\n\\]\nThis is the definition formula for the Gini impurity.\n\nDefinition 3.1 The Gini impurity is calculated using the following formula\n\\[\nGini = \\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2,\n\\] where \\(p_i\\) is the probability of class \\(i\\).\n\nThe way to understand Gini impurity is to consider some extreme examples.\n\nExample 3.1 Assume that we only have one class. Therefore \\(k=1\\), and \\(p_1=1\\). Then the Gini impurity is\n\\[\nGini = 1-1^2=0.\n\\] This is the minimum possible Gini impurity. It means that the dataset is pure: all the objects contained are of one unique class. In this case, we won’t make any mistakes if we randomly guess the label.\n\n\nExample 3.2 Assume that we have two classes. Therefore \\(k=2\\). Consider the distribution \\(p_1\\) and \\(p_2\\). We know that \\(p_1+p_2=1\\). Therefore \\(p_2=1-p_1\\). Then the Gini impurity is\n\\[\nGini(p_1) = 1-p_1^2-p_2^2=1-p_1^2-(1-p_1)^2=2p_1-2p_1^2.\n\\] When \\(0\\leq p_1\\leq 1\\), this function \\(Gini(p_1)\\) is between \\(0\\) and \\(0.5\\). - It gets \\(0\\) when \\(p_1=0\\) or \\(1\\). In these two cases, the dataset is still a one-class set since the size of one class is \\(0\\). - It gets \\(0.5\\) when \\(p_1=0.5\\). This means that the Gini impurity is maximized when the size of different classes are balanced.\n\n\n\n3.1.2 Algorithm\n\n\n\n\n\n\nNoteAlgorithm: Gini impurity\n\n\n\nInputs A dataset \\(S=\\{data=[features, label]\\}\\) with labels.\nOutputs The Gini impurity of the dataset.\n\nGet the size \\(n\\) of the dataset.\nGo through the label list, and find all unique labels: \\(uniqueLabelList\\).\nGo through each label \\(l\\) in \\(uniqueLabelList\\) and count how many elements belonging to the label, and record them as \\(n_l\\).\nUse the formula to compute the Gini impurity:\n\\[\nGini = 1-\\sum_{l\\in uniqueLabelList}\\left(\\frac{n_l}{n}\\right)^2.\n\\]\n\n\n\n\n\nThe sample manual codes are optional.\n\n\nimport pandas as pd\ndef gini(S):\n    N = len(S)\n    y = S[:, -1].reshape(N)\n    gini = 1 - ((pd.Series(y).value_counts()/N)**2).sum()\n    return gini",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#cart-algorithms",
    "href": "contents/3/intro.html#cart-algorithms",
    "title": "3  Decision Trees",
    "section": "3.2 CART Algorithms",
    "text": "3.2 CART Algorithms\n\n3.2.1 Ideas\nConsider a labeled dataset \\(S\\) with totally \\(m\\) elements. We use a feature \\(k\\) and a threshold \\(t_k\\) to split it into two subsets: \\(S_l\\) with \\(m_l\\) elements and \\(S_r\\) with \\(m_r\\) elements. Then the cost function of this split is\n\\[\nJ(k, t_k)=\\frac{m_l}mGini(S_l)+\\frac{m_r}{m}Gini(S_r).\n\\] It is not hard to see that the more pure the two subsets are the lower the cost function is. Therefore our goal is find a split that can minimize the cost function.\n\n\n\n\n\n\nNoteAlgorithm: Split the Dataset\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\).\nOutputs A best split \\((k, t_k)\\).\n\nFor each feature \\(k\\):\n\nFor each value \\(t\\) of the feature:\n\nSplit the dataset \\(S\\) into two subsets, one with \\(k\\leq t\\) and one with \\(k&gt;t\\).\nCompute the cost function \\(J(k,t)\\).\nCompare it with the current smallest cost. If this split has smaller cost, replace the current smallest cost and pair with \\((k, t)\\).\n\n\nReturn the pair \\((k,t_k)\\) that has the smallest cost function.\n\n\n\nWe then use this split algorithm recursively to get the decision tree.\n\n\n\n\n\n\nNoteClassification and Regression Tree, CART\n\n\n\nInputs Given a labeled dataset \\(S=\\{[features, label]\\}\\) and a maximal depth max_depth.\nOutputs A decision tree.\n\nStarting from the original dataset \\(S\\). Set the working dataset \\(G=S\\).\nConsider a dataset \\(G\\). If \\(Gini(G)\\neq0\\), split \\(G\\) into \\(G_l\\) and \\(G_r\\) to minimize the cost function. Record the split pair \\((k, t_k)\\).\nNow set the working dataset \\(G=G_l\\) and \\(G=G_r\\) respectively, and apply the above two steps to each of them.\nRepeat the above steps, until max_depth is reached.\n\n\n\n\n\nThe manual sample code is optional.\n\n\ndef split(G):\n    m = G.shape[0]\n    gmini = gini(G)\n    pair = None\n    if gini(G) != 0:\n        numOffeatures = G.shape[1] - 1\n        for k in range(numOffeatures):\n            for t in range(m):\n                Gl = G[G[:, k] &lt;= G[t, k]]\n                Gr = G[G[:, k] &gt; G[t, k]]\n                gl = gini(Gl)\n                gr = gini(Gr)\n                ml = Gl.shape[0]\n                mr = Gr.shape[0]\n                g = gl*ml/m + gr*mr/m\n                if g &lt; gmini:\n                    gmini = g\n                    pair = (k, G[t, k])\n                    Glm = Gl\n                    Grm = Gr\n        res = {'split': True,\n               'pair': pair,\n               'sets': (Glm, Grm)}\n    else:\n        res = {'split': False,\n               'pair': pair,\n               'sets': G}\n    return res\n\nFor the purpose of counting labels, we also write a code to do so.\n\nimport pandas as pd\ndef countlabels(S):\n    y = S[:, -1].reshape(S.shape[0])\n    labelCount = dict(pd.Series(y).value_counts())\n    return labelCount",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "href": "contents/3/intro.html#decision-tree-project-1-the-iris-dataset",
    "title": "3  Decision Trees",
    "section": "3.3 Decision Tree Project 1: the iris dataset",
    "text": "3.3 Decision Tree Project 1: the iris dataset\nWe are going to use the Decision Tree model to study the iris dataset. This dataset has already studied previously using k-NN. Again we will only use the first two features for visualization purpose.\n\n3.3.1 Initial setup\nSince the dataset will be splitted, we will put X and y together as a single variable S. In this case when we split the dataset by selecting rows, the features and the labels are still paired correctly.\nWe also print the labels and the feature names for our convenience.\n\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\niris = load_iris()\nX = iris.data[:, 2:]\ny = iris.target\ny = y.reshape((y.shape[0],1))\nS = np.concatenate([X,y], axis=1)\n\nprint(iris.target_names)\nprint(iris.feature_names)\n\n['setosa' 'versicolor' 'virginica']\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\n\n\n3.3.2 Use package sklearn\nNow we would like to use the decision tree package provided by sklearn. The process is straightforward. The parameter random_state=40 will be discussed {ref}later&lt;note-random_state&gt;, and it is not necessary in most cases.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(random_state=40, min_samples_split=2)\nclf.fit(X, y)\n\nDecisionTreeClassifier(random_state=40)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=40) \n\n\nsklearn provide a way to automatically generate the tree view of the decision tree. The code is as follows.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(3, 3), dpi=200)\nplot_tree(clf, filled=True, impurity=True, node_ids=True)\n\n[Text(0.5, 0.9166666666666666, 'node #0\\nx[1] &lt;= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n Text(0.4090909090909091, 0.75, 'node #1\\ngini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n Text(0.4545454545454546, 0.8333333333333333, 'True  '),\n Text(0.5909090909090909, 0.75, 'node #2\\nx[1] &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n Text(0.5454545454545454, 0.8333333333333333, '  False'),\n Text(0.36363636363636365, 0.5833333333333334, 'node #3\\nx[0] &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n Text(0.18181818181818182, 0.4166666666666667, 'node #4\\nx[1] &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'),\n Text(0.09090909090909091, 0.25, 'node #5\\ngini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'),\n Text(0.2727272727272727, 0.25, 'node #6\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.5454545454545454, 0.4166666666666667, 'node #7\\nx[1] &lt;= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'),\n Text(0.45454545454545453, 0.25, 'node #8\\ngini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'),\n Text(0.6363636363636364, 0.25, 'node #9\\nx[0] &lt;= 5.45\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'),\n Text(0.5454545454545454, 0.08333333333333333, 'node #10\\ngini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'),\n Text(0.7272727272727273, 0.08333333333333333, 'node #11\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.8181818181818182, 0.5833333333333334, 'node #12\\nx[0] &lt;= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'),\n Text(0.7272727272727273, 0.4166666666666667, 'node #13\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'),\n Text(0.9090909090909091, 0.4166666666666667, 'node #14\\ngini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')]\n\n\n\n\n\n\n\n\n\n\n\nVisualize decision boundary is optional.\n\nSimilar to k-NN, we may use sklearn.inspection.DecisionBoundaryDisplay to visualize the decision boundary of this decision tree.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap='coolwarm',\n    response_method=\"predict\",\n    xlabel=iris.feature_names[0],\n    ylabel=iris.feature_names[1],\n)\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=15)\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Tuning hyperparameters\nBuilding a decision tree is the same as splitting the training dataset. If we are alllowed to keep splitting it, it is possible to get to a case that each end node is pure: the Gini impurity is 0. This means two things:\n\nThe tree learns too many (unnecessary) detailed info from the training set which might not be applied to the test set. This is called overfitting. We should prevent a model from overfitting.\nWe could use the number of split to indicate the progress of the learning.\n\nIn other words, the finer the splits are, the further the learning is. We need to consider some early stop conditions to prevent the model from overfitting.\nSome possible early stop conditions:\n\nmax_depth: The max depth of the tree. The default is none which means no limits.\nmin_samples_split: The minimum number of samples required to split an internal node. The default is 2 which means that we will split a node with as few as 2 elements if needed.\nmin_samples_leaf: The minimum number of samples required to be at a leaf node. The default is 1 which means that we will still split the node even if one subset only contains 1 element.\n\nIf we don’t set them (which means that we use the default setting), the dataset will be split untill all subsets are pure.\n\nExample 3.3 (ALMOST all end nodes are pure) In this example, we let the tree grow as further as possible. It only stops when (almost) all end nodes are pure, even if the end nodes only contain ONE element (like #6 and #11).\n\nclf = DecisionTreeClassifier(random_state=40)\nclf.fit(X, y)\nplt.figure(figsize=(3, 3), dpi=200)\nplot_tree(clf, filled=True, impurity=True, node_ids=True)\n\n[Text(0.5, 0.9166666666666666, 'node #0\\nx[1] &lt;= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n Text(0.4090909090909091, 0.75, 'node #1\\ngini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n Text(0.4545454545454546, 0.8333333333333333, 'True  '),\n Text(0.5909090909090909, 0.75, 'node #2\\nx[1] &lt;= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n Text(0.5454545454545454, 0.8333333333333333, '  False'),\n Text(0.36363636363636365, 0.5833333333333334, 'node #3\\nx[0] &lt;= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n Text(0.18181818181818182, 0.4166666666666667, 'node #4\\nx[1] &lt;= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'),\n Text(0.09090909090909091, 0.25, 'node #5\\ngini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'),\n Text(0.2727272727272727, 0.25, 'node #6\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.5454545454545454, 0.4166666666666667, 'node #7\\nx[1] &lt;= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'),\n Text(0.45454545454545453, 0.25, 'node #8\\ngini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'),\n Text(0.6363636363636364, 0.25, 'node #9\\nx[0] &lt;= 5.45\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'),\n Text(0.5454545454545454, 0.08333333333333333, 'node #10\\ngini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'),\n Text(0.7272727272727273, 0.08333333333333333, 'node #11\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.8181818181818182, 0.5833333333333334, 'node #12\\nx[0] &lt;= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'),\n Text(0.7272727272727273, 0.4166666666666667, 'node #13\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'),\n Text(0.9090909090909091, 0.4166666666666667, 'node #14\\ngini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')]\n\n\n\n\n\n\n\n\n\nIn this example there is one exception. #13 node is not pure. The reason is as follows.\n\nX[(X[:, 0]&gt;0.8) & (X[:, 1] &gt;1.75) & (X[:, 0]&lt;=4.85)]\n\narray([[4.8, 1.8],\n       [4.8, 1.8],\n       [4.8, 1.8]])\n\n\nAll the data points in this node has the same feature while their labels are different. They cannot be split further purely based on features.\n\nTherefore we could treat these hyperparameters as an indicator about how far the dataset is split. The further the dataset is split, the further the learning goes, the more details are learned. Since we don’t want the model to be overfitting, we don’t want to split the dataset that far. In this case, we could use the learning curve to help us make the decision.\nIn this example, let us choose min_samples_leaf as the indicator. When min_samples_leaf is big, the learning just starts. When min_samples_leaf=1, we reach the far most side of learning. We will see how the training and testing accuracy changes along the learning process.\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\nnum_leaf = list(range(1, 100))\ntrain_acc = []\ntest_acc = []\nfor i in num_leaf:\n    clf = DecisionTreeClassifier(random_state=42, min_samples_leaf=i)\n    clf.fit(X_train, y_train)\n    train_acc.append(clf.score(X_train, y_train))\n    test_acc.append(clf.score(X_test, y_test))\nplt.plot(num_leaf, train_acc, label='training')\nplt.plot(num_leaf, test_acc, label='testing')\nplt.gca().set_xlim((max(num_leaf)+1, min(num_leaf)-1))\nplt.legend()\n\n\n\n\n\n\n\n\nFrom this plot, the accuracy has a big jump at min_sample_leaf=41. So we could choose this to be our hyperparameter.\n\n\n3.3.4 Apply CART manually (optional)\n\n\nThe manual sample code is optional.\n\nWe apply split to the dataset S.\n\nfrom assests.codes.dt import gini, split, countlabels\nr = split(S)\nif r['split'] is True:\n    Gl, Gr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Gl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gr)),\n          ' and its label counts is {d}'.format(d=countlabels(Gr)))\n\n(0, 1.9)\nThe left subset's Gini impurity is 0.00,  and its label counts is {0.0: 50}\nThe right subset's Gini impurity is 0.50,  and its label counts is {1.0: 50, 2.0: 50}\n\n\nThe results shows that S is splitted into two subsets based on the 0-th feature and the split value is 1.9.\nThe left subset is already pure since its Gini impurity is 0. All elements in the left subset is label 0 (which is setosa). The right one is mixed since its Gini impurity is 0.5. Therefore we need to apply split again to the right subset.\n\nr = split(Gr)\nif r['split'] is True:\n    Grl, Grr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Grl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grr)),\n          ' and its label counts is {d}'.format(d=countlabels(Grr)))\n\n(1, 1.7)\nThe left subset's Gini impurity is 0.17,  and its label counts is {1.0: 49, 2.0: 5}\nThe right subset's Gini impurity is 0.04,  and its label counts is {2.0: 45, 1.0: 1}\n\n\nThis time the subset is splitted into two more subsets based on the 1-st feature and the split value is 1.7. The total Gini impurity is minimized using this split.\nThe decision we created so far can be described as follows:\n\nCheck the first feature sepal length (cm) to see whether it is smaller or equal to 1.9.\n\nIf it is, classify it as lable 0 which is setosa.\nIf not, continue to the next stage.\n\nCheck the second feature sepal width (cm) to see whether it is smaller or equal to 1.7.\n\nIf it is, classify it as label 1 which is versicolor.\nIf not, classify it as label 2 which is virginica.\n\n\n\n3.3.4.1 Analyze the differences between the two methods\nThe tree generated by sklearn and the tree we got manually is a little bit different. Let us explore the differences here.\nTo make it easier to split the set, we could convert the numpy.ndarray to pandas.DataFrame.\n\nimport pandas as pd\n\ndf = pd.DataFrame(X)\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.4\n0.2\n\n\n1\n1.4\n0.2\n\n\n2\n1.3\n0.2\n\n\n3\n1.5\n0.2\n\n\n4\n1.4\n0.2\n\n\n\n\n\n\n\nNow based on our tree, we would like to get all data points that the first feature (which is marked as 0) is smaller or equal to 1.9. We save it as df1. Similarly based on the tree gotten from sklearn, we would like to get all data points taht the second feature (which is marked as 1) is smaller or equal to 0.8 and save it to df2.\n\ndf1 = df[df[0]&lt;=1.9]\ndf2 = df[df[1]&lt;=0.8]\n\nThen we would like to compare these two dataframes. What we want is to see whether they are the same regardless the order. One way to do this is to sort the two dataframes and then compare them directly.\nTo sort the dataframe we use the method DataFrame.sort_values. The details can be found here. Note that after sort_values we apply reset_index to reset the index just in case the index is massed by the sort operation.\nThen we use DataFrame.equals to check whether they are the same.\n\ndf1sorted = df1.sort_values(by=df1.columns.tolist()).reset_index(drop=True)\ndf2sorted = df2.sort_values(by=df2.columns.tolist()).reset_index(drop=True)\nprint(df1sorted.equals(df2sorted))\n\nTrue\n\n\nSo these two sets are really the same. The reason this happens can be seen from the following two graphs.\n\n\n\n\n\n\nFrom our code\n\n\n\n\n\n\n\nFrom sklearn\n\n\n\n\n\nSo you can see that either way can give us the same classification. This means that given one dataset the construction of the decision tree might be random at some points.\n\n\n\n\n\n\nNotenote-random_state\n\n\n\nSince the split is random, when using sklearn.DecisionTreeClassifier to construct decision trees, sometimes we might get the same tree as what we get from our naive codes.\nTo illustrate this phenomenaon I need to set the random state in case it will generate the same tree as ours when I need it to generate a different tree. The parameter random_state=40 mentioned before is for this purpose.\n\n\nAnother difference is the split value of the second branch. In our case it is 1.7 and in sklearn case it is 1.75. So after we get the right subset from the first split (which is called dfr), we would split it into two sets based on whether the second feature is above or below 1.7.\n\ndfr = df[df[0]&gt;1.9]\ndf2a = dfr[dfr[1]&gt;1.7]\ndf2b = dfr[dfr[1]&lt;=1.7]\nprint(df2b[1].max())\nprint(df2a[1].min())\n\n1.7\n1.8\n\n\nNow you can see where the split number comes from. In our code, when we found a split, we will directly use that number as the cut. In this case it is 1.7.\nIn sklearn, when it finds a split, the algorithm will go for the middle of the gap as the cut. In this case it is (1.7+1.8)/2=1.75.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/3/intro.html#exercises-and-projects",
    "href": "contents/3/intro.html#exercises-and-projects",
    "title": "3  Decision Trees",
    "section": "3.4 Exercises and Projects",
    "text": "3.4 Exercises and Projects\n\nExercise 3.1 The dataset and its scattering plot is given below.\n\nPlease calculate the Gini impurity of the whole set by hand.\nPlease apply CART to create the decision tree by hand.\nPlease use the tree you created to classify the following points:\n\n\\((0.4, 1.0)\\)\n\\((0.6, 1.0)\\)\n\\((0.6, 0)\\)\n\n\nThe following code is for ploting. You may also get the precise data points by reading the code. You don’t need to write codes to solve the problem.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.2 CHOOSE ONE: Please apply the Decision Tree to one of the following datasets.\n\ndating dataset (in Chpater 2).\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease find the best hyperparameters of your choice.\nPlease record the accuracy (or cross-validation score) of your model and compare it with the models you learned before (kNN).\nPlease find the two most important features and explane your reason.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html",
    "href": "contents/4/intro.html",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1 Bootstrap aggregating\nAfter we get some relatively simple classifiers (sometimes also called weak classifiers), we might put them together to form a more complicated classifier. This type of methods is called an ensemble method. The basic way to ``ensemble’’ classifiers together to through the voting machine.\nThere are mainly two ways to generate many classifiers.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#bootstrap-aggregating",
    "href": "contents/4/intro.html#bootstrap-aggregating",
    "title": "4  Ensemble methods",
    "section": "",
    "text": "4.1.1 Basic bagging\nOne approach to get many estimators is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting.\nConsider the following example. The dataset is the one we used in Chpater 3: make_moon. We split the dataset into training and test sets.\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nplt.scatter(x=X[:, 0], y=X[:, 1], c=y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\n\n\n\n\n\n\n\nWe would like to sample from the dataset to get some smaller minisets. We will use sklearn.model_selection.ShuffleSplit to perform the action.\nThe output of ShuffleSplit is a generator. To get the index out of it we need a for loop. You may check out the following code.\nNote that ShuffleSplit is originally used to shuffle data into training and test sets. We would only use the shuffle function out of it, so we will set test_size to be 1 and use _ later in the for loop since we won’t use that part of the information.\nWhat we finally get is a generator rs that produces indexes of subsets of X_train and y_train.\n\nfrom sklearn.model_selection import ShuffleSplit\nn_trees = 1000\nn_instances = 100\nrs = ShuffleSplit(n_splits=n_trees, test_size=1, train_size=n_instances).split(X_train)\n\nsklearn provides BaggingClassifier to directly perform bagging or pasting. The code is as follows.\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(),\n                            n_estimators=1000,\n                            max_samples=1.0,\n                            bootstrap=True)\n\nIn the above code, bag_clf is a bagging classifier, made of 500 DecisionTreeClassifers, and is trained over subsets of size 1.0. The option bootstrap=True means that it is bagging. If you would like to use pasting, the option is bootstrap=False.\nThis bag_clf also has .fit() and .predict() methods. It is used the same as our previous classifiers. Let us try the make_moon dataset.\n\nfrom sklearn.metrics import accuracy_score\n\nbag_clf.fit(X_train, y_train)\ny_pred_bag = bag_clf.predict(X_test)\naccuracy_score(y_pred_bag, y_test)\n\n0.854\n\n\n\n\n4.1.2 max_samples\nThe original bagging algoithm requires the “subset” has the same number of datapoints as the original set. Since duplicates are allowed, we are expected to use 63% of the data from the original dataset.\n\n\nClick to expand for more details.\n\nLet the original dataset have \\(N\\) elements. The probability to pick any specific data is \\(1/N\\), and therefore the probability not to pick it is \\(1-1/N\\). If we pick \\(N\\) elements with replacement, the probability that we never pick a specific element is \\[\n(1-\\frac1N)^N\\rightarrow \\mathrm e^{-1},\\quad \\text{when }N\\rightarrow\\infty.\n\\] Therefore when \\(N\\) is large, the probability for each element not to be picked is approximately \\(\\mathrm e^{-1}\\), and then the probability for it to be picked is approximately \\(1-\\mathrm e^{-1}\\).\nLet \\(I_i\\) be the random variable representing whether the element \\(i\\) is picked or not. So \\[\nI_i=\\begin{cases}1&i \\text{ is picked}\\\\\n0&i\\text{ is not picked}\\end{cases}.\n\\] So \\[\nP(I_i=1)=1-\\mathrm e^{-1},\\quad P(I_i=0)=\\mathrm e^{-1},\\quad \\text{ and }E[I_i]=1\\cdot(1-\\mathrm e^{-1})+0\\cdot(\\mathrm e^{-1})=1-\\mathrm e^{-1}.\n\\] The expectation of the final sample size proportion is \\[\nE[\\frac1N\\sum_{i=1}^NI_i]=\\frac1N\\sum_{i=1}^IE[I_i]=1-\\mathrm e^{-1}\\approx 0.63.\n\\]\nWith similar calculation, if we pick \\(\\alpha N\\) data, the expectation of the final sample size proportion is \\(1-\\mathrm e^{-\\alpha}\\).\n\n\nIn sklearn, the requiremnt is relaxed, that you may have any number of data in each sample. This gives more flexiblity to adjust your model, that the sample size may affect the performance of the model. It is set by the argument max_samples.\n\nIf it is float that between 0 and 1, it will be used as the proportion. The default value is \\(1.0\\) which means that we use the original size as the sample size.\nIF it is an integer that is bigger than \\(1\\), it will be used as the fixed sample size, no matter what the orginal data size is.\n\n\n\n4.1.3 OOB score\nWhen we use bagging, it is possible that some of the training data are not used. In this case, we could record which data are not used, and just use them as the test set, instead of providing extra data for test. The data that are not used is called out-of-bag instances, or oob for short. The accuracy over the oob data is called the oob score.\nWe could set oob_score=True to enable the function when creating a BaggingClassifier, and use .oob_score_ to get the oob score after training.\n\nbag_clf_oob = BaggingClassifier(DecisionTreeClassifier(),\n                                n_estimators=1000,\n                                bootstrap=True,\n                                oob_score=True)\nbag_clf_oob.fit(X_train, y_train)\nbag_clf_oob.oob_score_\n\n0.8369411764705882\n\n\n\n\n4.1.4 Random Forests\nWhen the classifiers used in a bagging classifier are all Decision Trees, the bagging classifier is called a random forest. sklearn provide RandomForestClassifier class. It is almost the same as BaggingClassifier + DecisionTreeClassifer.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\ny_pred_rnd = rnd_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.874\n\n\nWhen we use the Decision Tree as our base estimators, the class RandomForestClassifier provides more control over growing the random forest, with a certain optimizations. If you would like to use other estimators, then BaggingClassifier should be used.\n\n\n4.1.5 Extra-trees\nWhen growing a Decision Tree, our method is to search through all possible ways to find the best split point that get the lowest Gini impurity. Anohter method is to use a random split. Of course a random tree performs much worse, but if we use it to form a random forest, the voting system can help to increase the accuracy. On the other hand, random split is much faster than a regular Decision Tree.\nThis type of forest is called Extremely Randomized Trees, or Extra-Trees for short. We could modify the above random forest classifier code to implement the extra-tree algorithm. The key point is that we don’t apply the Decision Tree algorithm to X_subset. Instead we perform a random split.\nIn sklearn there is an ExtraTreesClassifier to create such a classifier. It is hard to say which random forest is better beforehand. What we can do is to test and calculate the cross-validation scores (with grid search for hyperparameters tuning).\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\ny_pred_rnd = ext_clf.predict(X_test)\naccuracy_score(y_pred_rnd, y_test)\n\n0.8686666666666667\n\n\nIn the above example, RandomForestClassifier and ExtraTreesClassifier get similar accuracy. However from the code below, you will see that in this example ExtraTreesClassifier is much faster than RandomForestClassifier.\n\nfrom time import time\nt0 = time()\nrnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=17)\nrnd_clf.fit(X_train, y_train)\nt1 = time()\nprint('Random Frorest: {}'.format(t1 - t0))\n\nt0 = time()\next_clf = ExtraTreesClassifier(n_estimators=1000, max_leaf_nodes=17)\next_clf.fit(X_train, y_train)\nt1 = time()\nprint('Extremely Randomized Trees: {}'.format(t1 - t0))\n\nRandom Frorest: 6.35691237449646\nExtremely Randomized Trees: 1.9866275787353516\n\n\n\n\n4.1.6 Gini importance\nAfter training a Decision Tree, we could look at each node. Each split is against a feature, which decrease the Gini impurity the most. In other words, we could say that the feature is the most important during the split.\nUsing the average Gini impurity decreased as a metric, we could measure the importance of each feature. This is called Gini importance. If the feature is useful, it tends to split mixed labeled nodes into pure single class nodes.\nIn the case of random forest, since there are many trees, we might compute the weighted average of the Gini importance across all trees. The weight depends on how many times the feature is used in a specific node.\nUsing RandomForestClassifier, we can directly get access to the Gini importance of each feature by .feature_importance_. Please see the following example.\n\nrnd_clf.fit(X_train, y_train)\nrnd_clf.feature_importances_\n\narray([0.43485037, 0.56514963])\n\n\nIn this example, you may see that the two features are relavely equally important, where the second feature is slightly more important since on average it decrease the Gini impurity a little bit more.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#voting-machine",
    "href": "contents/4/intro.html#voting-machine",
    "title": "4  Ensemble methods",
    "section": "4.2 Voting machine",
    "text": "4.2 Voting machine\n\n4.2.1 Voting classifier\nAssume that we have several trained classifiers. The easiest way to make a better classifer out of what we already have is to build a voting system. That is, each classifier give its own prediction, and it will be considered as a vote, and finally the highest vote will be the prediction of the system.\nIn sklearn, you may use VotingClassifier. It works as follows.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nclfs = [('knn', KNeighborsClassifier(n_neighbors=5)),\n        ('dt', DecisionTreeClassifier(max_depth=2))]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\n\nAll classifiers are stored in the list clfs, whose elements are tuples. The syntax is very similar to Pipeline. What the classifier does is to train all listed classifiers and use the majority vote to predict the class of given test data. If each classifier has one vote, the voting method is hard. There is also a soft voting method. In this case, every classifiers not only can predict the classes of the given data, but also estimiate the probability of the given data that belongs to certain classes. On coding level, each classifier should have the predict_proba() method. In this case, the weight of each vote is determined by the probability computed. In our course we mainly use hard voting.\nLet us use make_moon as an example. We first load the dataset.\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nWe would like to apply kNN model. As before, we build a data pipeline pipe to first apply MinMaxScaler and then KNeighborsClassifier.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline(steps=[('scalar', MinMaxScaler()),\n                       ('knn', KNeighborsClassifier())])\nparameters = {'knn__n_neighbors': list(range(1, 51))}\ngs_knn = GridSearchCV(pipe, param_grid=parameters) \ngs_knn.fit(X_train, y_train)\nclf_knn = gs_knn.best_estimator_\nclf_knn.score(X_test, y_test)\n\n0.8673333333333333\n\n\nThe resulted accuracy is shown above.\nWe then try it with the Decision Tree.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ngs_dt = GridSearchCV(DecisionTreeClassifier(), param_grid={'max_depth': list(range(1, 11)), 'max_leaf_nodes': list(range(10, 30))})\ngs_dt.fit(X_train, y_train)\nclf_dt = gs_dt.best_estimator_\nclf_dt.score(X_test, y_test)\n\n0.86\n\n\nWe would also want to try Logistic regression method. This will be covered in the next Chapter. At current stage we just use the default setting without changing any hyperparameters.\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression()\nclf_lr.fit(X_train, y_train)\nclf_lr.score(X_test, y_test)\n\n0.8273333333333334\n\n\nNow we use a voting classifier to combine the results.\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nclfs = [('knn', KNeighborsClassifier()),\n        ('dt', DecisionTreeClassifier()),\n        ('lr', LogisticRegression())]\nvoting_clf = VotingClassifier(estimators=clfs, voting='hard')\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n\n0.842\n\n\nYou may compare the results of all these four classifiers. The voting classifier is not guaranteed to be better. It is just a way to form a model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#adaboost",
    "href": "contents/4/intro.html#adaboost",
    "title": "4  Ensemble methods",
    "section": "4.3 AdaBoost",
    "text": "4.3 AdaBoost\nThis is the first algorithm that successfully implements the boosting idea. AdaBoost is short for Adaptive Boosting.\n\n4.3.1 Weighted dataset\nWe firstly talk about training a Decision Tree on a weighted dataset. The idea is very simple. When building a Decision Tree, we use some method to determine the split. In this course the Gini impurity is used. There are at least two other methods: cross-entropy and misclassified rate. For all three, the count of the elemnts in some classes is the essnetial part. To train the model over the weighted dataset, we just need to upgrade the count of the elements by the weighted count.\n\nExample 4.1 Consider the following data:\n\n\n\n\n\n\n\nx0\nx1\ny\nWeight\n\n\n\n\n1.0\n2.1\n+\n0.5\n\n\n1.0\n1.1\n+\n0.125\n\n\n1.3\n1.0\n-\n0.125\n\n\n1.0\n1.0\n-\n0.125\n\n\n2.0\n1.0\n+\n0.125\n\n\n\n\n\nThe weighted Gini impurity is\n\\[\n\\text{WeightedGini}=1-(0.5+0.125+0.125)^2-(0.125+0.125)^2=0.375.\n\\]\nYou may see that the original Gini impurity is just the weighted Gini impurity with equal weights. Therefore the first tree we get from AdaBoost (see below) is the same tree we get from the Decision Tree model in Chpater 3.\n\n\n\n4.3.2 General process\nHere is the rough description of AdaBoost.\n\nAssign weights to each data point. At the begining we could assign weights equally.\nTrain a classifier based on the weighted dataset, and use it to predict on the training set. Find out all wrong answers.\nAdjust the weights, by inceasing the weights of data points that are done wrongly in the previous generation.\nTrain a new classifier using the new weighted dataset. Predict on the training set and record the wrong answers.\nRepeat the above process to get many classifiers. The training stops either by hitting \\(0\\) error rate, or after a specific number of rounds.\nThe final results is based on the weighted total votes from all classifiers we trained.\n\nNow let us talk about the details. Assume there are \\(N\\) data points. Then the inital weights are set to be \\(\\dfrac1N\\). There are 2 sets of weights. Let \\(w^{(i)}\\) be weights of the \\(i\\)th data points. Let \\(\\alpha_j\\) be the weights of the \\(j\\)th classifier. After training the \\(j\\)th classifier, the error rate is denoted by \\(e_j\\). Then we have\n\\[\ne_j=\\frac{\\text{the total weights of data points that are misclassified by the $j$th classifier}}{\\text{the total weights of data points}}\n\\]\n\\[\n\\alpha_j=\\eta\\ln\\left(\\dfrac{1-e_j}{e_j}\\right).\n\\]\n\\[\nw^{(i)}_{\\text{new}}\\leftarrow\\text{normalization} \\leftarrow w^{(i)}\\leftarrow\\begin{cases}w^{(i)}&\\text{if the $i$th data is correctly classified,}\\\\w^{(i)}\\exp(\\alpha_j)&\\text{if the $i$th data is misclassified.}\\end{cases}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe first tree is the same tree we get from the regular Decision Tree model. In the rest of the training process, more weights are put on the data that we are wrong in the previous iteration. Therefore the process is the mimic of “learning from mistakes”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe \\(\\eta\\) in computing \\(\\alpha_j\\) is called the learning rate. It is a hyperparameter that will be specified mannually. It does exactly what it appears to do: alter the weights of each classifier. The default is 1.0. When the number is very small (which is recommended although it can be any positive number), more iterations will be expected.\n\n\n\n\n4.3.3 Example 1: the iris dataset\nSimilar to all previous models, sklearn provides AdaBoostClassifier. The way to use it is similar to previous models. Note that although we are able to use any classifiers for AdaBoost, the most popular choice is Decision Tree with max_depth=1. This type of Decision Trees are also called Decision Stumps.\nIn the following examples, we initialize an AdaBoostClassifier with 500 Decision Stumps and learning_rate=0.5.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=1000,\n                             learning_rate=.5)\n\nWe will use the iris dataset for illustration. The cross_val_score is calculated as follows.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nscores = cross_val_score(ada_clf, X, y, cv=5)\nscores.mean()\n\nnp.float64(0.9466666666666667)\n\n\n\n\n4.3.4 Example 2: the Horse Colic dataset\nThis dataset is from UCI Machine Learning Repository. The data is about whether horses survive if they get a disease called Colic. The dataset is preprocessed as follows. Note that there are a few missing values inside, and we replace them with 0.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, sep=r'\\s+', header=None)\ndf = df.replace(\"?\", np.nan)\ndf = df.fillna(0)\nX = df.iloc[:, 1:].to_numpy().astype(float)\ny = df[0].to_numpy().astype(int)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n\nclf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, learning_rate=0.2)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)\n\n0.8444444444444444",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/4/intro.html#exercises",
    "href": "contents/4/intro.html#exercises",
    "title": "4  Ensemble methods",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\n\nExercise 4.1 CHOOSE ONE: Please apply the random forest to one of the following datasets.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease answer the following questions.\n\nPlease use grid search to find the good max_leaf_nodes and max_depth.\nPlease record the cross-validation score and the OOB score of your model and compare it with the models you learned before (kNN, Decision Trees).\nPlease find some typical features (using the Gini importance) and draw the Decision Boundary against the features you choose.\n\n\n\nExercise 4.2 Please use the following code to get the mgq dataset.\n\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX1, y1 = make_gaussian_quantiles(cov=2.0, n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300,\n                                 n_features=2, n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, -y2 + 1))\n\nPlease build an AdaBoost model.\n\n\nExercise 4.3 Please use RandomForestClassifier, ExtraTreesClassifier and KNeighbourClassifier to form a voting classifier, and apply to the MNIST dataset.\n\n\n\n\n\n\n\nNoteMNIST\n\n\n\nThis dataset can be loaded using the following code.\n\nimport numpy as np\nimport requests\nfrom io import BytesIO\nr = requests.get('https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz', stream = True) \ndata = np.load(BytesIO(r.raw.read()))\nX_train = data['x_train']\nX_test = data['x_test']\ny_train = data['y_train']\ny_test = data['y_test']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ensemble methods</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html",
    "href": "contents/5/intro.html",
    "title": "5  Intro to Pytorch",
    "section": "",
    "text": "5.1 Linear regression (math)\nMost materials are based on [1].\nWe only consider the simplest case: simple linear regression (SLR). The idea is very simple. The dataset contains two variables (the independent variable \\(x\\) and the response variable \\(y\\).) The goal is to find the relation between \\(x\\) and \\(y\\) with the given dataset. We assume their relation is \\(y=b+wx\\). How do we find \\(b\\) and \\(w\\)?\nLet us first see an example. We would like to find the red line (which is the best fitted curve) shown below.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#sec-linearregression_math",
    "href": "contents/5/intro.html#sec-linearregression_math",
    "title": "5  Intro to Pytorch",
    "section": "",
    "text": "\\[\n\\require{physics}\n\\require{braket}\n\\]\n\\[\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n\\]\n\n\\[\n\\newcommand{\\Exp}{\\operatorname{E}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Mode}{\\operatorname{mode}}\n\\]\n\n\\[\n\\newcommand{\\pdfbinom}{{\\tt binom}}\n\\newcommand{\\pdfbeta}{{\\tt beta}}\n\\newcommand{\\pdfpois}{{\\tt poisson}}\n\\newcommand{\\pdfgamma}{{\\tt gamma}}\n\\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n\\]\n\n\\[\n\\newcommand{\\distbinom}{\\operatorname{B}}\n\\newcommand{\\distbeta}{\\operatorname{Beta}}\n\\newcommand{\\distgamma}{\\operatorname{Gamma}}\n\\newcommand{\\distexp}{\\operatorname{Exp}}\n\\newcommand{\\distpois}{\\operatorname{Poisson}}\n\\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n\\]\n\n\n\n\n\n5.1.1 Parameter space\nThe key here is to understand the idea of “parameter space”. Since we already know that the function we are looking for has a formula \\(y=b+wx\\), we could use the pair \\((b, w)\\) to denote different candidates of our answer. For example, the following plot show some possibilities in green dashed lines, while each possiblity is denoted by \\((b, w)\\). Then the problem is reworded as to find the best pair \\((b, w)\\).\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Loss function\nThe “best” is defined in the following way. The dataset is given \\(\\{(x_i, y_i)\\}\\). If we choose a pair of parameters \\((b,w)\\), we will have an estimated regression line, as well as a set of estimated \\(\\hat{y_i}\\). The idea is to let the difference between \\(y_i\\) and \\(\\hat{y_i}\\) is as small as possible. In other words, a loss function \\(J\\) is defined as follows:\n\\[\nJ_{\\{(x_i,y_i)\\}}(b,w)=\\frac1N\\sum_{i=1}^N(y_i-\\hat{y_i})^2=\\frac1N\\sum_{i=1}^N(y_i-b-wx_i)^2\n\\tag{5.1}\\] and we are expected to find the \\((b,w)\\) such that the loss function is minimized. The contour map of \\(J\\) is shown below.\n\n\n\n\n\n\n\n\n\n\n\n5.1.3 Gradient Descent\nWe use a technique called “gradient descent” to find the global minimal of \\(J\\). We start from a random point. For example \\((1.0, 1.5)\\). Then we find a direction where the cost \\(J\\) reduces the most, and move in that direction. This direction is computed by the gradient of the cost \\(J\\), and this is the reason why the algorithm is called “gradient descent”. After we get to a new point, we evaluate the new gradient and move in the new direction. The process is repeated and we are expected to get close to the minimal point after several iterations. Just like shown in the following plot.\n\n\n\n\n\n\n\n\n\nThe parameter updating rule is shown below. The \\(\\eta\\) is called the learning rate. It is a hyperparameter that is used to control the learning process.\n\\[\n\\begin{aligned}\n&\\pdv{J}{b}=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-1),\\quad &b_{new} = b_{old}-\\eta*\\pdv{J}{b},\\\\\n&\\pdv{J}{w}=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-x_i),\\quad &w_{new} = w_{old}-\\eta*\\pdv{J}{w},\n\\end{aligned}\n\\tag{5.2}\\]\n\n\n\n\n\n\nNoteLearning rate \\(\\eta\\)\n\n\n\n\n\nGenerally speaking, larger \\(\\eta\\) will move faster to the global minimal, but might be jumpy which cause it harder to converge. On the other side, smaller \\(\\eta\\) moves in a more stable fashion, but may take a long time to converge. See the following examples.\n\n\n\n\n\n\n\n\n\nIn the first example, \\(\\eta\\) is too small, that after 200 iterations it is not very close to the minimal. In the second example, \\(\\eta\\) becomes large. Although it gets to somewhere near the minimal, the path is very jumpy. It is able to converge only because the problem is indeed an easy one.\n\n\n\nWe may record the curve of the cost function.\n\n\nAfter 200 iterations, the parameters are (2.291241352364798, 1.203587494484257).\n\n\n\n\n\n\n\n\n\nThe cost is close to \\(0\\) after 200 iterations and seems to be convergent. Therefore we believe that we are close to the minimal point. The point we get is (2.291241352364798, 1.203587494484257).\n\n\n5.1.4 Summary\nLet us summarize the example above and generalize it to the general case.\n\nLet \\(\\{(X_i, y_i)\\}\\) be a given dataset. Assume that \\(y=f_{\\Theta}(X)\\) where \\(\\Theta\\) is the set of all parameters.\nThe cost function \\(J_{\\Theta, \\{(X_i, y_i)\\}}\\) is defined.\nTo find the minimal point of the cost function, the gradient descent is applied:\n\nStart from a random initial point \\(\\theta_0\\).\nCompute the gradient \\(\\nabla J\\) and update \\(\\theta_i=\\theta_{i-1}- \\eta \\nabla J\\) and repeat the process multiple times.\nDraw the learning curve and determine when to stop. Then we get the estimated best parameters \\(\\hat{\\Theta}\\).\n\nOur model under this setting is sovled. We then turn to evaluation phase.\n\n\n\n\n\n\n\nNote\n\n\n\nThe above process can be further developped. We will discuss many of them in later sections.\n\nThe cost function is related to each concerte problem.\nTo compute the gradient of the cost function, chain rule is usually used. In the setting of MLP which we will discuss later, the gradient computations with chain rule are summarized as the so-called Back propagation.\nWe go through the data points to compute the graident. How many points do we use? What is the frenqucy to update the gradient? This belongs to the topic of mini-batch.\nEven when we know that the graident gives the best direction, sometimes we don’t really want to go in that direction, but make some modifications for some reason. To modify the direction, as well as choosing the learning rate \\(\\eta\\), is the subject of optimizers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#linear-regression-numpy",
    "href": "contents/5/intro.html#linear-regression-numpy",
    "title": "5  Intro to Pytorch",
    "section": "5.2 Linear regression (numpy)",
    "text": "5.2 Linear regression (numpy)\n\n\n\\[\n\\require{physics}\n\\require{braket}\n\\]\n\\[\n\\newcommand{\\dl}[1]{{\\hspace{#1mu}\\mathrm d}}\n\\newcommand{\\me}{{\\mathrm e}}\n\\]\n\n\\[\n\\newcommand{\\Exp}{\\operatorname{E}}\n\\newcommand{\\Var}{\\operatorname{Var}}\n\\newcommand{\\Mode}{\\operatorname{mode}}\n\\]\n\n\\[\n\\newcommand{\\pdfbinom}{{\\tt binom}}\n\\newcommand{\\pdfbeta}{{\\tt beta}}\n\\newcommand{\\pdfpois}{{\\tt poisson}}\n\\newcommand{\\pdfgamma}{{\\tt gamma}}\n\\newcommand{\\pdfnormal}{{\\tt norm}}\n  \\newcommand{\\pdfexp}{{\\tt expon}}\n\\]\n\n\\[\n\\newcommand{\\distbinom}{\\operatorname{B}}\n\\newcommand{\\distbeta}{\\operatorname{Beta}}\n\\newcommand{\\distgamma}{\\operatorname{Gamma}}\n\\newcommand{\\distexp}{\\operatorname{Exp}}\n\\newcommand{\\distpois}{\\operatorname{Poisson}}\n\\newcommand{\\distnormal}{\\operatorname{\\mathcal N}}\n\\]\n\nWe will translate everything from the previous sections into codes.\n\n5.2.1 Prepare the dataset\nWe first randomly generate a dataset (X, y) for the linear regression problem.\n\nimport numpy as np\n\nnp.random.seed(42)\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\n\nWe set the seed to be 42 for reproducing the results. We will also split the dataset into training and test sets.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,\n                                                    random_state=42)\n\nWe will only focus only on the training set in this Chapter.\n\n\n5.2.2 Compute gradient\nRecall Equation 5.1 and Equation 5.2\n\\[\n\\begin{aligned}\nJ(b,w)&=\\frac1N\\sum_{i=1}^N(y_i-b-wx_i)^2,\\\\\n\\pdv{J}{b}&=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-1),\\\\\n\\pdv{J}{w}&=\\frac1N\\sum_{i=1}^N2(y_i-b-wx_i)(-x_i).\n\\end{aligned}\n\\]\n\ndef J(parameters, X, y):    \n    b = parameters[0]\n    w = parameters[1]\n    return ((y-b-w*X)**2).mean().item()\n\ndef dJ(parameters, X, y):\n    b = parameters[0]\n    w = parameters[1]\n    db = (2*(y-b-w*X)*(-1)).mean()\n    dw = (2*(y-b-w*X)*(-X)).mean()\n    return np.array([db, dw])\n\n\n\n5.2.3 Gradient descent\nIn general we need to random select a starting point. Here for the purpose of comparing to what we get from previous section, we will use a manual selected starting point \\((1, 1.5)\\). We then follow the path and move for a few steps. Here we will use \\(\\eta=0.2\\) as the learning rate.\n\np = np.array([1.0, 1.5])\nlr = 0.2\n\nplist = []\nfor _ in range(10):\n    J_i = J(p, X_train, y_train)\n    dJ_i = dJ(p, X_train, y_train)\n    p = p - lr * dJ_i\n    plist.append([p[0], p[1]])\n\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456744, 1.840388291132826],\n [1.937508248869905, 1.8352370440485253],\n [1.96239470703006, 1.8233031967656035],\n [1.9795421883017092, 1.8081901205764057],\n [1.9926365306310478, 1.7917185352872653],\n [2.0035512067846226, 1.7748049887103947],\n [2.013240136591026, 1.7579075228763736]]\n\n\nYou may compare the answer with the PyTorch implementation in Section 5.3.3.\n\n\n5.2.4 Mini-batch and optimizers\nReview the gradient formula Equation 5.2, the gradient is computed by looking at each given data point and putting the results together. Therefore it is possible to get the partial information of the gradient by just looking at part of the data. In other words, the updating process can be modify in the following way: divide the original dataset into several groups, run through each group to compute the gradient with the data in only one group and then update the parameters. In general there are three types:\n\nThere is only 1 group: we update the parameters only once when we finish looking at all data points. This is the way we mentioned previously. It is called batch gradient descent.\nEvery single point forms a group: we update the parameters eachtime we look at one data point. This method is called stocastic gradient descent (SGD). Since we compute the gradient with only one data point, it is expected that the direction is far from perfect, and the descent process is expected to be more “random”.\nMultiple groups of the same size are formed, with a reasonable group size and group number. This is called mini-batch gradient descent. It is the middle point between the above two methods. The batch size, which is the size of each group, is a very important hyperparameter for trainning.\n\n\n\n\n\n\n\nNoteEpochs\n\n\n\nOne epoch is the process that you see each data point exactly once, no matter what the batch size is.\n\n\nUsually batch gradient descent is expected to have a more smooth trajection but move slowly, while SGD is expected to move faster to the minimal point but may never really get to it since the trajection is too jumpy. Mini-batch is meant to strike a balanced point by finding a good batch size. In the example below, we show the mini-batch gradient descent in the first 10 epochs.\n\np = np.array([1.0, 1.5])\nlr = 0.2\nbatchsize = 32\nRANDOMSEED = 42\n\nN = X_train.shape[0]\nindx = np.arange(N)\n\nnp.random.seed(RANDOMSEED)\nnp.random.shuffle(indx)\nbatches = []\n\nbatch_num = int(np.ceil(N / batchsize))\nfor i in range(batch_num):\n    last = np.minimum((i+1)*batchsize, N)\n    batches.append(indx[i*batchsize: last])\n\nplist = []\nfor epoch in range(10):\n    for i in range(batch_num):\n        dJ_i = dJ(p, X_train[batches[i]], y_train[batches[i]])\n        p = p - lr * dJ_i\n    plist.append([p[0], p[1]])\nplist\n\n[[1.8396487079358765, 1.8350571390420554],\n [1.9759220547433842, 1.826737782139383],\n [2.0168483670845423, 1.7772907765564647],\n [2.043224019190362, 1.726425817587749],\n [2.065921614604292, 1.6790714304374827],\n [2.086528794877608, 1.635572387082582],\n [2.1053898608038017, 1.595691439736766],\n [2.1226730826892424, 1.559137786653576],\n [2.1385131544625695, 1.5256351733493791],\n [2.1530309370105214, 1.494929115540467]]\n\n\n\n\n\n\n\n\nNoteNon-shuffle version\n\n\n\n\n\nHere is the result for the non-shuffle version. You could compare the results with what we do later.\n\np = np.array([1.0, 1.5])\nlr = 0.2\nbatchsize = 32\n\nN = X_train.shape[0]\nindx = np.arange(N)\n\nbatches = []\nbatch_num = int(np.ceil(N / batchsize))\nfor i in range(batch_num):\n    last = np.minimum((i+1)*batchsize, N)\n    batches.append(indx[i*batchsize: last])\n\nplist = []\nfor epoch in range(10):\n    for i in range(batch_num):\n        dJ_i = dJ(p, X_train[batches[i]], y_train[batches[i]])\n        p = p - lr * dJ_i\n    plist.append([p[0], p[1]])\nplist\n\n[[1.8277028504755573, 1.8368193572906044],\n [1.9607104449826838, 1.8293981130981023],\n [2.001626059409397, 1.7815077539441087],\n [2.0286935704191, 1.7321715193480243],\n [2.0522055690695757, 1.686138097785071],\n [2.0736381185747943, 1.6437403254745735],\n [2.0933134526016604, 1.6047617600958677],\n [2.111393711486754, 1.5689357968513453],\n [2.1280105514943686, 1.5360086358936902],\n [2.143282725696795, 1.505745878510758]]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#linear-regression-pytorch",
    "href": "contents/5/intro.html#linear-regression-pytorch",
    "title": "5  Intro to Pytorch",
    "section": "5.3 Linear regression (PyTorch)",
    "text": "5.3 Linear regression (PyTorch)\n\n5.3.1 Construct torch.Tensor\nThere are multiple ways to construct a tensor. I just discuss those confusing ones.\n\ntorch.Tensor is the PyTorch tensor data structure. Itself serves as the constructor of the class, therefore you may use torch.Tensor(data) to construct a tensor. This is relative basic, and will have a default float type.\ntorch.tensor is the recommendated function to construct a tensor from data. It has two benefits over torch.Tensor: it will automatically induce the datatype from data instead of always using float; and it is easier to change datatype with the argument dtype.\ntorch.as_tensor is a function to construct a tensor from data. If the original data is numpy array, this tensor shares data with it. This means that if one is changed, the other is changed as well.\n\n\nimport numpy as np\nimport torch\n\nexample = np.array([1, 2])\nexample_tensor0 = torch.Tensor(example)\nexample_tensor1 = torch.tensor(example)\nexample_tensor2 = torch.as_tensor(example)\n\nprint(f'Tensor: dtype: {example_tensor0.dtype}, tensor: dtype: {example_tensor1.dtype}')\n\nprint(f'tensor: {example_tensor1}, as_tensor: {example_tensor2}, original: {example}')\n\nexample[0] = 0\nprint(f'tensor: {example_tensor1}, as_tensor: {example_tensor2}, original: {example}')\n\nTensor: dtype: torch.float32, tensor: dtype: torch.int32\ntensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([1, 2], dtype=torch.int32), original: [1 2]\ntensor: tensor([1, 2], dtype=torch.int32), as_tensor: tensor([0, 2], dtype=torch.int32), original: [0 2]\n\n\nIn general, it is recommended to use torch.as_tensor over torch.tensor (since for large data to create a view is much faster than to create a copy) and to use torch.tensor over torch.Tensor (due to the benefits mentioned above).\n\n\n\n\n\n\nNoteScalar\n\n\n\nA tensor with only one element is still a tensor in PyTorch. To use it as a scalar, you need to use itme() method.\n\na = torch.tensor(1)\na\n\ntensor(1)\n\n\n\na.item()\n\n1\n\n\nNote that for numpy, before 2.0 version an array with one element is considered as scalar. However after 2.0, it behaves very similar to PyTorch.\n\n\n\n\n\n\n\n\nNotedatatype\n\n\n\nThe datatype in PyTorch is very strict. Many functions can work with only some of the datatypes. In most cases float and double are used. Other types may or may not be supported by a specific function.\nHowever, there are a lot of ways to play with types. For example, you may use torch.tensor([1], dtype=torch.double) to directly construct a double tensor, or use torch.tensor([1]).double() to first construct an int tensor and then cast it into a double tensor.\n\n\n\n\n\n\n\n\nNoteanother datatype note\n\n\n\nnumpy also has dtype setting but since it is not strict on it, we ignored it previous. Here is the case: the default setting for numpy is double type, or float64, while in PyTorch float, or float32, is commonly used. Since the precision is different, when cast from double to float, the number might be changed a little bit.\n\na = np.random.random(1)\nb = torch.as_tensor(a)\nc = torch.as_tensor(a, dtype=torch.float)\nd = torch.as_tensor(a, dtype=float)\ne = torch.as_tensor(a, dtype=torch.float64)\nf = b.float()\ng = f.double()\nprint(f'a: {a[0]}, type of a: {a.dtype}\\n'\n      f'b: {b.item()}, type of b: {b.dtype}\\n'\n      f'c: {c.item()}, type of c: {c.dtype}\\n'\n      f'd: {d.item()}, type of d: {d.dtype}\\n'\n      f'e: {e.item()}, type of e: {e.dtype}\\n'\n      f'f: {f.item()}, type of e: {f.dtype}\\n'\n      f'g: {g.item()}, type of e: {g.dtype}\\n')\n\na: 0.28093450968738076, type of a: float64\nb: 0.28093450968738076, type of b: torch.float64\nc: 0.28093451261520386, type of c: torch.float32\nd: 0.28093450968738076, type of d: torch.float64\ne: 0.28093450968738076, type of e: torch.float64\nf: 0.28093451261520386, type of e: torch.float32\ng: 0.28093451261520386, type of e: torch.float64\n\n\n\nYou may notice the difference from the example, and also take notes about the convetion of which setting is corresponding to which type. Note that dtype=float actually create double type.\nIn this notes we will use double type by setting dtype=float or torch.float64 to reduce the possibility of that small differences.\n\n\nWe now construct a PyTorch tensor version of the dataset we used in previous sections. The device part will be introduced later.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\n\nRANDOMSEED = 42\nnp.random.seed(RANDOMSEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nX = np.random.rand(100)\ny = 2.3 + 1.2 * X + np.random.randn(100) * 0.1\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,\n                                                    random_state=RANDOMSEED)\nX_tensor_train = torch.as_tensor(X_train, device=device, dtype=float)\ny_tensor_train = torch.as_tensor(y_train, device=device, dtype=float)\n\n\n\n\n\n\n\nNoteBack to numpy\n\n\n\nIf we would like to turn a tensor back to numpy, usually we would like to remove it from related computation graphs and just keep the data. Therefore usually we would like to apply detach() method to the tensor before converting. Also later we will talk about devices. When taking GPU into consideration, we also want to send the tensor back to CPU before converting. Therefore the code to turn a PyTorch tensor to a numpy array is x.detach().cpu().numpy().\n\n\n\n\n5.3.2 devices\nWe coulde use torch.cuda.is_available() to check whether we have GPU/CUDA supported devices. If the answer is no, we don’t need to change any codes and everything works fine but slow.\nIf we have GPU/CUDA supported devices, we could send our tensors to them and do computations there. Google Colab is a good place to play with it if we don’t have our own hardware.\nIn most cases we use to(device) method to send a tensor to a device. Sometimes some function has device=device argument to automatically construct tensors in a device. Note that if one needs to compute the gradient of a tensor and send the tensor to a device, we need to manually set requires_grad_(True) or create the tensor with device argument.\n\n\n\n\n\n\nNoteto(device)\n\n\n\nWhen sending tensors to other devices by to, gradient info might be lost. Therefore if we need to send trainable tensors to GPU some special methods should be used (e.g. setting device when creating the tensor). However for the dataset we don’t need to worry about it.\n\n\nHere are some examples, although they only makes sense in a GPU environment.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nt1 = torch.tensor(1, dtype=float, device=device)\nt2 = torch.tensor(1, dtype=float)\nprint(f't1: {t1.type()}, t2: {t2.type()}')\n\nIf you can see cuda in the output of type, it is a GPU tensor. Otherwise it is a CPU tensor. We may use to to convert a CPU tensor to be a GPU tensor. If this tensor requires gradient, we should set it manually.\n\nt3 = t2.to(device)\nt3 = t3.requires_grad_(True)\n\nIt is usually recommended to write codes with device in mind like above, since the codes work for both CPU and GPU machines.\n\n\n5.3.3 Gradient\nPyTorch can use autograd to automatically compute the gradient of given formula. All computations are done within the context of tensors. The biggest difference between PyTorch tensor and numpy array is that PyTorch tensor carries gradient infomation on its own.\nThe step is very easy: first use PyTorch tensor to write a formula, enable gradients on correct tensors, and then use the backward() method.\n\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\n\nloss = ((y_tensor_train - b - w * X_tensor_train)**2).mean()\nloss.backward()\nprint(f'db: {b.grad}, dw: {w.grad}')\n\ndb: -2.310644882519191, dw: -1.0130224137389\n\n\nWe could manually compute the first few iterations and record the results. You may compare it with the numpy implementation in Section 5.2.3. The answer is exactly the same.\n\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\nlr = 0.2\nplist = []\nfor _ in range(10):\n    loss = ((y_tensor_train - b - w * X_tensor_train)**2).mean()\n    loss.backward()\n    with torch.no_grad():\n        b -= lr * b.grad\n        w -= lr * w.grad\n    b.grad.zero_()\n    w.grad.zero_()\n    plist.append([b.item(), w.item()])\n    \nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code has some tricky parts. The main issue is to let PyTorch know which gradient infomation should be kept, which should not. In this code, to make it run correctly, we need to pay attention to the following three things:\n\nBefore updating b and w, with torch.no_grad() should be used, to tell PyTorch don’t compute gradient here.\nWhen updating b and w, we should use the in-place syntax b -= db instead of b = b - db. Again, the reason is related to updating gradient: the out-of-place syntax b = b - db will lose the grad info.\nAfter updating b and w, we need to zero out the grad info by applying b.grad.zero_() and w.grad.zero_().\n\n\n\nWe will skip mini-batch gradient descent here, and leave it to the next section with a more systematic treatment.\n\n\n5.3.4 Optimizers\nAfter we get the gradient, there are still many tricks to move one step further. We already talked about the learning rate before. It is not the only case. Another example is that sometimes we don’t really want to move in the direction given by the gradient, but we want to modify it a little bit. All these tricks are combined together and are called optimizers.\nAn optimizer is a set of rules to update parameters after the gradient is computed. We already talked about SGD (stochastic gradient descent). Other common ones include RMSprop and Adam. In general, Adam is the generic best optimizer. If you don’t know which optimizer to use, Adam is always the go-to choice.\nHere we rewrite our previous code by optimizers. We use SGD in this example. Again, we may compare the results to Section 5.2.3 and Section 5.3.3.\n\nfrom torch.optim import SGD\nimport torch\n\nlr = 0.2\nb = torch.tensor(1, dtype=float, device=device, requires_grad=True)\nw = torch.tensor(1.5, dtype=float, device=device, requires_grad=True)\n\noptimizer = SGD([b, w], lr=lr)\nplist = []\n\nfor epoch in range(10):\n    loss = ((y_tensor_train - b - w*X_tensor_train)**2).mean()\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    plist.append([b.item(), w.item()])\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n5.3.5 Use class to describe the model\nWe now want to upgrade the code we wrote in previous sections in terms of classes, since it is a good way to wrap up our own code.\n\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.b = nn.Parameter(torch.tensor(1, requires_grad=True, dtype=float))\n        self.w = nn.Parameter(torch.tensor(1.5, requires_grad=True, dtype=float))\n\n    def forward(self, x):\n        return self.b + self.w * x\n\nRANDOMSEED = 42\ntorch.manual_seed(RANDOMSEED)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = LR().to(device)\nmodel.state_dict()\n\nOrderedDict([('b', tensor(1., dtype=torch.float64)),\n             ('w', tensor(1.5000, dtype=torch.float64))])\n\n\nWe could use model.state_dict() to look at the parameters of the model. Another way to see the parameters is to use model.parameters() method. The latter will return an iterator that help you go through all parameters.\n\nfor item in model.parameters():\n    print(item)\n\nParameter containing:\ntensor(1., dtype=torch.float64, requires_grad=True)\nParameter containing:\ntensor(1.5000, dtype=torch.float64, requires_grad=True)\n\n\nNow we reproduce the training code for LR class.\n\nfrom torch.optim import SGD\n\ndef loss_fn(yhat, y):\n    return ((yhat-y)**2).mean()\n\nlr = 0.2\noptimizer = SGD(model.parameters(), lr=lr)\n\nepoch_num = 10\n\nplist = []\nfor epoch in range(epoch_num):\n    model.train()\n\n    yhat = model(X_tensor_train)\n    loss = loss_fn(yhat, y_tensor_train)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    p = model.state_dict()\n    plist.append([p['b'].item(), p['w'].item()])\n\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n5.3.6 Using standard modules\nWe hand write our models and set parameters in our previous versions. PyTorch provides many standard modules that we can use directly. For example, the linear regression model can be found in nn.modules as Linear, while our loss function is the mean square differene function which is MSELoss from nn.\n\nfrom torch.nn.modules import Linear\nfrom torch.nn import MSELoss\n\nclass BetterLR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n\n        self.linear = Linear(in_features=1, out_features=1)\n        self.linear.bias = torch.nn.Parameter(torch.tensor([1.0], dtype=float))\n        self.linear.weight = torch.nn.Parameter(torch.tensor([[1.5]], dtype=float))\n\n    def forward(self, x):\n        return self.linear(x)\n\nlr = 0.2\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel2 = BetterLR().to(device)\noptimizer2 = SGD(model2.parameters(), lr=lr)\n\nepoch_num = 10\nplist = []\n\nfor epoch in range(epoch_num):\n    model2.train()\n\n    yhat = model2(X_tensor_train.reshape(-1, 1))\n    loss2 = MSELoss(reduction='mean')(yhat, y_tensor_train.reshape(-1, 1))\n    loss2.backward()\n    optimizer2.step()\n    optimizer2.zero_grad()\n    p = model2.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\n\nplist\n\n[[1.462128976503838, 1.70260448274778],\n [1.701791352506138, 1.7949449226054148],\n [1.8284450974134463, 1.8316393022111805],\n [1.8976247633456746, 1.840388291132826],\n [1.9375082488699051, 1.8352370440485253],\n [1.9623947070300602, 1.8233031967656035],\n [1.9795421883017095, 1.8081901205764057],\n [1.992636530631048, 1.7917185352872653],\n [2.003551206784623, 1.7748049887103945],\n [2.013240136591026, 1.7579075228763734]]\n\n\n\n\n\n\n\n\nNoteInitialize the parameters\n\n\n\nIn all our examples we initialize the parameters to be \\((1, 1.5)\\) for the purpose of comparision. In most cases, we don’t manually set the intial values, but use random numbers. In this case, we simply delete the manual codes.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that if we directly change our function to the standard one we will encounter some issues. The main reason is that our previous code is an oversimplifed version that we treat b and w as two scalars. They are scalars in our particular problem, but it is better to treat them as a special case of tensors for the purpose of better generalization. Actually based on the standard functions from PyTorch (as well as many others like sklearn) X and y are expected to be 2D tensors. This is the reason why there are some strange reshape(-1, 1 ) in the codes.\nWe will reconstruct it in the later sections.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#sec-dataloader",
    "href": "contents/5/intro.html#sec-dataloader",
    "title": "5  Intro to Pytorch",
    "section": "5.4 Dataloader",
    "text": "5.4 Dataloader\n\n5.4.1 Convert the previous dataset using DataLoader\nUsually we use a class to provide data. The class is based on Dataset class, and need to implement the constructor, __getitem__ method and __len__ method. Here is an example.\n\n\n\n\n\n\nCaution\n\n\n\nNote that we directly change X and y to be 2D tensors when we create the dataset.\n\n\n\nfrom torch.utils.data import Dataset\n\nclass MyData(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.tensor(x, dtype=float).reshape(-1, 1)\n        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)\n\n    def __getitem__(self, index):\n        return (self.x[index], self.y[index])\n\n    def __len__(self):\n        return len(self.y)\n\ntrain_data = MyData(X_train, y_train)\ntrain_data[1]\n\n(tensor([0.7713], dtype=torch.float64), tensor([3.1575], dtype=torch.float64))\n\n\nThen we use Dataloader to feed the data into our model.\n\nfrom torch.utils.data import DataLoader \ntrain_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n\nIt is used in the following way.\n\nlr = 0.2\nepoch_num = 10\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = BetterLR().to(device)\noptimizer = SGD(model.parameters(), lr=lr)\n\nplist = []\nfor epoch in range(epoch_num):\n    for X_batch, y_batch in train_loader:\n        yhat = model(X_batch)\n        loss = MSELoss(reduction='mean')(yhat, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    p = model.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\nplist\n\n[[1.823910885110298, 1.8337838971747065],\n [1.943046588179278, 1.8159156063329505],\n [2.008997965681829, 1.7804701154492042],\n [2.0285249013384035, 1.7270125310793967],\n [2.0563058412652087, 1.6825156563053996],\n [2.0699681006441395, 1.6356185362954465],\n [2.105228875384819, 1.6040273694037541],\n [2.0967822112836743, 1.552514187091518],\n [2.124683201405048, 1.5249842249986072],\n [2.1368486965881486, 1.493812615037102]]\n\n\nWhen applying mini-batch, usually we will shuffle the dataset. If we disable the shuffle here as well as the shuffle in numpy case, you will see that we get exactly the same answer.\n\n\n\n\n\n\nNoteNon-shuffle version\n\n\n\n\n\nHere is the result for non-shuffle version.\n\nfrom torch.utils.data import DataLoader \n\nlr = 0.2\nepoch_num = 10\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = BetterLR().to(device)\noptimizer = SGD(model.parameters(), lr=lr)\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=False)\n\nplist = []\nfor epoch in range(epoch_num):\n    for X_batch, y_batch in train_loader:\n        yhat = model(X_batch)\n        loss = MSELoss(reduction='mean')(yhat, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    p = model.state_dict()\n    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])\nplist\n\n[[1.8277028504755573, 1.8368193572906044],\n [1.9607104449826838, 1.8293981130981023],\n [2.001626059409397, 1.7815077539441087],\n [2.0286935704191, 1.7321715193480243],\n [2.0522055690695757, 1.686138097785071],\n [2.0736381185747943, 1.6437403254745735],\n [2.0933134526016604, 1.6047617600958677],\n [2.111393711486754, 1.5689357968513453],\n [2.1280105514943686, 1.5360086358936902],\n [2.143282725696795, 1.505745878510758]]\n\n\n\n\n\nYou may notice that we use some not-very-elegent way to display the result. Don’t worry about it. We will work on a better solution in the next section.\n\n\n5.4.2 Rewrite using random_split\nIt is possible to purely use PyTorch instead of going through sklearn. After we get the Dataset, we could use random_split to create training set and testing set.\n\nfrom torch.utils.data import random_split\nimport numpy as np\n\ndataset = MyData(X, y)\ntrain_data, val_data = random_split(dataset, [.85, .15], generator=torch.Generator().manual_seed(42))\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=32)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/5/intro.html#exercises",
    "href": "contents/5/intro.html#exercises",
    "title": "5  Intro to Pytorch",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nExercise 5.1 Try to reconstruct all the plots in Section Section 5.1.\n\n\n\n\n\n[1] Godoy, D. V. (2022). Deep learning with PyTorch step-by-step: A beginner’s guide. https://leanpub.com/pytorch.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Pytorch</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html",
    "href": "contents/6/intro.html",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1 Basic idea\nLogistic regression is very similar to linear regression, but applied to classification problems. In this chpater our idea is to treat it as the simplest example of a neural network instead of using other methods. The code we developped in the last chapter will be used extensively.\nAssume that we have a binary classfification problem with \\(N\\) features. Our model starts from the logit instead of the label \\(y\\) itself.\n\\[\nlogit(y)=\\theta_0+\\sum_{j=1}^N\\theta_jx_j.\n\\]\nThe logit function is used to describe the logorithm of the binary odds. The odd ratio is the ratio between the probability of success and the probability of failure. Assume the probability of success is \\(p\\). Then\n\\[\noddratio(p)=\\frac{p}{1-p},\\quad logit(p)=z = \\log\\left(\\frac{p}{1-p}\\right).\n\\] We could solve the logit function, and get its inverse: the function is the Sigmoid function. Once we have the logit value, we could use it to get the probability. \\[\np=\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}.\n\\]\nTherefore the model for Logistic regression is as follows:\n\\[\np=\\sigma(L(x))=\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\sigma\\left(\\Theta \\hat{x}^T\\right).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#basic-idea",
    "href": "contents/6/intro.html#basic-idea",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1.1 Sigmoid function\nThe Sigmoid function is defined as follows:\n\\[\n\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}.\n\\] The graph of the function is shown below.\n\n\n\n\n\n\n\n\n\nThe main properties of \\(\\sigma\\) are listed below as a Lemma.\n\nLemma 6.1 The Sigmoid function \\(\\sigma(z)\\) satisfies the following properties.\n\n\\(\\sigma(z)\\rightarrow \\infty\\) when \\(z\\mapsto \\infty\\).\n\\(\\sigma(z)\\rightarrow -\\infty\\) when \\(z\\mapsto -\\infty\\).\n\\(\\sigma(0)=0.5\\).\n\\(\\sigma(z)\\) is always increasing.\n\\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\).\n\n\n\nSolution. We will only look at the last one.\n\\[\n\\begin{split}\n\\sigma'(z)&=-\\frac{(1+\\mathrm e^{-z})'}{(1+\\mathrm e^{-z})^2}=\\frac{\\mathrm e^{-z}}{(1+\\mathrm e^{-z})^2}=\\frac{1}{1+\\mathrm e^{-z}}\\frac{\\mathrm e^{-z}}{1+\\mathrm e^{-z}}\\\\\n&=\\sigma(z)\\left(\\frac{1+\\mathrm e^{-z}}{1+\\mathrm e^{-z}}-\\frac{1}{1+\\mathrm e^{-z}}\\right)=\\sigma(z)(1-\\sigma(z)).\n\\end{split}\n\\]\n\n\n\n6.1.2 Gradient descent\n\n\nWe would like to use Gradient descent to sovle Logistic regression problems. For binary classification problem, the cost function is defined to be\n\\[\nJ(\\Theta)=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\log(p^{(i)})+(1-y^{(i)})\\log(1-p^{(i)})\\right].\n\\] Here \\(m\\) is the number of data points, \\(y^{(i)}\\) is the labelled result (which is either \\(0\\) or \\(1\\)), \\(p^{(i)}\\) is the predicted value (which is between \\(0\\) and \\(1\\)).\n\n\n\n\n\n\nNote\n\n\n\nThe algorithm gets its name since we are using the gradient to find a direction to lower our height.\n\n\n\n\n6.1.3 The Formulas\n\nTheorem 6.1 The gradient of \\(J\\) is computed by\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\tag{6.1}\\]\n\n\n\nClick for details.\n\n\nProof. The formula is an application of the chain rule for the multivariable functions.\n\\[\n\\begin{split}\n\\dfrac{\\partial p}{\\partial \\theta_k}&=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma(L(\\Theta))\\\\\n&=\\sigma(L)(1-\\sigma(L))\\dfrac{\\partial}{\\partial \\theta_k}\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)\\\\\n&=\\begin{cases}\np(1-p)&\\text{ if }k=0,\\\\\np(1-p)x_k&\\text{ otherwise}.\n\\end{cases}\n\\end{split}\n\\] Then\n\\[\n\\nabla p = \\left(\\frac{\\partial p}{\\partial\\theta_0},\\ldots,\\frac{\\partial p}{\\partial\\theta_n}\\right) = p(1-p)\\hat{x}.\n\\]\nThen\n\\[\n\\nabla \\log(p) = \\frac{\\nabla p}p =\\frac{p(1-p)\\hat{x}}{p}=(1-p)\\hat{x}.\n\\]\n\\[\n\\nabla \\log(1-p) = \\frac{-\\nabla p}{1-p} =-\\frac{p(1-p)\\hat{x}}{1-p}=-p\\hat{x}.\n\\]\nThen\n\\[\n\\begin{split}\n\\nabla J& = -\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\nabla \\log(p^{(i)})+(1-y^{(i)})\\nabla \\log(1-p^{(i)})\\right]\\\\\n&=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}(1-p^{(i)})\\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\\hat{x}^{(i)})\\right]\\\\\n&=-\\frac1m\\sum_{i=1}^m\\left[(y^{(i)}-p^{(i)})\\hat{x}^{(i)}\\right].\n\\end{split}\n\\]\nWe write \\(\\hat{x}^{(i)}\\) as row vectors, and stack all these row vectors vertically. What we get is a matrix \\(\\hat{\\textbf X}\\) of the size \\(m\\times (1+n)\\). We stack all \\(y^{(i)}\\) (resp. \\(p^{(i)}\\)) vectically to get the \\(m\\)-dim column vector \\(\\textbf y\\) (resp. \\(\\textbf p\\)).\nUsing this notation, the previous formula becomes\n\\[\n\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n\\]\nAfter the gradient can be computed, we can start to use the gradient descent method. Note that, although \\(\\Theta\\) are not explicitly presented in the formula of \\(\\nabla J\\), this is used to modify \\(\\Theta\\):\n\\[\n\\Theta_{s+1} = \\Theta_s - \\alpha\\nabla J.\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you directly use library, like sklearn or PyTorch, they will handle the concrete computation of these gradients.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#regularization",
    "href": "contents/6/intro.html#regularization",
    "title": "6  Logistic regression",
    "section": "6.2 Regularization",
    "text": "6.2 Regularization\n\n6.2.1 Three types of errors\nEvery estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.\n\n\n6.2.2 Underfit vs Overfit\nWhen fit a model to data, it is highly possible that the model is underfit or overfit.\nRoughly speaking, underfit means the model is not sufficient to fit the training samples, and overfit means that the models learns too many noise from the data. In many cases, high bias is related to underfit, and high variance is related to overfit.\nThe following example is from the sklearn guide. Although it is a polynomial regression example, it grasps the key idea of underfit and overfit.\n\n\n\n\n\n\n\n\n\n\n\n6.2.3 Learning curves (accuracy vs training size)\nA learning curve shows the validation and training score of an estimator for varying a key hyperparameter. In most cases the key hyperparameter is the training size or the number of epochs. It is a tool to find out how much we benefit from altering the hyperparameter by training more data or training for more epochs, and whether the estimator suffers more from a variance error or a bias error.\nsklearn provides sklearn.model_selection.learning_curve() to generate the values that are required to plot such a learning curve. However this function is just related to the sample size. If we would like to talk about epochs, we need other packages.\nLet us first look at the learning curve about sample size. The official document page is here. The function takes input estimator, dataset X, y, and an arry-like argument train_sizes. The dataset (X, y) will be split into pieces using the cross-validation technique. The number of pieces is set by the argument cv. The default value is cv=5. For details about cross-validation please see Section 2.2.5.\nThen the model is trained over a random sample of the training set, and evaluate the score over the test set. The size of the sample of the training set is set by the argument train_sizes. This argument is array-like. Therefore the process will be repeated several times, and we can see the impact of increasing the training size.\nThe output contains three pieces. The first is train_sizes_abs which is the number of elements in each training set. This output is mainly for reference. The difference between the output and the input train_sizes is that the input can be float which represents the percentagy. The output is always the exact number of elements.\nThe second output is train_scores and the third is test_scores, both of which are the scores we get from the training and testing process. Note that both are 2D numpy arrays, of the size (number of different sizes, cv). Each row is a 1D numpy array representing the cross-validation scores, which is corresponding to a train size. If we want the mean as the cross-validation score, we could use train_scores.mean(axis=1).\nAfter understanding the input and output, we could plot the learning curve. We still use the horse colic as the example. The details about the dataset can be found here.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nC:\\Users\\Xinli\\AppData\\Local\\Temp\\ipykernel_49168\\73942173.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  df = pd.read_csv(url, delim_whitespace=True, header=None)\nC:\\Users\\Xinli\\AppData\\Local\\Temp\\ipykernel_49168\\73942173.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[23].replace({1: 1, 2: 0}, inplace=True)\n\n\nWe use the model LogisticRegression. The following code plot the learning curve for this model.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nclf = LogisticRegression(max_iter=1000)\nsteps = [('scalar', MinMaxScaler()),\n         ('log', clf)]\npipe = Pipeline(steps=steps)\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\ntrain_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n                                                        train_sizes=np.linspace(0.1, 1, 20))\n\nimport matplotlib.pyplot as plt\nplt.plot(train_sizes, train_scores.mean(axis=1), label='train')\nplt.plot(train_sizes, test_scores.mean(axis=1), label='test')\nplt.legend()\n\n\n\n\n\n\n\n\nThe learning curve is a primary tool for us to study the bias and variance. Usually\n\nIf the two training curve and the testing curve are very close to each other, this means that the variance is low. Otherwise the variance is high, and this means that the model probabily suffer from overfitting.\nIf the absolute training curve score is high, this means that the bias is low. Otherwise the bias is high, and this means that the model probabily suffer from underfitting.\n\nIn the above example, although regularization is applied by default, you may still notice some overfitting there.\n\n\n6.2.4 Regularization\nRegularization is a technique to deal with overfitting. Here we only talk about the simplest method: ridge regression, also known as Tikhonov regularizaiton. Because of the formula given below, it is also called \\(L_2\\) regularization. The idea is to add an additional term \\(\\dfrac{\\alpha}{2m}\\sum_{i=1}^m\\theta_i^2\\) to the original cost function. When training with the new cost function, this additional term will force the parameters in the original term to be as small as possible. After finishing training, the additional term will be dropped, and we use the original cost function for validation and testing. Note that in the additional term \\(\\theta_0\\) is not presented.\nThe hyperparameter \\(\\alpha\\) is the regularization strength. If \\(\\alpha=0\\), the new cost function becomes the original one; If \\(\\alpha\\) is very large, the additional term dominates, and it will force all parameters to be almost \\(0\\). In different context, the regularization strength is also given by \\(C=\\dfrac{1}{2\\alpha}\\), called inverse of regularization strength.\n\n6.2.4.1 The math of regularization\n\nTheorem 6.2 The gradient of the ridge regression cost function is\n\\[\n\\nabla J=\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}+\\frac{\\alpha}{m}\\Theta.\n\\]\nNote that \\(\\Theta\\) doesn’t contain \\(\\theta_0\\), or you may treat \\(\\theta_0=0\\).\n\nThe computation is straightforward.\n\n\n6.2.4.2 The code\nRegularization is directly provided by the logistic regression functions.\n\nIn LogisticRegression, the regularization is given by the argument penalty and C. penalty specifies the regularizaiton method. It is l2 by default, which is the method above. C is the inverse of regularization strength, whose default value is 1.\nIn SGDClassifier, the regularization is given by the argument penalty and alpha. penalty is the same as that in LogisticRegression, and alpha is the regularization strength, whose default value is 0.0001.\n\nLet us see the above example.\n\nclf = LogisticRegression(max_iter=1000, C=0.1)\nsteps = [('scalar', MinMaxScaler()),\n         ('log', clf)]\npipe = Pipeline(steps=steps)\nfrom sklearn.model_selection import learning_curve\nimport numpy as np\ntrain_sizes, train_scores, test_scores = learning_curve(pipe, X_train, y_train,\n                                                        train_sizes=np.linspace(0.1, 1, 20))\n\nimport matplotlib.pyplot as plt\nplt.plot(train_sizes, train_scores.mean(axis=1), label='train')\nplt.plot(train_sizes, test_scores.mean(axis=1), label='test')\nplt.legend()\n\n\n\n\n\n\n\n\nAfter we reduce C from 1 to 0.1, the regularization strength is increased. Then you may find that the gap between the two curves are reduced. However the overall performace is also reduced, from 85%~90% in C=1 case to around 80% in C=0.1 case. This means that the model doesn’t fit the data well as the previous one. Therefore this is a trade-off: decrease the variance but increase the bias.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#neural-network-implement-of-logistic-regression",
    "href": "contents/6/intro.html#neural-network-implement-of-logistic-regression",
    "title": "6  Logistic regression",
    "section": "6.3 Neural network implement of Logistic regression",
    "text": "6.3 Neural network implement of Logistic regression\nIn the previous sections, we use gradient descent to run the Logistic regression model. We mentioned some important concepts, like epochs, mini-batch, etc.. We are going to use PyTorch to implement it. We will reuse many codes we wrote in the previous chapter. \n\n6.3.1 Example\nWe still use the horse colic dataset as an example. We first prepare the dataset.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, sep=r'\\s+', header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf = df.fillna(0)\ndf = df.drop(columns=[2, 24, 25, 26, 27])\ndf[23] = df[23].replace({1: 1, 2: 0})\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nSEED = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=SEED)\n\nWe need to perform normalization before throwing the data into the model. Here we use the MinMaxScaler() from sklearn package.\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nX_train = mms.fit_transform(X_train, y_train)\nX_test = mms.transform(X_test)\n\nThen we write a Dataset class to build the dataset and create the dataloaders. Since the set is already split, we don’t need to random_split here.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MyData(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=float)\n        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)\n\n    def __getitem__(self, index):\n        return (self.X[index], self.y[index])\n\n    def __len__(self):\n        return len(self.y)\n\n\ntrain_set = MyData(X_train, y_train)\nval_set = MyData(X_test, y_test)\n\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32)\n\nIn the following code, we first set up the original model.\n\nimport torch.nn as nn\nfrom torch.nn.modules import Linear\n\nclass LoR(nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n        self.linear = Linear(in_features=22, out_features=1, dtype=float)\n        self.activation = nn.Sigmoid()\n\n    def forward(self, X):\n        # pred = self.activation(self.linear(X))\n        pred = self.linear(X)\n        # return (pred &gt;= 0).float()\n        return pred\n\nThen we derive the base ModelTemplate class.\n\nclass LoRModel(ModelTemplate):\n    def __init__(self, model, loss_fn, optimizer):\n        super().__init__(model, loss_fn, optimizer)\n        self.stats['acc_train'] = []\n        self.stats['acc_val'] = []\n\n    def compute_acc(self, dataloader):\n        with torch.no_grad():\n            acc = []\n            for X_batch, y_batch in dataloader:\n                yhat = torch.sigmoid(self.model(X_batch))\n                y_pred = (yhat&gt;=0.5).to(float)\n                acc.append((y_pred==y_batch).sum().item())\n            # print(acc_train)\n        return np.sum(acc)/len(dataloader.dataset)\n\n    def log_update(self, train_time, loss, val_time, val_loss, train_loader, val_loader):\n        super().log_update(train_time, loss, val_time, val_loss, train_loader, val_loader)\n        acc_train = self.compute_acc(train_loader)\n        acc_val = self.compute_acc(val_loader)\n        self.stats['acc_train'].append(acc_train)\n        self.stats['acc_val'].append(acc_val)\n\n\n        # p = self.model.state_dict()\n        # self.stats['acc'].append([p['linear.bias'].item(), p['linear.weight'].item()])\n\n    def log_output(self, verbose=0):\n        s = super().log_output(verbose=0, formatstr=':.6f')\n        s.append(f'acc_train: {self.stats['acc_train'][-1]:.6f}')\n        s.append(f'acc_val: {self.stats['acc_val'][-1]:.6f}')\n        # s.append(f'p: [{self.stats['p'][-1][0]:.6f}, {self.stats['p'][-1][1]:.6f}]')\n        if verbose == 1:\n            print(' '.join(s))\n        return s\n\n\nfrom torch.optim import SGD\nfrom torch.nn import BCEWithLogitsLoss, BCELoss\n\noriginal_model = LoR()\nmodel = LoRModel(model=original_model, loss_fn=BCEWithLogitsLoss(),\n                 optimizer=SGD(original_model.parameters(), lr = 0.1))\n\nmodel.train(train_loader, val_loader, epoch_num=100, verbose=1)\n\nepoch 1 train_time: 0.026031 loss: 0.692410 val_time: 0.003061 val_loss: 0.627087 acc_train: 0.627451 acc_val: 0.688889\nepoch 2 train_time: 0.010491 loss: 0.652692 val_time: 0.001000 val_loss: 0.609877 acc_train: 0.627451 acc_val: 0.688889\nepoch 3 train_time: 0.009510 loss: 0.635841 val_time: 0.002000 val_loss: 0.590118 acc_train: 0.627451 acc_val: 0.688889\nepoch 4 train_time: 0.007524 loss: 0.623492 val_time: 0.000997 val_loss: 0.581516 acc_train: 0.627451 acc_val: 0.688889\nepoch 5 train_time: 0.009110 loss: 0.614160 val_time: 0.002263 val_loss: 0.573444 acc_train: 0.627451 acc_val: 0.711111\nepoch 6 train_time: 0.010945 loss: 0.605191 val_time: 0.002010 val_loss: 0.567160 acc_train: 0.623529 acc_val: 0.711111\nepoch 7 train_time: 0.011597 loss: 0.595647 val_time: 0.002012 val_loss: 0.565006 acc_train: 0.650980 acc_val: 0.711111\nepoch 8 train_time: 0.010981 loss: 0.587077 val_time: 0.001997 val_loss: 0.559978 acc_train: 0.670588 acc_val: 0.688889\nepoch 9 train_time: 0.009526 loss: 0.581139 val_time: 0.004566 val_loss: 0.558800 acc_train: 0.686275 acc_val: 0.711111\nepoch 10 train_time: 0.010691 loss: 0.573550 val_time: 0.002012 val_loss: 0.552963 acc_train: 0.698039 acc_val: 0.688889\nepoch 11 train_time: 0.013057 loss: 0.567221 val_time: 0.001998 val_loss: 0.546816 acc_train: 0.701961 acc_val: 0.688889\nepoch 12 train_time: 0.011427 loss: 0.561712 val_time: 0.002520 val_loss: 0.542746 acc_train: 0.713725 acc_val: 0.666667\nepoch 13 train_time: 0.013263 loss: 0.556938 val_time: 0.002014 val_loss: 0.542486 acc_train: 0.737255 acc_val: 0.688889\nepoch 14 train_time: 0.011984 loss: 0.551774 val_time: 0.001000 val_loss: 0.541814 acc_train: 0.745098 acc_val: 0.688889\nepoch 15 train_time: 0.011615 loss: 0.548951 val_time: 0.000999 val_loss: 0.537785 acc_train: 0.741176 acc_val: 0.688889\nepoch 16 train_time: 0.012518 loss: 0.542220 val_time: 0.000999 val_loss: 0.534548 acc_train: 0.741176 acc_val: 0.688889\nepoch 17 train_time: 0.012544 loss: 0.538324 val_time: 0.001301 val_loss: 0.531510 acc_train: 0.737255 acc_val: 0.711111\nepoch 18 train_time: 0.012636 loss: 0.534326 val_time: 0.001989 val_loss: 0.529663 acc_train: 0.745098 acc_val: 0.711111\nepoch 19 train_time: 0.010523 loss: 0.530893 val_time: 0.001000 val_loss: 0.528623 acc_train: 0.745098 acc_val: 0.711111\nepoch 20 train_time: 0.013565 loss: 0.526439 val_time: 0.002002 val_loss: 0.526963 acc_train: 0.737255 acc_val: 0.711111\nepoch 21 train_time: 0.011507 loss: 0.523326 val_time: 0.000996 val_loss: 0.524094 acc_train: 0.741176 acc_val: 0.711111\nepoch 22 train_time: 0.010525 loss: 0.519909 val_time: 0.001011 val_loss: 0.520526 acc_train: 0.741176 acc_val: 0.711111\nepoch 23 train_time: 0.014093 loss: 0.517783 val_time: 0.002012 val_loss: 0.524199 acc_train: 0.760784 acc_val: 0.688889\nepoch 24 train_time: 0.008948 loss: 0.514302 val_time: 0.001123 val_loss: 0.519917 acc_train: 0.760784 acc_val: 0.688889\nepoch 25 train_time: 0.010410 loss: 0.511550 val_time: 0.002524 val_loss: 0.517309 acc_train: 0.760784 acc_val: 0.688889\nepoch 26 train_time: 0.011788 loss: 0.509695 val_time: 0.001085 val_loss: 0.518875 acc_train: 0.772549 acc_val: 0.711111\nepoch 27 train_time: 0.009179 loss: 0.506055 val_time: 0.002450 val_loss: 0.518615 acc_train: 0.776471 acc_val: 0.688889\nepoch 28 train_time: 0.013106 loss: 0.504401 val_time: 0.002412 val_loss: 0.513253 acc_train: 0.776471 acc_val: 0.711111\nepoch 29 train_time: 0.010534 loss: 0.501465 val_time: 0.001031 val_loss: 0.515084 acc_train: 0.780392 acc_val: 0.688889\nepoch 30 train_time: 0.010514 loss: 0.499432 val_time: 0.001988 val_loss: 0.511518 acc_train: 0.784314 acc_val: 0.711111\nepoch 31 train_time: 0.010056 loss: 0.497888 val_time: 0.002010 val_loss: 0.513222 acc_train: 0.784314 acc_val: 0.688889\nepoch 32 train_time: 0.010761 loss: 0.494562 val_time: 0.002010 val_loss: 0.507340 acc_train: 0.784314 acc_val: 0.711111\nepoch 33 train_time: 0.010147 loss: 0.493165 val_time: 0.001022 val_loss: 0.509089 acc_train: 0.784314 acc_val: 0.688889\nepoch 34 train_time: 0.011519 loss: 0.490244 val_time: 0.001817 val_loss: 0.509378 acc_train: 0.784314 acc_val: 0.688889\nepoch 35 train_time: 0.011365 loss: 0.489067 val_time: 0.002219 val_loss: 0.507884 acc_train: 0.780392 acc_val: 0.688889\nepoch 36 train_time: 0.011341 loss: 0.487054 val_time: 0.001994 val_loss: 0.509482 acc_train: 0.780392 acc_val: 0.688889\nepoch 37 train_time: 0.013555 loss: 0.484665 val_time: 0.002000 val_loss: 0.506997 acc_train: 0.780392 acc_val: 0.711111\nepoch 38 train_time: 0.009275 loss: 0.483160 val_time: 0.001002 val_loss: 0.506836 acc_train: 0.784314 acc_val: 0.688889\nepoch 39 train_time: 0.011241 loss: 0.481378 val_time: 0.001127 val_loss: 0.505223 acc_train: 0.784314 acc_val: 0.688889\nepoch 40 train_time: 0.012624 loss: 0.479956 val_time: 0.002517 val_loss: 0.503222 acc_train: 0.788235 acc_val: 0.688889\nepoch 41 train_time: 0.012966 loss: 0.478671 val_time: 0.001000 val_loss: 0.505004 acc_train: 0.784314 acc_val: 0.688889\nepoch 42 train_time: 0.011547 loss: 0.476852 val_time: 0.002002 val_loss: 0.503714 acc_train: 0.788235 acc_val: 0.688889\nepoch 43 train_time: 0.014060 loss: 0.475048 val_time: 0.002005 val_loss: 0.506144 acc_train: 0.788235 acc_val: 0.688889\nepoch 44 train_time: 0.010523 loss: 0.473206 val_time: 0.001998 val_loss: 0.501974 acc_train: 0.788235 acc_val: 0.711111\nepoch 45 train_time: 0.009708 loss: 0.470979 val_time: 0.001997 val_loss: 0.499234 acc_train: 0.788235 acc_val: 0.711111\nepoch 46 train_time: 0.009526 loss: 0.470202 val_time: 0.002000 val_loss: 0.500705 acc_train: 0.792157 acc_val: 0.711111\nepoch 47 train_time: 0.009771 loss: 0.468638 val_time: 0.001004 val_loss: 0.500395 acc_train: 0.800000 acc_val: 0.711111\nepoch 48 train_time: 0.014464 loss: 0.468042 val_time: 0.000915 val_loss: 0.500268 acc_train: 0.800000 acc_val: 0.711111\nepoch 49 train_time: 0.011622 loss: 0.466178 val_time: 0.002021 val_loss: 0.498534 acc_train: 0.800000 acc_val: 0.711111\nepoch 50 train_time: 0.009206 loss: 0.464770 val_time: 0.001993 val_loss: 0.496961 acc_train: 0.800000 acc_val: 0.711111\nepoch 51 train_time: 0.012317 loss: 0.463955 val_time: 0.002001 val_loss: 0.497893 acc_train: 0.800000 acc_val: 0.711111\nepoch 52 train_time: 0.014271 loss: 0.462824 val_time: 0.001990 val_loss: 0.499063 acc_train: 0.792157 acc_val: 0.711111\nepoch 53 train_time: 0.014168 loss: 0.461682 val_time: 0.002027 val_loss: 0.496441 acc_train: 0.800000 acc_val: 0.755556\nepoch 54 train_time: 0.012194 loss: 0.459662 val_time: 0.002997 val_loss: 0.492270 acc_train: 0.800000 acc_val: 0.733333\nepoch 55 train_time: 0.014059 loss: 0.458662 val_time: 0.002012 val_loss: 0.491803 acc_train: 0.800000 acc_val: 0.733333\nepoch 56 train_time: 0.012563 loss: 0.457229 val_time: 0.002481 val_loss: 0.491321 acc_train: 0.800000 acc_val: 0.733333\nepoch 57 train_time: 0.009601 loss: 0.456286 val_time: 0.001996 val_loss: 0.492217 acc_train: 0.800000 acc_val: 0.755556\nepoch 58 train_time: 0.010608 loss: 0.455783 val_time: 0.002020 val_loss: 0.487916 acc_train: 0.800000 acc_val: 0.733333\nepoch 59 train_time: 0.010141 loss: 0.453292 val_time: 0.000999 val_loss: 0.489003 acc_train: 0.800000 acc_val: 0.733333\nepoch 60 train_time: 0.012346 loss: 0.453518 val_time: 0.001011 val_loss: 0.490606 acc_train: 0.796078 acc_val: 0.755556\nepoch 61 train_time: 0.009809 loss: 0.451734 val_time: 0.001002 val_loss: 0.488805 acc_train: 0.800000 acc_val: 0.755556\nepoch 62 train_time: 0.013299 loss: 0.451545 val_time: 0.001581 val_loss: 0.487090 acc_train: 0.800000 acc_val: 0.755556\nepoch 63 train_time: 0.013508 loss: 0.449707 val_time: 0.001999 val_loss: 0.486808 acc_train: 0.800000 acc_val: 0.755556\nepoch 64 train_time: 0.016053 loss: 0.450538 val_time: 0.002020 val_loss: 0.486242 acc_train: 0.803922 acc_val: 0.755556\nepoch 65 train_time: 0.013196 loss: 0.448147 val_time: 0.001979 val_loss: 0.487293 acc_train: 0.803922 acc_val: 0.755556\nepoch 66 train_time: 0.012548 loss: 0.447152 val_time: 0.002000 val_loss: 0.485226 acc_train: 0.807843 acc_val: 0.755556\nepoch 67 train_time: 0.013009 loss: 0.447074 val_time: 0.002012 val_loss: 0.490903 acc_train: 0.800000 acc_val: 0.755556\nepoch 68 train_time: 0.014956 loss: 0.446093 val_time: 0.001959 val_loss: 0.491226 acc_train: 0.800000 acc_val: 0.733333\nepoch 69 train_time: 0.012352 loss: 0.444100 val_time: 0.001586 val_loss: 0.486315 acc_train: 0.811765 acc_val: 0.755556\nepoch 70 train_time: 0.013045 loss: 0.444518 val_time: 0.002509 val_loss: 0.486090 acc_train: 0.811765 acc_val: 0.755556\nepoch 71 train_time: 0.011571 loss: 0.442104 val_time: 0.001170 val_loss: 0.484866 acc_train: 0.811765 acc_val: 0.755556\nepoch 72 train_time: 0.011689 loss: 0.441630 val_time: 0.001127 val_loss: 0.484937 acc_train: 0.807843 acc_val: 0.755556\nepoch 73 train_time: 0.012974 loss: 0.440029 val_time: 0.001003 val_loss: 0.483483 acc_train: 0.811765 acc_val: 0.755556\nepoch 74 train_time: 0.012347 loss: 0.440303 val_time: 0.001000 val_loss: 0.483814 acc_train: 0.811765 acc_val: 0.755556\nepoch 75 train_time: 0.014063 loss: 0.438612 val_time: 0.001996 val_loss: 0.484908 acc_train: 0.807843 acc_val: 0.755556\nepoch 76 train_time: 0.009543 loss: 0.438110 val_time: 0.002003 val_loss: 0.482800 acc_train: 0.811765 acc_val: 0.755556\nepoch 77 train_time: 0.011945 loss: 0.437530 val_time: 0.001011 val_loss: 0.485628 acc_train: 0.803922 acc_val: 0.755556\nepoch 78 train_time: 0.011561 loss: 0.438161 val_time: 0.001986 val_loss: 0.485713 acc_train: 0.800000 acc_val: 0.755556\nepoch 79 train_time: 0.011048 loss: 0.434821 val_time: 0.002539 val_loss: 0.482151 acc_train: 0.807843 acc_val: 0.755556\nepoch 80 train_time: 0.013206 loss: 0.435002 val_time: 0.001999 val_loss: 0.483443 acc_train: 0.803922 acc_val: 0.755556\nepoch 81 train_time: 0.008998 loss: 0.434560 val_time: 0.001589 val_loss: 0.480240 acc_train: 0.807843 acc_val: 0.755556\nepoch 82 train_time: 0.013892 loss: 0.433661 val_time: 0.001005 val_loss: 0.478751 acc_train: 0.807843 acc_val: 0.755556\nepoch 83 train_time: 0.012849 loss: 0.432380 val_time: 0.000997 val_loss: 0.481034 acc_train: 0.803922 acc_val: 0.755556\nepoch 84 train_time: 0.013296 loss: 0.431976 val_time: 0.001001 val_loss: 0.478955 acc_train: 0.803922 acc_val: 0.755556\nepoch 85 train_time: 0.010300 loss: 0.432697 val_time: 0.002019 val_loss: 0.481476 acc_train: 0.803922 acc_val: 0.777778\nepoch 86 train_time: 0.010152 loss: 0.429912 val_time: 0.000998 val_loss: 0.479263 acc_train: 0.803922 acc_val: 0.755556\nepoch 87 train_time: 0.015875 loss: 0.430506 val_time: 0.003228 val_loss: 0.482495 acc_train: 0.803922 acc_val: 0.777778\nepoch 88 train_time: 0.013010 loss: 0.428551 val_time: 0.001598 val_loss: 0.478993 acc_train: 0.803922 acc_val: 0.777778\nepoch 89 train_time: 0.017349 loss: 0.427902 val_time: 0.001508 val_loss: 0.477756 acc_train: 0.803922 acc_val: 0.755556\nepoch 90 train_time: 0.023073 loss: 0.426869 val_time: 0.003977 val_loss: 0.477177 acc_train: 0.803922 acc_val: 0.755556\nepoch 91 train_time: 0.018015 loss: 0.427140 val_time: 0.001512 val_loss: 0.477494 acc_train: 0.803922 acc_val: 0.777778\nepoch 92 train_time: 0.016495 loss: 0.426070 val_time: 0.000998 val_loss: 0.477016 acc_train: 0.803922 acc_val: 0.777778\nepoch 93 train_time: 0.015220 loss: 0.425416 val_time: 0.001245 val_loss: 0.476892 acc_train: 0.803922 acc_val: 0.777778\nepoch 94 train_time: 0.012934 loss: 0.426518 val_time: 0.002001 val_loss: 0.476839 acc_train: 0.803922 acc_val: 0.777778\nepoch 95 train_time: 0.013817 loss: 0.424043 val_time: 0.002026 val_loss: 0.475227 acc_train: 0.803922 acc_val: 0.777778\nepoch 96 train_time: 0.012024 loss: 0.424209 val_time: 0.002005 val_loss: 0.476354 acc_train: 0.803922 acc_val: 0.777778\nepoch 97 train_time: 0.027023 loss: 0.423044 val_time: 0.002215 val_loss: 0.473174 acc_train: 0.803922 acc_val: 0.777778\nepoch 98 train_time: 0.015872 loss: 0.422057 val_time: 0.002020 val_loss: 0.471185 acc_train: 0.803922 acc_val: 0.755556\nepoch 99 train_time: 0.013004 loss: 0.422980 val_time: 0.002994 val_loss: 0.471742 acc_train: 0.803922 acc_val: 0.777778\nepoch 100 train_time: 0.010961 loss: 0.421178 val_time: 0.001197 val_loss: 0.471171 acc_train: 0.803922 acc_val: 0.777778",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/6/intro.html#exercises-and-projects",
    "href": "contents/6/intro.html#exercises-and-projects",
    "title": "6  Logistic regression",
    "section": "6.4 Exercises and Projects",
    "text": "6.4 Exercises and Projects\n\nExercise 6.1 Please hand write a report about the details of the math formulas for Logistic regression.\n\n\n\nExercise 6.2 CHOOSE ONE: Please use PyTorch to apply the LogisticRegression to one of the following datasets.\n\nthe iris dataset.\nthe dating dataset.\nthe titanic dataset.\n\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nWhat is the batch size do you use?\nPlot the learning curve (loss vs epochs, accuracy vs epochs).\nAnalyze the bias / variance status.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "contents/7/intro.html",
    "href": "contents/7/intro.html",
    "title": "7  Netural networks",
    "section": "",
    "text": "7.1 Neural network: Back propagation\nThere are many different architects of netural networks. In our course we will only talk about the simplest one: multilayer perceptron (MLP). We will treat it as the generalization of logistic regression. In other words, we will treat logistic regression as an one-layer netural network. Under this idea, all the concepts and ideas, like gradient descent, mini-batch training, loss functions, learning curves, etc.. will be used.\nTo train a MLP model, we still use gradient descent. Therefore it is very important to know how to compute the gradient. Actually the idea is the same as logistic regreesion. The only issue is that now the model is more complicated. The gradient computation is summrized as an algorithm called back propagation. It is described as follows.\nHere is an example of a Neural network with one hidden layer.\n\\(\\Theta\\) is the coefficients of the whole Neural network.\nThe dependency is as follows:\nEach layer is represented by the following diagram:\nThe diagram says:\n\\[\nz^{(k+1)}=b^{(k)}+\\Theta^{(k)}a^{(k)},\\quad z^{(k+1)}_j=b^{(k)}_j+\\sum \\Theta^{(k)}_{jl}a^{(k)}_l,\\quad a^{(k)}_j=\\sigma(z^{(k)}_j).\n\\]\nAssume \\(r,j\\geq1\\). Then\n\\[\n\\begin{aligned}\n\\diffp{z^{(k+1)}_i}{a^{(k)}_r}&=\\diffp*{\\left(b^{(k)}_i+\\sum\\Theta^{(k)}_{il}a^{(k)}_l\\right)}{a^{(k)}_r}=\\Theta_{ir}^{(k)},\\\\\n% \\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{ij}}&=\\diffp*{\\qty(a^{(k)}_0+\\sum\\Theta^{(k)}_{il}a^{(k)}_l)}{\\Theta^{(k)}_{ij}}=a^{(k)}_j,\\\\\n\\diffp{z^{(k+1)}_i}{z^{(k)}_j}&=\\sum_r \\diffp{z^{(k+1)}_i}{a^{k}_r}\\diffp{a^{(k)}_r}{z^{(k)}_j}+\\sum_{p,g}\\diffp{z^{(k+1)}_i}{\\Theta^{(k)}_{pq}}\\diffp{\\Theta^{(k)}_{pq}}{z^{(k)}_j}+\\sum_r \\diffp{z^{(k+1)}_i}{b^{k}_r}\\diffp{b^{(k)}_r}{z^{(k)}_j}\\\\\n&=\\sum_r \\Theta^{(k)}_{ir}\\diffp{a^{(k)}_r}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\diffp{a^{(k)}_j}{z^{(k)}_j}=\\Theta^{(k)}_{ij}\\sigma'(z^{(k)}_j),\\\\\n\\diffp{J}{z^{(k)}_j}&=\\sum_r \\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{z^{(k)}_j}=\\sum_r\\diffp{J}{z^{(k+1)}_r}\\Theta^{(k)}_{rj}\\sigma'(z^{(k)}_j).\n\\end{aligned}\n\\]\nWe set\nThen we have the following formula. Note that there are ``\\(z_0\\)’’ terms.\n\\[\n    \\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k).\n\\]\n\\[\n\\begin{aligned}\n\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}&=\\diffp*{\\left(b^{(k)}_r+\\sum_l\\Theta^{(k)}_{rl}a^{(k)}_l\\right)}{\\Theta^{(k)}_{pq}}=\\begin{cases}\n0&\\text{ for }r\\neq q,\\\\\na^{(k)}_q&\\text{ for }r=q,\n\\end{cases}\\\\\n\\diffp{J}{\\Theta^{(k)}_{pq}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{\\Theta^{(k)}_{pq}}=\\diffp{J}{z^{(k+1)}_p}\\diffp{z^{(k+1)}_p}{\\Theta^{(k)}_{pq}}=\\delta^{k+1}_pa^{k}_q,\\\\\n\\diffp{J}{b^{(k)}_{j}}&=\\sum_{r}\\diffp{J}{z^{(k+1)}_r}\\diffp{z^{(k+1)}_r}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}\\diffp{z^{(k+1)}_j}{b^{(k)}_{j}}=\\diffp{J}{z^{(k+1)}_j}=\\delta^{k+1}_j.\n\\end{aligned}\n\\]\nExtend \\(\\hat{\\Theta}=\\left[b^{(k)},\\Theta^{(k)}\\right]\\), and \\(\\partial^k J=\\left[\\diffp{J}{\\hat{\\Theta}^{(k)}_{ij}}\\right]\\). Then \\[\n    \\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right].\n\\] Then the algorithm is as follows.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/7/intro.html#neural-network-back-propagation",
    "href": "contents/7/intro.html#neural-network-back-propagation",
    "title": "7  Netural networks",
    "section": "",
    "text": "\\[\n\\newcommand\\diffp[2]{\\dfrac{\\partial #1}{\\partial #2}}\n\\]\n\n\n\n\n\n\n\\(a^{(1)}=\\hat{\\textbf{x}}\\) is the input. \\(a_0^{(1)}\\) is added. This is an \\((n+1)\\)-dimension column vector.\n\\(\\Theta^{(1)}\\) is the coefficient matrix from the input layer to the hidden layer, of size \\(k\\times(n+1)\\).\n\\(z^{(2)}=\\Theta^{(1)}a^{(1)}\\).\n\\(a^{(2)}=\\sigma(z^{(2)})\\), and then add \\(a^{(2)}_0\\). This is an \\((k+1)\\)-dimension column vector.\n\\(\\Theta^{(2)}\\) is the coefficient matrix from the hidden layer to the output layer, of size \\(r\\times(k+1)\\).\n\\(z^{(3)}=\\Theta^{(2)}a^{(2)}\\).\n\\(a^{(3)}=\\sigma(z^{(3)})\\). Since this is the output layer, \\(a^{(3)}_0\\) won’t be added. %\nThese \\(a^{(3)}\\) are \\(h_{\\Theta}(\\textbf{x})\\).\n\n\n\n\\(J\\) depends on \\(z^{(3)}\\) and \\(a^{(3)}\\).\n\\(z^{(3)}\\) and \\(a^{(3)}\\) depends on \\(\\Theta^{(2)}\\) and \\(a^{(2)}\\).\n\\(z^{(2)}\\) and \\(a^{(2)}\\) depends on \\(\\Theta^{(1)}\\) and \\(a^{(1)}\\).\n\\(J\\) depends on \\(\\Theta^{(1)}\\), \\(\\Theta^{(2)}\\) and \\(a^{(1)}\\).\n\n\n\n\n\n\n\n\n\n\\(\\delta^k_j=\\diffp{J}{z^{(k)}_j}\\), \\(\\delta^k=\\left[\\delta^k_1,\\delta_2^k,\\ldots\\right]^T\\).\n\\(\\mathbf{z}^k=\\left[z^{(k)}_1,z^{(k)}_2,\\ldots\\right]^T\\), \\(\\mathbf{a}^k=\\left[a^{(k)}_1,a^{(k)}_2,\\ldots\\right]^T\\), \\(\\hat{\\mathbf{a}}^k=\\left[a^{(k)}_0,a^{(k)}_1,\\ldots\\right]^T\\).\n\\(\\Theta^{k}=\\left[\\Theta^{(k)}_{ij}\\right]\\).\n\n\n\n\n\n\nStarting from \\(x\\), \\(y\\) and some random \\(\\Theta\\).\nForward computation: compute \\(z^{(k)}\\) and \\(a^{(k)}\\). The last \\(a^{(n)}\\) is \\(h\\).\nCompute \\(\\delta^n=\\nabla J\\circ\\sigma'(z^{(n)})\\). In the case of \\(J=\\frac12||{h-y}||^2\\), \\(\\nabla J=(a^{(n)}-y)\\), and then \\(\\delta^n=(a^{(n)}-y)\\circ\\sigma'(z^{(n)})\\).\nBackwards: \\(\\delta^k=\\left[(\\Theta^k)^T\\delta^{k+1}\\right]\\circ \\sigma'(\\mathbf{z}^k)\\), and \\(\\partial^k J=\\left[\\delta^{k+1}, \\delta^{k+1}(\\mathbf{a}^k)^T\\right]\\) .\n\n\nExample 7.1 Consider there are 3 layers: input, hidden and output. There are \\(m+1\\) nodes in the input layer, \\(n+1\\) nodes in the hidden layer and \\(k\\) in the output layer. Therefore\n\n\\(a^{(1)}\\) and \\(\\delta^1\\) are \\(m\\)-dim column vectors.\n\\(z^{(2)}\\), \\(a^{(2)}\\) and \\(\\delta^2\\) are \\(n\\)-dim column vectors.\n\\(z^{(3)}\\), \\(a^{(3)}\\) and \\(\\delta^3\\) are \\(k\\)-dim column vectors.\n\\(\\hat{\\Theta}^1\\) is \\(n\\times(m+1)\\), \\(\\hat{\\Theta}^2\\) is \\(k\\times(n+1)\\).\n\\(z^{(2)}=b^{(1)}+\\Theta^{(1)}a^{(1)}=\\hat{\\Theta}^{(1)}\\hat{a}^{(1)}\\), \\(z^{(3)}=b^{(2)}+\\Theta^{(2)}a^{(2)}=\\hat{\\Theta}^{(2)}\\hat{a}^{(2)}\\).\n\\(\\delta^3=\\nabla_aJ\\circ\\sigma'(z^{(3)})\\). This is a \\(k\\)-dim column vector.\n\\(\\partial^2 J=\\left[\\delta^3,\\delta^3(a^{(2)})^T\\right]\\).\n\\(\\delta^2=\\left[(\\Theta^2)^T\\delta^3\\right]\\circ \\sigma'(z^{(2)})\\), where \\((\\hat{\\Theta^2})^T\\delta^3=(\\hat{\\Theta^2})^T\\delta^3\\) and then remove the first row.\n\\(\\delta^1=\\begin{bmatrix}(\\Theta^1)^T\\delta^2\\end{bmatrix}\\circ \\sigma'(z^{(1)})\\), where \\((\\hat{\\Theta^1})^T\\delta^2=(\\hat{\\Theta^1})^T\\delta^2\\) and then remove the first row.\n\\(\\partial^1 J=\\left[\\delta^2,\\delta^2(a^{(1)})^T\\right]\\).\nWhen \\(J=-\\frac1m\\sum y\\ln a+(1-y)\\ln(1-a)\\), \\(\\delta^3=\\frac1m(\\sum a^{(3)}-\\sum y)\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/7/intro.html#example",
    "href": "contents/7/intro.html#example",
    "title": "7  Netural networks",
    "section": "7.2 Example",
    "text": "7.2 Example\nLet us take some of our old dataset as an example. This is an continuation of the horse colic dataset from Logistic regression. Note that most of the codes are directly taken from logistic regression section, since MLP is just a generalization of logistic regression.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, sep=r'\\s+', header=None)\ndf = df.replace(\"?\", np.NaN)\n\ndf = df.fillna(0)\ndf = df.drop(columns=[2, 24, 25, 26, 27])\ndf[23] = df[23].replace({1: 1, 2: 0})\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\nmms.fit(X_train)\nX_train = mms.transform(X_train)\nX_test = mms.transform(X_test)\n\nThe data is feed into the dataloader. Note that we change the batch size of the test dataloader to be the whole set, since I don’t want to do batch evaluation. This can be modified accordingly.\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MyDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, idx):\n        return (self.X[idx], self.y[idx])\n\ntrainloader = DataLoader(MyDataset(X_train, y_train), batch_size =32)\ntestloader = DataLoader(MyDataset(X_test, y_test), batch_size=X_test.shape[0])\n\nNow we build a neural network. This is a 2-layer model, with 1 hidden layer with 10 nodes. Since we are going to use BCEWithLogitsLoss, we don’t add the final activation function here in the model, but leave it to the loss function.\n\nimport torch.nn as nn\nclass MyModel(nn.Module):\n    def __init__(self, num_inputs):\n        super().__init__()\n        self.linear1 = nn.Linear(num_inputs, 10)\n        self.act1 = nn.ReLU()\n        self.linear2 = nn.Linear(10, 1)\n        # self.act2 = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.act1(x)\n        x = self.linear2(x)\n        # x = self.act2(x)\n        return x\n\nNow we start to train the model and evaluate. Note that the majority part of the code is about evaluating the result. Since we are doing binary classification, our result can be computed by checking whether our model output (before the final sigmoid function) is positive or negative. This is where (y_pred_test&gt;0) comes from.\nFor simplicity, when recording the training and validating results, I only record those from the last batch. This can be improved by designing a better result recorder.\n\nfrom torch.optim import SGD\nfrom torch.nn import BCEWithLogitsLoss\nfrom sklearn.metrics import accuracy_score\n\nEPOCHS = 500\nlearning_rate = 0.05\n\nmodel = MyModel(X.shape[1])\n\noptimizer = SGD(model.parameters(), lr=learning_rate)\nloss_fn = BCEWithLogitsLoss()\n\nloss_train = []\nloss_val = []\nacc_train = []\nacc_val = []\n\nfor epoch in range(EPOCHS):\n    model.train()\n    for X_batch, y_batch in trainloader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    with torch.no_grad():\n        loss_train.append(loss.item())\n        y_hat = (y_pred&gt;0).to(torch.float32)\n        acc_train.append(accuracy_score(y_hat, y_batch))\n        model.eval()\n        for X_test, y_test in testloader:\n            y_pred_test = model(X_test)\n        loss_test = loss_fn(y_pred_test, y_test)\n        loss_val.append(loss_test.item())\n        y_hat_test= (y_pred_test&gt;0).to(torch.float32)\n        acc_val.append(accuracy_score(y_hat_test, y_test))\n\nAnd the learning curve are shown in the following plots.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2)\nax[0].plot(loss_train, label='train_loss')\nax[0].plot(loss_val, label='val_loss')\nax[0].legend()\n\nax[1].plot(acc_train, label='train_acc')\nax[1].plot(acc_val, label='val_acc')\nax[1].legend()\n\n\n\n\n\n\n\n\nAs you may see, to build a netural network model it requires many testing. There are many established models. When you build your own architecture, you may start from there and modify it to fit your data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/7/intro.html#exercises-and-projects",
    "href": "contents/7/intro.html#exercises-and-projects",
    "title": "7  Netural networks",
    "section": "7.3 Exercises and Projects",
    "text": "7.3 Exercises and Projects\n\nExercise 7.1 Please hand write a report about the details of back propagation.\n\n\nExercise 7.2 CHOOSE ONE: Please use netural network to one of the following datasets. - the iris dataset. - the dating dataset. - the titanic dataset.\nPlease in addition answer the following questions.\n\nWhat is your accuracy score?\nHow many epochs do you use?\nWhat is the batch size do you use?\nPlot the learning curve (loss vs epochs, accuracy vs epochs).\nAnalyze the bias / variance status.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Netural networks</span>"
    ]
  },
  {
    "objectID": "contents/app/setup.html",
    "href": "contents/app/setup.html",
    "title": "Appendix A — Python IDE Setup",
    "section": "",
    "text": "A.1 VS Code\nWe choose VS Code as our IDE for Python. The installation is straightforward starting from the Official website. After installation, we need to do a few configurations to make the experience better.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python IDE Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/setup.html#sec-vscode",
    "href": "contents/app/setup.html#sec-vscode",
    "title": "Appendix A — Python IDE Setup",
    "section": "",
    "text": "For development of Python, the essential plugins are the Python plugin and Jupyter plugin. Each come with a bunch of other plugins.\n\n\n\nYou may start the terminal from top menu. Note that there are many different choices of terminals. After you start the first default terminal, you may start any new terminals using the small +.\n\n\n\n\n\n\n\n\nCautionChange Terminal default profile\n\n\n\n\n\nIn Windows the default terminal is Powershell when you first install VS Code. In the past it had bugs with conda (and I don’t know whether it is fixed now). In order to avoid headaches due to the bugs, we simply switch to other terminals, like Command Prompt. You may change the default terminal by\n\nPress Ctrl+Shift+P or F1 to open the top prompt menu.\nFind the Terminal: Select Default Profile command.\n\n\n\nSelect the desired terminal as the default one.\n\n\n\n\n\n\nSince many Python functionalities in VS Code depends on Python itself, we should turn to install Python now.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python IDE Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/setup.html#python-virtual-environment-installation",
    "href": "contents/app/setup.html#python-virtual-environment-installation",
    "title": "Appendix A — Python IDE Setup",
    "section": "A.2 Python Virtual Environment installation",
    "text": "A.2 Python Virtual Environment installation\nWe have three choices here: conda, pip and uv. Each tool has its pros and cons. After setting up, for this course they shouldn’t have any major differences.\n\n\n\n\n\n\nNoteconda\n\n\n\n\n\nFirst go to the Official website to download and install conda. I recommand using miniconda instead of anaconda to avoid installing tons of unnecessary packages.\nWhen installing, please choose Just Me and Add Miniconda3 to my Path environment variable. (If you choose All users it is possible that Add to Path var won’t show up.) The Add to Path var is not recommended because it might be complicated in some situation, but if this is your first installation of Python or you want to mainly use conda it is completely fine.\n\nYou may need to restart VS Code after the installation. Then open a terminal and use conda --version to test whether the installation is successful.\nThen use the following command to setup the first virtual environment for this course.\n\nCreate a new virtual environment. Note that your name can be anything and is not limited to venv:\n\nconda create --name venv\n\nActivate the virtual environment.\n\nconda activate venv\n\nInstall packages.\n\npandas, matplotlib and scikit-learn are packages for the course.\njupyter is a package used to power Jupyter notebook as well as the VS Code interative window. It is essential for homework for the course.\n\n\nconda install pandas matplotlib scikit-learn jupyter\n\n\n\n\n\n\n\n\n\nNotevenv\n\n\n\n\n\nThis is the default Python virtual environment tools. It is definitely not as good as the other tools, but in many situations that you cannot freely install applications this is your only choice. To use it, you should first have a Python installed. This Python can be from anywhere. For example, we could install it from the Official website.\nUnlike conda, venv by default creates local environmnet. It creates a folder in your working folder, and install packages inside this folder.\n\nCreate a new virtual environment. Here the last venv is the name of the virtual environment, as well as the name of the environment folder. You can freely change it to anything.\n\npython -m venv venv\n\nActivate the environment by venv\\scripts\\activate in Windows Command Prompt or source venv/bin/activate in Linux Bash.\nInstall packages.\n\npandas, matplotlib and scikit-learn are packages for the course.\njupyter is a package used to power Jupyter notebook as well as the VS Code interative window. It is essential for homework for the course.\n\n\npip install pandas matplotlib scikit-learn jupyter\n\n\n\n\n\n\n\n\n\nNoteuv\n\n\n\n\n\nuv is a new tool that can be used to manage Python virtual environments and packages. Its idea is very similar to the Python built-in tools and it can be used as a drop-in replacement of pip. With uv you don’t need to have any Python installed (just like conda). You may go to the Official website to download and install uv.\n\nCreate a new virtual environment. Here the last venv is the name of the virtual environment, as well as the name of the environment folder. You can freely change it to anything.\n\nuv venv venv\n\nActivate the environment by venv\\scripts\\activate in Windows Command Prompt or source venv/bin/activate in Linux Bash.\nInstall packages.\n\npandas, matplotlib and scikit-learn are packages for the course.\njupyter is a package used to power Jupyter notebook as well as the VS Code interative window. It is essential for homework for the course.\n\n\nuv pip install pandas matplotlib scikit-learn jupyter",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python IDE Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/setup.html#back-to-vs-code-and-start-jupyter-notebook",
    "href": "contents/app/setup.html#back-to-vs-code-and-start-jupyter-notebook",
    "title": "Appendix A — Python IDE Setup",
    "section": "A.3 Back to VS Code and start Jupyter notebook",
    "text": "A.3 Back to VS Code and start Jupyter notebook\nAfter Python is installed and virtual environment is set up, we could start to do a Hello World project.\nThe way VS Code organizes projects is through folders by default. So we first start a new folder (called the working folder) and all project related files should be put inside this folder. (There are ways to work with files outside the working folder, but you don’t need it in most cases.)\n\nThen you may create new files/folders or copy files/folders into this working folder. For demonstration a new file called helloworld.ipynb is created.\nipynb means this is a Jupyter notebook file (which is short for interactive Python notebook). This is the format for this course’s homework assignment.\nThis is the appearance of an empty notebook file.\n\nBefore using we need to attach the virtual environment we just created to this notebook. Click the Select Kernel on the right upper corner and select the environment we want.\n\n\n\n\n\n\n\nCautionExpose kernels\n\n\n\n\n\nBy default conda creates global virtual environment while pip/uv creates local virtual environment. You may always use the following method to expose your environments to other projects when necessary.\n\nMake sure ipykernel package is installed.\nIn the desired environment, run the command python -m ipykernel install --user --name venv --display-name \"Python (myenv)\".\nThen you may find it in VS Code as the name “Python (myenv)”.\n\n\n\n\nNow we could start using a notebook. The best part about the notebook is that it can acommendate codes and narratives and outputs together in one file.\n\n\n\n\n\n\n\nNoteQuick Markdown syntax\n\n\n\n\n\nMarkdown is a light weight text format. For this course we only use the most basic syntax.\n\nText Formatting\n\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n*italics*, **bold**, ***bold italics***\nitalics, bold, bold italics\n\n\nsuperscript^2^ / subscript~2~\nsuperscript2 / subscript2\n\n\n~~strikethrough~~\nstrikethrough\n\n\n`verbatim code`\nverbatim code\n\n\n\n\nHeadings {#headings}\n\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n# Heading 1\nHeading 1\n\n\n## Heading 2\nHeading 2\n\n\n### Heading 3\nHeading 3\n\n\n\n\nLists\n\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n* unordered list\n  + sub-item 1\n  + sub-item 2\n    - sub-sub-item 1\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1\n\n\n\n\n\n1. ordered list\n2. item 2\n   i) sub-item 1\n      A.  sub-sub-item 1\n\nordered list\nitem 2\n\nsub-item 1\n\nsub-sub-item 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python IDE Setup</span>"
    ]
  },
  {
    "objectID": "contents/app/setup.html#sec-googlecolab",
    "href": "contents/app/setup.html#sec-googlecolab",
    "title": "Appendix A — Python IDE Setup",
    "section": "A.4 Google Colab",
    "text": "A.4 Google Colab\nGoogle Colab is a product from Google Research, that allows anybody to write and execute arbitrary Python code through the browser, and is especially well suited to machine learning, data analysis and education.\nHere is the link to Google Colab. To use it you should have a Google account. Otherwise it is very simple to start, since a lot of packages for our course are already installed.\n\nA.4.1 Install packages\nIf you would like to install more packages, you can type the following code in a code cell and execute it.\n%pip install &lt;pkg name&gt;\n%conda install &lt;pkg name&gt;\nThe drawback here is that Google Colab can only stay for 24 hours. After that, all additionaly installed packages will be earsed. However you may put the installation code mentioned above at the beginning of your notebook and these packages will be installed every time you run the notebook.\n\n\nA.4.2 Upload files\nYou may directly upload files to the working directory of Google Colab. This has to be done in the browser. When working with these files, you may just use relative paths.\nThe drawback here is that Google Colab can only stay for 24 hours. After that, although your .ipynb files will be stores, all other files will be earsed.\n\n\nA.4.3 Mount Google Drive\nOne way to let the uploaded files stay in cloud is to upload them to Google Drive, and then load your Google Drive contents from Google Colab.\nGoole Drive is a cloud storage service provided by Google. When you register a Google account you will be automatically assigned a Google Drive account. You may get access to it from this link.\nHere are the steps to mount Google Drive:\n\nUpload your files to your Google Drive.\nRun the following codes in Colab code cells before you are loading the uploaded files:\n\n\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n\n\nA window pop up asking you about the permission. Authorize and the drive is mounted.\nTo work in directories, the most popular commands are\n\n%ls: list all files and folders in the working directory.\n%cd + folder name: Get into a specific folder.\n%cd..: Get into the parent folder. Then use these commands to find the files your just uploaded.\n\nFinally you may directly get access to those files just like they are in the working directory.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Python IDE Setup</span>"
    ]
  }
]