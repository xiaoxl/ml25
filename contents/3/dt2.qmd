## Decision Tree Project 2: `make_moons` dataset

`sklearn` includes various random sample generators that can be used to build artificial datasets of controlled size and complexity. We are going to use `make_moons` in this section. More details can be found [here](https://scikit-learn.org/stable/datasets/sample_generators.html).

`make_moons` generate 2d binary classification datasets that are challenging to certain algorithms (e.g. centroid-based clustering or linear classification), including optional Gaussian noise. `make_moons` produces two interleaving half circles. It is useful for visualization. 

Let us explorer the dataset first.

 
```{python}
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)
plt.scatter(x=X[:, 0], y=X[:, 1], c=y)
```
 
Now we are applying `sklearn.DecisionTreeClassifier` to construct the decision tree. The steps are as follows.

1. Split the dataset into training data and test data. 
2. Construct the pipeline. Since we won't apply any transformers there for this problem, we may just use the classifier `sklearn.DecisionTreeClassifier` directly without really construct the pipeline object.
3. Consider the hyperparameter space for grid search. For this problme we choose `min_samples_split` and `max_leaf_nodes` as the hyperparameters we need. We will let `min_samples_split` run through 2 to 5, and `max_leaf_nodes` run through 2 to 50. We will use `grid_search_cv` to find the best hyperparameter for our model. For cross-validation, the number of split is set to be `3` which means that we will run trainning 3 times for each pair of hyperparameters.
4. Run `grid_search_cv`. Find the best hyperparameters and the best estimator. Test it on the test set to get the accuracy score.


```{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```


```{python}
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

num_leaf = list(range(1, 100))
train_acc = []
test_acc = []
for i in num_leaf:
    clf = DecisionTreeClassifier(random_state=42, min_samples_leaf=i)
    clf.fit(X_train, y_train)
    train_acc.append(clf.score(X_train, y_train))
    test_acc.append(clf.score(X_test, y_test))
plt.plot(num_leaf, train_acc, label='training')
plt.plot(num_leaf, test_acc, label='testing')
plt.gca().set_xlim((max(num_leaf)+1, min(num_leaf)-1))
plt.legend()
```




```{python}

from sklearn.base import clone
def eval_ccp_alphas(clf, X, y):
    ccp_path = clf.cost_complexity_pruning_path(X, y)
    ccp_alphas, impurities = ccp_path.ccp_alphas, ccp_path.impurities
    clfs = []
    for ccp_alpha in ccp_alphas:
        clf_tmp = clone(clf)
        clf_tmp.set_params(ccp_alpha=ccp_alpha)
        clf_tmp.fit(X, y)
        clfs.append(clf_tmp)

    node_counts = [clf.tree_.node_count for clf in clfs]
    depth = [clf.tree_.max_depth for clf in clfs]
    train_acc = [clf.score(X_train, y_train) for clf in clfs]
    test_acc = [clf.score(X_test, y_test) for clf in clfs]

    fig, ax = plt.subplots(3, 1)
    ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
    ax[0].set_xlabel("alpha")
    ax[0].set_ylabel("number of nodes")
    ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
    ax[1].set_xlabel("alpha")
    ax[1].set_ylabel("depth of tree")
    ax[2].plot(ccp_alphas, train_acc, marker="o", drawstyle="steps-post", label='train')
    ax[2].plot(ccp_alphas, test_acc, marker="o", drawstyle="steps-post", label='test')
    ax[2].set_xlabel("alpha")
    ax[2].set_ylabel("accuracy")
    ax[2].legend()

    return ccp_alphas

clf = DecisionTreeClassifier(random_state=42)
eval_ccp_alphas(clf, X_train, y_train)
```




```{python}
# Step 3
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
import numpy as np

params = {'min_samples_split': list(range(2, 5)),
          'max_leaf_nodes': list(range(2, 50))}
grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), 
                              params, verbose=1, cv=3)
grid_search_cv.fit(X_train, y_train)
```



```{python}
# Step 4
from sklearn.metrics import accuracy_score

clf = grid_search_cv.best_estimator_
print(grid_search_cv.best_params_)
y_pred = clf.predict(X_test)
accuracy_score(y_pred, y_test)
```


Now you can see that for this `make_moons` dataset, the best decision tree should have at most `17` leaf nodes and the minimum number of samples required to be at a leaft node is `2`. The fitted decision tree can get 86.95% accuracy on the test set. 

Now we can plot the decision tree and the decision surface.


```{python}
from sklearn import tree
plt.figure(figsize=(15, 15), dpi=300)
tree.plot_tree(clf, filled=True)

```



```{python}
from sklearn.inspection import DecisionBoundaryDisplay

DecisionBoundaryDisplay.from_estimator(
    clf,
    X,
    cmap=plt.cm.RdYlBu,
    response_method="predict"
)
plt.scatter(
    X[:, 0],
    X[:, 1],
    c=y,
    cmap='gray',
    edgecolor="black",
    s=15,
    alpha=.15)
```

Since it is not very clear what the boundary looks like, I will draw the decision surface individually below.

```{python}
DecisionBoundaryDisplay.from_estimator(
    clf,
    X,
    cmap=plt.cm.RdYlBu,
    response_method="predict"
)
```


