<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Logistic regression – Machine Learning {{&lt; var info.date &gt;}}</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/7/intro.html" rel="next">
<link href="../../contents/5/intro.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-bbfe0a5b9fb228be1b45e71d50e4f997.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-0e3b29b5fa97dc5090eff3fd9e71dcd2.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-bbfe0a5b9fb228be1b45e71d50e4f997.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c0d2154c8f415210cf187502ecb0c845.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-abfcbbee754f2352fb90360e6f57f36e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-c0d2154c8f415210cf187502ecb0c845.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/bookup_fonts_gwf-0.0/fonts-embed.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning 2025 Fall</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/6/intro.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/1/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/2/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">k-Nearest Neighbors algorithm (k-NN)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/3/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/4/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Ensemble methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/5/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Intro to Pytorch</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/6/intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/7/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Netural networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/app/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Python IDE Setup</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#basic-idea" id="toc-basic-idea" class="nav-link active" data-scroll-target="#basic-idea"><span class="header-section-number">6.1</span> Basic idea</a>
  <ul class="collapse">
  <li><a href="#sigmoid-function" id="toc-sigmoid-function" class="nav-link" data-scroll-target="#sigmoid-function"><span class="header-section-number">6.1.1</span> Sigmoid function</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="header-section-number">6.1.2</span> Gradient descent</a></li>
  <li><a href="#the-formulas" id="toc-the-formulas" class="nav-link" data-scroll-target="#the-formulas"><span class="header-section-number">6.1.3</span> The Formulas</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">6.2</span> Regularization</a>
  <ul class="collapse">
  <li><a href="#three-types-of-errors" id="toc-three-types-of-errors" class="nav-link" data-scroll-target="#three-types-of-errors"><span class="header-section-number">6.2.1</span> Three types of errors</a></li>
  <li><a href="#underfit-vs-overfit" id="toc-underfit-vs-overfit" class="nav-link" data-scroll-target="#underfit-vs-overfit"><span class="header-section-number">6.2.2</span> Underfit vs Overfit</a></li>
  <li><a href="#learning-curves-accuracy-vs-training-size" id="toc-learning-curves-accuracy-vs-training-size" class="nav-link" data-scroll-target="#learning-curves-accuracy-vs-training-size"><span class="header-section-number">6.2.3</span> Learning curves (accuracy vs training size)</a></li>
  <li><a href="#regularization-1" id="toc-regularization-1" class="nav-link" data-scroll-target="#regularization-1"><span class="header-section-number">6.2.4</span> Regularization</a></li>
  </ul></li>
  <li><a href="#neural-network-implement-of-logistic-regression" id="toc-neural-network-implement-of-logistic-regression" class="nav-link" data-scroll-target="#neural-network-implement-of-logistic-regression"><span class="header-section-number">6.3</span> Neural network implement of Logistic regression</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"><span class="header-section-number">6.3.1</span> Example</a></li>
  </ul></li>
  <li><a href="#exercises-and-projects" id="toc-exercises-and-projects" class="nav-link" data-scroll-target="#exercises-and-projects"><span class="header-section-number">6.4</span> Exercises and Projects</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Logistic regression is very similar to linear regression, but applied to classification problems. In this chpater our idea is to treat it as the simplest example of a neural network instead of using other methods. The code we developped in the last chapter will be used extensively.</p>
<!-- 
Consider a set of training data $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots$, where $x^{(i)}=(x^{(i)}_1, x^{(i)}_2, \ldots, x^{(i)}_n)$ is a $n$-dim vector, and $y^{(i)}$ is a real number. We would like to use Linear regression to find the relation between $x$ and $y$. 

In this case, we assume that $y$ is a linear function of $x$:

$$
y=\theta_0 + \sum_{j=1}^n\theta_jx_j.
$$
The purpose of Linear regression is to used the given training data to find out the best $\Theta=(\theta_0, \theta_1, \theta_2,\ldots,\theta_n)$. 

If we set $\hat{x}=(1, x_1, \ldots,x_n)$, then the above formula can be reformulated by matrix multiplication.

$$
y=\Theta \hat{x}^T.
$$

When we want to deal with classification problem, we may still use this regression idea, but we have to do some modification.
 -->
<section id="basic-idea" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="basic-idea"><span class="header-section-number">6.1</span> Basic idea</h2>
<p>Assume that we have a binary classfification problem with <span class="math inline">\(N\)</span> features. Our model starts from the <em>logit</em> instead of the label <span class="math inline">\(y\)</span> itself.</p>
<p><span class="math display">\[
logit(y)=\theta_0+\sum_{j=1}^N\theta_jx_j.
\]</span></p>
<p>The logit function is used to describe the logorithm of the binary odds. The odd ratio is the ratio between the probability of success and the probability of failure. Assume the probability of success is <span class="math inline">\(p\)</span>. Then</p>
<p><span class="math display">\[
oddratio(p)=\frac{p}{1-p},\quad logit(p)=z = \log\left(\frac{p}{1-p}\right).
\]</span> We could solve the logit function, and get its inverse: the function is the <em>Sigmoid</em> function. Once we have the logit value, we could use it to get the probability. <span class="math display">\[
p=\sigma(z)=\frac{1}{1+\mathrm{e}^{-z}}.
\]</span> <!-- 
The Logsitic regression is used to predict the probability of a data point belonging to a specific class. It is based on linear regression. The major difference is that logistic regreesion will have an activation function $\sigma$ at the final stage to change the predicted values of the linear regression to the values that indicate classes. In the case of binary classification, the outcome of $\sigma$ will be between $0$ and $1$, which is related to the two classes respectively. In this case, the number is interepted as the probability of the data to be in one of the specific class. --></p>
<p>Therefore the model for Logistic regression is as follows:</p>
<p><span class="math display">\[
p=\sigma(L(x))=\sigma\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)=\sigma\left(\Theta \hat{x}^T\right).
\]</span></p>
<!-- In most cases, this activation function is chosen to be the Sigmoid funciton. -->
<section id="sigmoid-function" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="sigmoid-function"><span class="header-section-number">6.1.1</span> Sigmoid function</h3>
<p>The <em>Sigmoid</em> function is defined as follows:</p>
<p><span class="math display">\[
\sigma(z)=\frac{1}{1+\mathrm{e}^{-z}}.
\]</span> The graph of the function is shown below.</p>
<div id="41830e9c" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-2-output-1.png" width="571" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The main properties of <span class="math inline">\(\sigma\)</span> are listed below as a Lemma.</p>
<div id="lem-sig" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 6.1</strong></span> The Sigmoid function <span class="math inline">\(\sigma(z)\)</span> satisfies the following properties.</p>
<ol type="1">
<li><span class="math inline">\(\sigma(z)\rightarrow \infty\)</span> when <span class="math inline">\(z\mapsto \infty\)</span>.</li>
<li><span class="math inline">\(\sigma(z)\rightarrow -\infty\)</span> when <span class="math inline">\(z\mapsto -\infty\)</span>.</li>
<li><span class="math inline">\(\sigma(0)=0.5\)</span>.</li>
<li><span class="math inline">\(\sigma(z)\)</span> is always increasing.</li>
<li><span class="math inline">\(\sigma'(z)=\sigma(z)(1-\sigma(z))\)</span>.</li>
</ol>
</div>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>We will only look at the last one.</p>
<p><span class="math display">\[
\begin{split}
\sigma'(z)&amp;=-\frac{(1+\mathrm e^{-z})'}{(1+\mathrm e^{-z})^2}=\frac{\mathrm e^{-z}}{(1+\mathrm e^{-z})^2}=\frac{1}{1+\mathrm e^{-z}}\frac{\mathrm e^{-z}}{1+\mathrm e^{-z}}\\
&amp;=\sigma(z)\left(\frac{1+\mathrm e^{-z}}{1+\mathrm e^{-z}}-\frac{1}{1+\mathrm e^{-z}}\right)=\sigma(z)(1-\sigma(z)).
\end{split}
\]</span></p>
</div>
</section>
<section id="gradient-descent" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">6.1.2</span> Gradient descent</h3>
<!-- Assume that we would like to minimize a function $J(\Theta)$, where this $\Theta$ is an $N$-dim vector. Geometricly, we could treat $J$ as a height function, and it tells us the height of the mountain. Then to minimize $J$ is the same thing as to find the lowest point. One idea is to move towards the lowest point step by step. During each step we only need to lower our current height. After several steps we will be around the lowest point.

The geometric meaning of $\nabla J$ is the direction that $J$ increase the most. Therefore the opposite direction is the one we want to move in. The formula to update $x$ is 

$$
\Theta_{\text{new}} = \Theta_{\text{old}}-\alpha \nabla J(\Theta_{\text{old}}),
$$
where $\alpha$ is called the *learning rate* which controls how fast you want to learn. Usually if $\alpha$ is small, the learning tends to be slow and stble, and when $\alpha$ is big, the learning tends to be fast and unstable. -->
<!-- In machine learning, in most cases we would like to formulate the problem in terms of finding the lowest point of a *cost function* $J(\Theta)$.  -->
<p>We would like to use Gradient descent to sovle Logistic regression problems. For binary classification problem, the cost function is defined to be</p>
<p><span class="math display">\[
J(\Theta)=-\frac1m\sum_{i=1}^m\left[y^{(i)}\log(p^{(i)})+(1-y^{(i)})\log(1-p^{(i)})\right].
\]</span> Here <span class="math inline">\(m\)</span> is the number of data points, <span class="math inline">\(y^{(i)}\)</span> is the labelled result (which is either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>), <span class="math inline">\(p^{(i)}\)</span> is the predicted value (which is between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The algorithm gets its name since we are using the gradient to find a direction to lower our height.</p>
</div>
</div>
</section>
<section id="the-formulas" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="the-formulas"><span class="header-section-number">6.1.3</span> The Formulas</h3>
<div id="thm-reggrad" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1</strong></span> The gradient of <span class="math inline">\(J\)</span> is computed by</p>
<p><span id="eq-nablaJ"><span class="math display">\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.
\tag{6.1}\]</span></span></p>
</div>
<details>
<summary>
Click for details.
</summary>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The formula is an application of the chain rule for the multivariable functions.</p>
<p><span class="math display">\[
\begin{split}
\dfrac{\partial p}{\partial \theta_k}&amp;=\dfrac{\partial}{\partial \theta_k}\sigma\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)=\dfrac{\partial}{\partial \theta_k}\sigma(L(\Theta))\\
&amp;=\sigma(L)(1-\sigma(L))\dfrac{\partial}{\partial \theta_k}\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)\\
&amp;=\begin{cases}
p(1-p)&amp;\text{ if }k=0,\\
p(1-p)x_k&amp;\text{ otherwise}.
\end{cases}
\end{split}
\]</span> Then</p>
<p><span class="math display">\[
\nabla p = \left(\frac{\partial p}{\partial\theta_0},\ldots,\frac{\partial p}{\partial\theta_n}\right) = p(1-p)\hat{x}.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\nabla \log(p) = \frac{\nabla p}p =\frac{p(1-p)\hat{x}}{p}=(1-p)\hat{x}.
\]</span></p>
<p><span class="math display">\[
\nabla \log(1-p) = \frac{-\nabla p}{1-p} =-\frac{p(1-p)\hat{x}}{1-p}=-p\hat{x}.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\begin{split}
\nabla J&amp; = -\frac1m\sum_{i=1}^m\left[y^{(i)}\nabla \log(p^{(i)})+(1-y^{(i)})\nabla \log(1-p^{(i)})\right]\\
&amp;=-\frac1m\sum_{i=1}^m\left[y^{(i)}(1-p^{(i)})\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\hat{x}^{(i)})\right]\\
&amp;=-\frac1m\sum_{i=1}^m\left[(y^{(i)}-p^{(i)})\hat{x}^{(i)}\right].
\end{split}
\]</span></p>
<p>We write <span class="math inline">\(\hat{x}^{(i)}\)</span> as row vectors, and stack all these row vectors vertically. What we get is a matrix <span class="math inline">\(\hat{\textbf X}\)</span> of the size <span class="math inline">\(m\times (1+n)\)</span>. We stack all <span class="math inline">\(y^{(i)}\)</span> (resp. <span class="math inline">\(p^{(i)}\)</span>) vectically to get the <span class="math inline">\(m\)</span>-dim column vector <span class="math inline">\(\textbf y\)</span> (resp. <span class="math inline">\(\textbf p\)</span>).</p>
<p>Using this notation, the previous formula becomes</p>
<p><span class="math display">\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.
\]</span></p>
<p>After the gradient can be computed, we can start to use the gradient descent method. Note that, although <span class="math inline">\(\Theta\)</span> are not explicitly presented in the formula of <span class="math inline">\(\nabla J\)</span>, this is used to modify <span class="math inline">\(\Theta\)</span>:</p>
<p><span class="math display">\[
\Theta_{s+1} = \Theta_s - \alpha\nabla J.
\]</span></p>
</div>
</details>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you directly use library, like <code>sklearn</code> or <code>PyTorch</code>, they will handle the concrete computation of these gradients.</p>
</div>
</div>
</section>
</section>
<section id="regularization" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="regularization"><span class="header-section-number">6.2</span> Regularization</h2>
<section id="three-types-of-errors" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="three-types-of-errors"><span class="header-section-number">6.2.1</span> Three types of errors</h3>
<p>Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The <strong>bias</strong> of an estimator is its average error for different training sets. The <strong>variance</strong> of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.</p>
</section>
<section id="underfit-vs-overfit" class="level3 page-columns page-full" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="underfit-vs-overfit"><span class="header-section-number">6.2.2</span> Underfit vs Overfit</h3>
<p>When fit a model to data, it is highly possible that the model is underfit or overfit.</p>
<p>Roughly speaking, <strong>underfit</strong> means the model is not sufficient to fit the training samples, and <strong>overfit</strong> means that the models learns too many noise from the data. In many cases, high bias is related to underfit, and high variance is related to overfit.</p>
<p>The following example is from <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py">the <code>sklearn</code> guide</a>. Although it is a polynomial regression example, it grasps the key idea of underfit and overfit.</p>
<div id="dff74340" class="cell page-columns page-full" data-execution_count="2">
<div class="cell-output cell-output-display page-columns page-full">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="intro_files/figure-html/cell-3-output-1.png" width="1079" height="445" class="figure-img column-page"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="learning-curves-accuracy-vs-training-size" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="learning-curves-accuracy-vs-training-size"><span class="header-section-number">6.2.3</span> Learning curves (accuracy vs training size)</h3>
<p>A learning curve shows the validation and training score of an estimator for varying a key hyperparameter. In most cases the key hyperparameter is the training size or the number of epochs. It is a tool to find out how much we benefit from altering the hyperparameter by training more data or training for more epochs, and whether the estimator suffers more from a variance error or a bias error.</p>
<p><code>sklearn</code> provides <code>sklearn.model_selection.learning_curve()</code> to generate the values that are required to plot such a learning curve. However this function is just related to the sample size. If we would like to talk about epochs, we need other packages.</p>
<p>Let us first look at the learning curve about sample size. The official document page is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html">here</a>. The function takes input <code>estimator</code>, dataset <code>X</code>, <code>y</code>, and an arry-like argument <code>train_sizes</code>. The dataset <code>(X, y)</code> will be split into pieces using the cross-validation technique. The number of pieces is set by the argument <code>cv</code>. The default value is <code>cv=5</code>. For details about cross-validation please see <a href="../2/intro.html#sec-cross-validation" class="quarto-xref"><span>Section 2.2.5</span></a>.</p>
<p>Then the model is trained over a random sample of the training set, and evaluate the score over the test set. The size of the sample of the training set is set by the argument <code>train_sizes</code>. This argument is array-like. Therefore the process will be repeated several times, and we can see the impact of increasing the training size.</p>
<p>The output contains three pieces. The first is <code>train_sizes_abs</code> which is the number of elements in each training set. This output is mainly for reference. The difference between the output and the input <code>train_sizes</code> is that the input can be float which represents the percentagy. The output is always the exact number of elements.</p>
<p>The second output is <code>train_scores</code> and the third is <code>test_scores</code>, both of which are the scores we get from the training and testing process. Note that both are 2D <code>numpy</code> arrays, of the size <code>(number of different sizes, cv)</code>. Each row is a 1D <code>numpy</code> array representing the cross-validation scores, which is corresponding to a train size. If we want the mean as the cross-validation score, we could use <code>train_scores.mean(axis=1)</code>.</p>
<p>After understanding the input and output, we could plot the learning curve. We still use the <code>horse colic</code> as the example. The details about the dataset can be found <a href="https://xiaoxl.github.io/Datasets/contents/horse_colic.html">here</a>.</p>
<div id="43fa92ec" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>filepath <span class="op">=</span> <span class="st">"assests/datasets/horse_colic_clean.csv"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(filepath)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.iloc[:, :<span class="dv">22</span>].to_numpy().astype(<span class="bu">float</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (df.iloc[:, <span class="dv">22</span>]<span class="op">&lt;</span><span class="dv">2</span>).to_numpy().astype(<span class="bu">int</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span>SEED)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We use the model <code>LogisticRegression</code>. The following code plot the learning curve for this model.</p>
<div id="f95b71dc" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> [(<span class="st">'scalar'</span>, MinMaxScaler()),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>         (<span class="st">'log'</span>, clf)]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> Pipeline(steps<span class="op">=</span>steps)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(pipe, X_train, y_train,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>                                                        train_sizes<span class="op">=</span>np.linspace(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">20</span>))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, train_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'train'</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, test_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'test'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-5-output-1.png" width="588" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The learning curve is a primary tool for us to study the bias and variance. Usually</p>
<ul>
<li>If the two training curve and the testing curve are very close to each other, this means that the variance is low. Otherwise the variance is high, and this means that the model probabily suffer from overfitting.</li>
<li>If the absolute training curve score is high, this means that the bias is low. Otherwise the bias is high, and this means that the model probabily suffer from underfitting.</li>
</ul>
<p>In the above example, although regularization is applied by default, you may still notice some overfitting there.</p>
</section>
<section id="regularization-1" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="regularization-1"><span class="header-section-number">6.2.4</span> Regularization</h3>
<p>Regularization is a technique to deal with overfitting. Here we only talk about the simplest method: ridge regression, also known as Tikhonov regularizaiton. Because of the formula given below, it is also called <em><span class="math inline">\(L_2\)</span> regularization</em>. The idea is to add an additional term <span class="math inline">\(\dfrac{\alpha}{2m}\sum_{i=1}^m\theta_i^2\)</span> to the original cost function. When training with the new cost function, this additional term will force the parameters in the original term to be as small as possible. After finishing training, the additional term will be dropped, and we use the original cost function for validation and testing. Note that in the additional term <span class="math inline">\(\theta_0\)</span> is not presented.</p>
<p>The hyperparameter <span class="math inline">\(\alpha\)</span> is the <em>regularization strength</em>. If <span class="math inline">\(\alpha=0\)</span>, the new cost function becomes the original one; If <span class="math inline">\(\alpha\)</span> is very large, the additional term dominates, and it will force all parameters to be almost <span class="math inline">\(0\)</span>. In different context, the regularization strength is also given by <span class="math inline">\(C=\dfrac{1}{2\alpha}\)</span>, called <em>inverse of regularization strength</em>.</p>
<section id="the-math-of-regularization" class="level4" data-number="6.2.4.1">
<h4 data-number="6.2.4.1" class="anchored" data-anchor-id="the-math-of-regularization"><span class="header-section-number">6.2.4.1</span> The math of regularization</h4>
<div id="thm-ridgegrad" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.2</strong></span> The gradient of the ridge regression cost function is</p>
<p><span class="math display">\[
\nabla J=\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}+\frac{\alpha}{m}\Theta.
\]</span></p>
<p>Note that <span class="math inline">\(\Theta\)</span> doesn’t contain <span class="math inline">\(\theta_0\)</span>, or you may treat <span class="math inline">\(\theta_0=0\)</span>.</p>
</div>
<p>The computation is straightforward.</p>
</section>
<section id="the-code" class="level4" data-number="6.2.4.2">
<h4 data-number="6.2.4.2" class="anchored" data-anchor-id="the-code"><span class="header-section-number">6.2.4.2</span> The code</h4>
<p>Regularization is directly provided by the logistic regression functions.</p>
<ul>
<li>In <code>LogisticRegression</code>, the regularization is given by the argument <code>penalty</code> and <code>C</code>. <code>penalty</code> specifies the regularizaiton method. It is <code>l2</code> by default, which is the method above. <code>C</code> is the inverse of regularization strength, whose default value is <code>1</code>.</li>
<li>In <code>SGDClassifier</code>, the regularization is given by the argument <code>penalty</code> and <code>alpha</code>. <code>penalty</code> is the same as that in <code>LogisticRegression</code>, and <code>alpha</code> is the regularization strength, whose default value is <code>0.0001</code>.</li>
</ul>
<p>Let us see the above example.</p>
<div id="27114108" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span><span class="dv">1000</span>, C<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> [(<span class="st">'scalar'</span>, MinMaxScaler()),</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>         (<span class="st">'log'</span>, clf)]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> Pipeline(steps<span class="op">=</span>steps)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> learning_curve</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>train_sizes, train_scores, test_scores <span class="op">=</span> learning_curve(pipe, X_train, y_train,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                                                        train_sizes<span class="op">=</span>np.linspace(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">20</span>))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, train_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'train'</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.plot(train_sizes, test_scores.mean(axis<span class="op">=</span><span class="dv">1</span>), label<span class="op">=</span><span class="st">'test'</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-6-output-1.png" width="579" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>After we reduce <code>C</code> from <code>1</code> to <code>0.1</code>, the regularization strength is increased. Then you may find that the gap between the two curves are reduced. However the overall performace is also reduced, from 85%~90% in <code>C=1</code> case to around 80% in <code>C=0.1</code> case. This means that the model doesn’t fit the data well as the previous one. Therefore this is a trade-off: decrease the variance but increase the bias.</p>
</section>
</section>
</section>
<section id="neural-network-implement-of-logistic-regression" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="neural-network-implement-of-logistic-regression"><span class="header-section-number">6.3</span> Neural network implement of Logistic regression</h2>
<p>In the previous sections, we use gradient descent to run the Logistic regression model. We mentioned some important concepts, like epochs, mini-batch, etc.. We are going to use <code>PyTorch</code> to implement it. We will reuse many codes we wrote in the previous chapter.</p>
<section id="example" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="example"><span class="header-section-number">6.3.1</span> Example</h3>
<p>We still use the horse colic dataset as an example. We first prepare the dataset.</p>
<div id="2d0ae807" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>filepath <span class="op">=</span> <span class="st">"assests/datasets/horse_colic_clean.csv"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(filepath)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.iloc[:, :<span class="dv">22</span>].to_numpy().astype(<span class="bu">float</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (df.iloc[:, <span class="dv">22</span>]<span class="op">&lt;</span><span class="dv">2</span>).to_numpy().astype(<span class="bu">int</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span>SEED)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We need to perform normalization before throwing the data into the model. Here we use the <code>MinMaxScaler()</code> from <code>sklearn</code> package.</p>
<div id="ecb37802" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mms <span class="op">=</span> MinMaxScaler()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> mms.fit_transform(X_train, y_train)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> mms.transform(X_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Then we write a <code>Dataset</code> class to build the dataset and create the dataloaders. Since the set is already split, we don’t need to <code>random_split</code> here.</p>
<div id="6ed85356" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyData(Dataset):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, X, y):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.X <span class="op">=</span> torch.tensor(X, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y <span class="op">=</span> torch.tensor(y, dtype<span class="op">=</span><span class="bu">float</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="va">self</span>.X[index], <span class="va">self</span>.y[index])</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.y)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>train_set <span class="op">=</span> MyData(X_train, y_train)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>val_set <span class="op">=</span> MyData(X_test, y_test)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_set, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_set, batch_size<span class="op">=</span><span class="dv">32</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In the following code, we first set up the original model.</p>
<div id="c6eabf89" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.modules <span class="im">import</span> Linear</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoR(nn.Module):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(in_features<span class="op">=</span><span class="dv">22</span>, out_features<span class="op">=</span><span class="dv">1</span>, dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pred = self.activation(self.linear(X))</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> <span class="va">self</span>.linear(X)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return (pred &gt;= 0).float()</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pred</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Then we run our regular training loop.</p>
<div id="c035d38d" class="cell" data-output-fold="true" data-output-summary="Click to view results" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> SGD</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> BCEWithLogitsLoss</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LoR()</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> BCEWithLogitsLoss()</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Meter:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, total<span class="op">=</span><span class="fl">0.0</span>, count<span class="op">=</span><span class="dv">0</span>, value<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.total <span class="op">=</span> total</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.count <span class="op">=</span> count</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> value</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avg <span class="op">=</span> <span class="va">self</span>.total <span class="op">/</span> <span class="va">self</span>.count <span class="cf">if</span> <span class="va">self</span>.count <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, value, n<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> value</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.total <span class="op">+=</span> value <span class="op">*</span> n</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.count <span class="op">+=</span> n</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avg <span class="op">=</span> <span class="va">self</span>.total <span class="op">/</span> <span class="va">self</span>.count <span class="cf">if</span> <span class="va">self</span>.count <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="fl">0.0</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> {<span class="st">'loss'</span>: [], <span class="st">'acc'</span>: [], <span class="st">'loss_test'</span>: [], <span class="st">'acc_test'</span>: []}</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    monitor_loss <span class="op">=</span> Meter()</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    monitor_loss_test <span class="op">=</span> Meter()</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    monitor_acc <span class="op">=</span> Meter()</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    monitor_acc_test <span class="op">=</span> Meter()</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    monitor_time <span class="op">=</span> Meter()</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X_batch, y_batch) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        optim.zero_grad()</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> model(X_batch)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(p, y_batch)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        optim.step()</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        t1 <span class="op">=</span> time.perf_counter()</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> (p<span class="op">&gt;</span><span class="dv">0</span>).to(torch.<span class="bu">long</span>)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>            acc <span class="op">=</span> (pred <span class="op">==</span> y_batch).to(torch.<span class="bu">float</span>).mean().item()</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>            monitor_acc.update(acc, n<span class="op">=</span>X_batch.shape[<span class="dv">0</span>])</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>            monitor_loss.update(loss.item(), n<span class="op">=</span>X_batch.shape[<span class="dv">0</span>])</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>            monitor_time.update(t1<span class="op">-</span>t0, n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, batch: </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(train_loader)<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'time: </span><span class="sc">{</span>monitor_time<span class="sc">.</span>value<span class="sc">: .4f}</span><span class="ss"> (</span><span class="sc">{</span>monitor_time<span class="sc">.</span>total<span class="sc">: .4f}</span><span class="ss">) '</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'loss: </span><span class="sc">{</span>monitor_loss<span class="sc">.</span>value<span class="sc">: .4f}</span><span class="ss"> (</span><span class="sc">{</span>monitor_loss<span class="sc">.</span>avg<span class="sc">: .4f}</span><span class="ss">) '</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'acc: </span><span class="sc">{</span>monitor_acc<span class="sc">.</span>value<span class="sc">: .2f}</span><span class="ss"> (</span><span class="sc">{</span>monitor_acc<span class="sc">.</span>avg<span class="sc">: .2f}</span><span class="ss">)'</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'loss'</span>].append(monitor_loss.avg)</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    history[<span class="st">'acc'</span>].append(monitor_acc.avg)</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X_batch_test, y_batch_test <span class="kw">in</span> val_loader:</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>            p <span class="op">=</span> model(X_batch_test)</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>            loss_test <span class="op">=</span> loss_fn(p, y_batch_test)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>            monitor_loss_test.update(loss_test.item(), n<span class="op">=</span>X_batch_test.shape[<span class="dv">0</span>])</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>            pred_test <span class="op">=</span> (p<span class="op">&gt;</span><span class="dv">0</span>).to(torch.<span class="bu">int</span>)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>            acc_test <span class="op">=</span> ( pred_test <span class="op">==</span> y_batch_test).to(torch.<span class="bu">float</span>).mean().item()</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>            monitor_acc_test.update(acc_test, n<span class="op">=</span>X_batch_test.shape[<span class="dv">0</span>])</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'test epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> '</span></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'test loss: </span><span class="sc">{</span>monitor_loss_test<span class="sc">.</span>avg<span class="sc">: .4f}</span><span class="ss"> '</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f'test acc: </span><span class="sc">{</span>monitor_acc_test<span class="sc">.</span>avg<span class="sc">: .2f}</span><span class="ss">'</span></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'loss_test'</span>].append(monitor_loss_test.avg)</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>        history[<span class="st">'acc_test'</span>].append(monitor_acc_test.avg)</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>fig.set_size_inches((<span class="dv">10</span>,<span class="dv">3</span>))</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'training_loss'</span>)</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(history[<span class="st">'loss_test'</span>], label<span class="op">=</span><span class="st">'testing_loss'</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].legend()</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(history[<span class="st">'acc'</span>], label<span class="op">=</span><span class="st">'training_acc'</span>)</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].plot(history[<span class="st">'acc_test'</span>], label<span class="op">=</span><span class="st">'testing_acc'</span>)</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].legend()</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_title(<span class="st">'Loss'</span>)<span class="op">;</span></span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_title(<span class="st">'Accuracy'</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<details><summary>Click to view results</summary>
<div class="cell-output cell-output-stdout">
<pre><code>epoch: 0, batch: 1/10 time:  0.0022 ( 0.0022) loss:  0.6584 ( 0.6584) acc:  0.75 ( 0.75)
epoch: 0, batch: 2/10 time:  0.0005 ( 0.0027) loss:  0.7044 ( 0.6814) acc:  0.59 ( 0.67)
epoch: 0, batch: 3/10 time:  0.0003 ( 0.0030) loss:  0.6570 ( 0.6733) acc:  0.72 ( 0.69)
epoch: 0, batch: 4/10 time:  0.0003 ( 0.0033) loss:  0.7152 ( 0.6838) acc:  0.56 ( 0.66)
epoch: 0, batch: 5/10 time:  0.0003 ( 0.0035) loss:  0.6857 ( 0.6842) acc:  0.56 ( 0.64)
epoch: 0, batch: 6/10 time:  0.0003 ( 0.0038) loss:  0.6566 ( 0.6796) acc:  0.66 ( 0.64)
epoch: 0, batch: 7/10 time:  0.0003 ( 0.0041) loss:  0.6427 ( 0.6743) acc:  0.66 ( 0.64)
epoch: 0, batch: 8/10 time:  0.0003 ( 0.0043) loss:  0.6190 ( 0.6674) acc:  0.72 ( 0.65)
epoch: 0, batch: 9/10 time:  0.0003 ( 0.0046) loss:  0.6959 ( 0.6706) acc:  0.50 ( 0.64)
epoch: 0, batch: 10/10 time:  0.0003 ( 0.0049) loss:  0.7219 ( 0.6745) acc:  0.38 ( 0.62)
test epoch 0 test loss:  0.6541 test acc:  0.73
epoch: 1, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.6616 ( 0.6616) acc:  0.75 ( 0.75)
epoch: 1, batch: 2/10 time:  0.0003 ( 0.0005) loss:  0.6560 ( 0.6588) acc:  0.66 ( 0.70)
epoch: 1, batch: 3/10 time:  0.0003 ( 0.0008) loss:  0.6504 ( 0.6560) acc:  0.81 ( 0.74)
epoch: 1, batch: 4/10 time:  0.0003 ( 0.0011) loss:  0.6697 ( 0.6594) acc:  0.66 ( 0.72)
epoch: 1, batch: 5/10 time:  0.0003 ( 0.0014) loss:  0.6172 ( 0.6510) acc:  0.78 ( 0.73)
epoch: 1, batch: 6/10 time:  0.0003 ( 0.0016) loss:  0.6446 ( 0.6499) acc:  0.62 ( 0.71)
epoch: 1, batch: 7/10 time:  0.0003 ( 0.0019) loss:  0.6477 ( 0.6496) acc:  0.62 ( 0.70)
epoch: 1, batch: 8/10 time:  0.0003 ( 0.0022) loss:  0.6102 ( 0.6447) acc:  0.69 ( 0.70)
epoch: 1, batch: 9/10 time:  0.0003 ( 0.0024) loss:  0.6013 ( 0.6399) acc:  0.75 ( 0.70)
epoch: 1, batch: 10/10 time:  0.0003 ( 0.0027) loss:  0.6371 ( 0.6396) acc:  0.62 ( 0.70)
test epoch 1 test loss:  0.6211 test acc:  0.62
epoch: 2, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.6241 ( 0.6241) acc:  0.66 ( 0.66)
epoch: 2, batch: 2/10 time:  0.0003 ( 0.0005) loss:  0.6928 ( 0.6584) acc:  0.59 ( 0.62)
epoch: 2, batch: 3/10 time:  0.0005 ( 0.0010) loss:  0.6390 ( 0.6520) acc:  0.62 ( 0.62)
epoch: 2, batch: 4/10 time:  0.0004 ( 0.0014) loss:  0.5751 ( 0.6328) acc:  0.81 ( 0.67)
epoch: 2, batch: 5/10 time:  0.0006 ( 0.0019) loss:  0.6836 ( 0.6429) acc:  0.53 ( 0.64)
epoch: 2, batch: 6/10 time:  0.0005 ( 0.0024) loss:  0.5619 ( 0.6294) acc:  0.75 ( 0.66)
epoch: 2, batch: 7/10 time:  0.0004 ( 0.0028) loss:  0.5764 ( 0.6218) acc:  0.75 ( 0.67)
epoch: 2, batch: 8/10 time:  0.0004 ( 0.0032) loss:  0.6054 ( 0.6198) acc:  0.62 ( 0.67)
epoch: 2, batch: 9/10 time:  0.0003 ( 0.0035) loss:  0.6098 ( 0.6187) acc:  0.66 ( 0.67)
epoch: 2, batch: 10/10 time:  0.0003 ( 0.0038) loss:  0.6017 ( 0.6174) acc:  0.67 ( 0.67)
test epoch 2 test loss:  0.6008 test acc:  0.68
epoch: 3, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5425 ( 0.5425) acc:  0.81 ( 0.81)
epoch: 3, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.5580 ( 0.5503) acc:  0.72 ( 0.77)
epoch: 3, batch: 3/10 time:  0.0003 ( 0.0008) loss:  0.6470 ( 0.5825) acc:  0.62 ( 0.72)
epoch: 3, batch: 4/10 time:  0.0003 ( 0.0011) loss:  0.6384 ( 0.5965) acc:  0.62 ( 0.70)
epoch: 3, batch: 5/10 time:  0.0003 ( 0.0014) loss:  0.5693 ( 0.5910) acc:  0.69 ( 0.69)
epoch: 3, batch: 6/10 time:  0.0003 ( 0.0016) loss:  0.5759 ( 0.5885) acc:  0.72 ( 0.70)
epoch: 3, batch: 7/10 time:  0.0003 ( 0.0019) loss:  0.5581 ( 0.5842) acc:  0.72 ( 0.70)
epoch: 3, batch: 8/10 time:  0.0003 ( 0.0022) loss:  0.6703 ( 0.5949) acc:  0.59 ( 0.69)
epoch: 3, batch: 9/10 time:  0.0003 ( 0.0024) loss:  0.5802 ( 0.5933) acc:  0.75 ( 0.69)
epoch: 3, batch: 10/10 time:  0.0003 ( 0.0027) loss:  0.6372 ( 0.5967) acc:  0.54 ( 0.68)
test epoch 3 test loss:  0.5868 test acc:  0.71
epoch: 4, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.6048 ( 0.6048) acc:  0.69 ( 0.69)
epoch: 4, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.5619 ( 0.5833) acc:  0.75 ( 0.72)
epoch: 4, batch: 3/10 time:  0.0004 ( 0.0010) loss:  0.5671 ( 0.5779) acc:  0.75 ( 0.73)
epoch: 4, batch: 4/10 time:  0.0003 ( 0.0013) loss:  0.5956 ( 0.5823) acc:  0.75 ( 0.73)
epoch: 4, batch: 5/10 time:  0.0004 ( 0.0017) loss:  0.5915 ( 0.5841) acc:  0.72 ( 0.73)
epoch: 4, batch: 6/10 time:  0.0003 ( 0.0021) loss:  0.5150 ( 0.5726) acc:  0.91 ( 0.76)
epoch: 4, batch: 7/10 time:  0.0003 ( 0.0024) loss:  0.6464 ( 0.5832) acc:  0.59 ( 0.74)
epoch: 4, batch: 8/10 time:  0.0003 ( 0.0027) loss:  0.5387 ( 0.5776) acc:  0.75 ( 0.74)
epoch: 4, batch: 9/10 time:  0.0003 ( 0.0030) loss:  0.6427 ( 0.5848) acc:  0.59 ( 0.72)
epoch: 4, batch: 10/10 time:  0.0003 ( 0.0033) loss:  0.5574 ( 0.5827) acc:  0.79 ( 0.73)
test epoch 4 test loss:  0.5750 test acc:  0.73
epoch: 5, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5797 ( 0.5797) acc:  0.69 ( 0.69)
epoch: 5, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.5209 ( 0.5503) acc:  0.81 ( 0.75)
epoch: 5, batch: 3/10 time:  0.0003 ( 0.0009) loss:  0.6054 ( 0.5687) acc:  0.72 ( 0.74)
epoch: 5, batch: 4/10 time:  0.0003 ( 0.0013) loss:  0.5838 ( 0.5725) acc:  0.62 ( 0.71)
epoch: 5, batch: 5/10 time:  0.0003 ( 0.0016) loss:  0.5455 ( 0.5671) acc:  0.78 ( 0.72)
epoch: 5, batch: 6/10 time:  0.0003 ( 0.0019) loss:  0.6077 ( 0.5738) acc:  0.66 ( 0.71)
epoch: 5, batch: 7/10 time:  0.0003 ( 0.0022) loss:  0.5492 ( 0.5703) acc:  0.84 ( 0.73)
epoch: 5, batch: 8/10 time:  0.0003 ( 0.0026) loss:  0.6037 ( 0.5745) acc:  0.62 ( 0.72)
epoch: 5, batch: 9/10 time:  0.0003 ( 0.0029) loss:  0.6508 ( 0.5830) acc:  0.56 ( 0.70)
epoch: 5, batch: 10/10 time:  0.0003 ( 0.0033) loss:  0.4544 ( 0.5731) acc:  0.88 ( 0.71)
test epoch 5 test loss:  0.5655 test acc:  0.73
epoch: 6, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5311 ( 0.5311) acc:  0.75 ( 0.75)
epoch: 6, batch: 2/10 time:  0.0004 ( 0.0008) loss:  0.5566 ( 0.5438) acc:  0.69 ( 0.72)
epoch: 6, batch: 3/10 time:  0.0003 ( 0.0011) loss:  0.5546 ( 0.5474) acc:  0.75 ( 0.73)
epoch: 6, batch: 4/10 time:  0.0003 ( 0.0014) loss:  0.5244 ( 0.5417) acc:  0.84 ( 0.76)
epoch: 6, batch: 5/10 time:  0.0003 ( 0.0017) loss:  0.5732 ( 0.5480) acc:  0.72 ( 0.75)
epoch: 6, batch: 6/10 time:  0.0003 ( 0.0019) loss:  0.6045 ( 0.5574) acc:  0.62 ( 0.73)
epoch: 6, batch: 7/10 time:  0.0003 ( 0.0022) loss:  0.5773 ( 0.5602) acc:  0.72 ( 0.73)
epoch: 6, batch: 8/10 time:  0.0003 ( 0.0025) loss:  0.5471 ( 0.5586) acc:  0.75 ( 0.73)
epoch: 6, batch: 9/10 time:  0.0003 ( 0.0027) loss:  0.6237 ( 0.5658) acc:  0.62 ( 0.72)
epoch: 6, batch: 10/10 time:  0.0003 ( 0.0030) loss:  0.5710 ( 0.5662) acc:  0.75 ( 0.72)
test epoch 6 test loss:  0.5595 test acc:  0.73
epoch: 7, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.4970 ( 0.4970) acc:  0.78 ( 0.78)
epoch: 7, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.5936 ( 0.5453) acc:  0.62 ( 0.70)
epoch: 7, batch: 3/10 time:  0.0003 ( 0.0008) loss:  0.6611 ( 0.5839) acc:  0.59 ( 0.67)
epoch: 7, batch: 4/10 time:  0.0003 ( 0.0011) loss:  0.5075 ( 0.5648) acc:  0.81 ( 0.70)
epoch: 7, batch: 5/10 time:  0.0003 ( 0.0014) loss:  0.6279 ( 0.5774) acc:  0.62 ( 0.69)
epoch: 7, batch: 6/10 time:  0.0003 ( 0.0017) loss:  0.5523 ( 0.5732) acc:  0.75 ( 0.70)
epoch: 7, batch: 7/10 time:  0.0003 ( 0.0019) loss:  0.5473 ( 0.5695) acc:  0.75 ( 0.71)
epoch: 7, batch: 8/10 time:  0.0003 ( 0.0022) loss:  0.5202 ( 0.5633) acc:  0.75 ( 0.71)
epoch: 7, batch: 9/10 time:  0.0003 ( 0.0025) loss:  0.5188 ( 0.5584) acc:  0.81 ( 0.72)
epoch: 7, batch: 10/10 time:  0.0003 ( 0.0028) loss:  0.5376 ( 0.5568) acc:  0.75 ( 0.72)
test epoch 7 test loss:  0.5524 test acc:  0.73
epoch: 8, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5120 ( 0.5120) acc:  0.72 ( 0.72)
epoch: 8, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.6222 ( 0.5671) acc:  0.66 ( 0.69)
epoch: 8, batch: 3/10 time:  0.0003 ( 0.0009) loss:  0.5453 ( 0.5599) acc:  0.75 ( 0.71)
epoch: 8, batch: 4/10 time:  0.0005 ( 0.0014) loss:  0.5277 ( 0.5518) acc:  0.72 ( 0.71)
epoch: 8, batch: 5/10 time:  0.0035 ( 0.0049) loss:  0.5859 ( 0.5586) acc:  0.72 ( 0.71)
epoch: 8, batch: 6/10 time:  0.0007 ( 0.0056) loss:  0.5251 ( 0.5530) acc:  0.72 ( 0.71)
epoch: 8, batch: 7/10 time:  0.0004 ( 0.0060) loss:  0.5165 ( 0.5478) acc:  0.69 ( 0.71)
epoch: 8, batch: 8/10 time:  0.0004 ( 0.0064) loss:  0.4864 ( 0.5402) acc:  0.88 ( 0.73)
epoch: 8, batch: 9/10 time:  0.0004 ( 0.0069) loss:  0.6029 ( 0.5471) acc:  0.69 ( 0.73)
epoch: 8, batch: 10/10 time:  0.0004 ( 0.0072) loss:  0.5846 ( 0.5500) acc:  0.75 ( 0.73)
test epoch 8 test loss:  0.5491 test acc:  0.71
epoch: 9, batch: 1/10 time:  0.0004 ( 0.0004) loss:  0.4657 ( 0.4657) acc:  0.75 ( 0.75)
epoch: 9, batch: 2/10 time:  0.0003 ( 0.0007) loss:  0.5291 ( 0.4974) acc:  0.75 ( 0.75)
epoch: 9, batch: 3/10 time:  0.0003 ( 0.0011) loss:  0.5921 ( 0.5290) acc:  0.66 ( 0.72)
epoch: 9, batch: 4/10 time:  0.0003 ( 0.0014) loss:  0.5263 ( 0.5283) acc:  0.72 ( 0.72)
epoch: 9, batch: 5/10 time:  0.0003 ( 0.0017) loss:  0.5122 ( 0.5251) acc:  0.81 ( 0.74)
epoch: 9, batch: 6/10 time:  0.0003 ( 0.0021) loss:  0.6247 ( 0.5417) acc:  0.66 ( 0.72)
epoch: 9, batch: 7/10 time:  0.0003 ( 0.0024) loss:  0.5446 ( 0.5421) acc:  0.66 ( 0.71)
epoch: 9, batch: 8/10 time:  0.0007 ( 0.0031) loss:  0.5521 ( 0.5433) acc:  0.75 ( 0.72)
epoch: 9, batch: 9/10 time:  0.0005 ( 0.0036) loss:  0.5627 ( 0.5455) acc:  0.72 ( 0.72)
epoch: 9, batch: 10/10 time:  0.0004 ( 0.0040) loss:  0.5223 ( 0.5437) acc:  0.79 ( 0.72)
test epoch 9 test loss:  0.5460 test acc:  0.75
epoch: 10, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5409 ( 0.5409) acc:  0.75 ( 0.75)
epoch: 10, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.4766 ( 0.5088) acc:  0.81 ( 0.78)
epoch: 10, batch: 3/10 time:  0.0003 ( 0.0008) loss:  0.6287 ( 0.5487) acc:  0.66 ( 0.74)
epoch: 10, batch: 4/10 time:  0.0003 ( 0.0011) loss:  0.4914 ( 0.5344) acc:  0.78 ( 0.75)
epoch: 10, batch: 5/10 time:  0.0003 ( 0.0014) loss:  0.5745 ( 0.5424) acc:  0.66 ( 0.73)
epoch: 10, batch: 6/10 time:  0.0003 ( 0.0017) loss:  0.5640 ( 0.5460) acc:  0.69 ( 0.72)
epoch: 10, batch: 7/10 time:  0.0003 ( 0.0019) loss:  0.4918 ( 0.5383) acc:  0.78 ( 0.73)
epoch: 10, batch: 8/10 time:  0.0003 ( 0.0022) loss:  0.5732 ( 0.5426) acc:  0.66 ( 0.72)
epoch: 10, batch: 9/10 time:  0.0003 ( 0.0025) loss:  0.5759 ( 0.5463) acc:  0.66 ( 0.72)
epoch: 10, batch: 10/10 time:  0.0003 ( 0.0028) loss:  0.4954 ( 0.5424) acc:  0.83 ( 0.72)
test epoch 10 test loss:  0.5418 test acc:  0.75
epoch: 11, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.4829 ( 0.4829) acc:  0.81 ( 0.81)
epoch: 11, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.4749 ( 0.4789) acc:  0.75 ( 0.78)
epoch: 11, batch: 3/10 time:  0.0003 ( 0.0009) loss:  0.5430 ( 0.5003) acc:  0.72 ( 0.76)
epoch: 11, batch: 4/10 time:  0.0003 ( 0.0012) loss:  0.4967 ( 0.4994) acc:  0.81 ( 0.77)
epoch: 11, batch: 5/10 time:  0.0003 ( 0.0015) loss:  0.5847 ( 0.5164) acc:  0.69 ( 0.76)
epoch: 11, batch: 6/10 time:  0.0003 ( 0.0018) loss:  0.5853 ( 0.5279) acc:  0.66 ( 0.74)
epoch: 11, batch: 7/10 time:  0.0003 ( 0.0020) loss:  0.5038 ( 0.5245) acc:  0.75 ( 0.74)
epoch: 11, batch: 8/10 time:  0.0005 ( 0.0025) loss:  0.5343 ( 0.5257) acc:  0.69 ( 0.73)
epoch: 11, batch: 9/10 time:  0.0005 ( 0.0030) loss:  0.5502 ( 0.5284) acc:  0.75 ( 0.74)
epoch: 11, batch: 10/10 time:  0.0005 ( 0.0035) loss:  0.6476 ( 0.5376) acc:  0.58 ( 0.72)
test epoch 11 test loss:  0.5391 test acc:  0.75
epoch: 12, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5289 ( 0.5289) acc:  0.78 ( 0.78)
epoch: 12, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.5411 ( 0.5350) acc:  0.66 ( 0.72)
epoch: 12, batch: 3/10 time:  0.0003 ( 0.0009) loss:  0.6046 ( 0.5582) acc:  0.66 ( 0.70)
epoch: 12, batch: 4/10 time:  0.0003 ( 0.0012) loss:  0.6275 ( 0.5755) acc:  0.69 ( 0.70)
epoch: 12, batch: 5/10 time:  0.0003 ( 0.0015) loss:  0.4627 ( 0.5530) acc:  0.84 ( 0.72)
epoch: 12, batch: 6/10 time:  0.0003 ( 0.0017) loss:  0.5156 ( 0.5467) acc:  0.72 ( 0.72)
epoch: 12, batch: 7/10 time:  0.0003 ( 0.0020) loss:  0.4435 ( 0.5320) acc:  0.88 ( 0.75)
epoch: 12, batch: 8/10 time:  0.0003 ( 0.0023) loss:  0.4894 ( 0.5267) acc:  0.75 ( 0.75)
epoch: 12, batch: 9/10 time:  0.0003 ( 0.0026) loss:  0.5453 ( 0.5287) acc:  0.72 ( 0.74)
epoch: 12, batch: 10/10 time:  0.0003 ( 0.0029) loss:  0.6099 ( 0.5350) acc:  0.58 ( 0.73)
test epoch 12 test loss:  0.5360 test acc:  0.75
epoch: 13, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.4960 ( 0.4960) acc:  0.78 ( 0.78)
epoch: 13, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.6075 ( 0.5517) acc:  0.66 ( 0.72)
epoch: 13, batch: 3/10 time:  0.0003 ( 0.0009) loss:  0.4341 ( 0.5125) acc:  0.81 ( 0.75)
epoch: 13, batch: 4/10 time:  0.0003 ( 0.0012) loss:  0.5259 ( 0.5159) acc:  0.72 ( 0.74)
epoch: 13, batch: 5/10 time:  0.0003 ( 0.0015) loss:  0.4593 ( 0.5045) acc:  0.84 ( 0.76)
epoch: 13, batch: 6/10 time:  0.0003 ( 0.0018) loss:  0.6933 ( 0.5360) acc:  0.56 ( 0.73)
epoch: 13, batch: 7/10 time:  0.0004 ( 0.0022) loss:  0.5897 ( 0.5437) acc:  0.69 ( 0.72)
epoch: 13, batch: 8/10 time:  0.0004 ( 0.0027) loss:  0.4361 ( 0.5302) acc:  0.84 ( 0.74)
epoch: 13, batch: 9/10 time:  0.0004 ( 0.0031) loss:  0.5077 ( 0.5277) acc:  0.75 ( 0.74)
epoch: 13, batch: 10/10 time:  0.0004 ( 0.0035) loss:  0.5831 ( 0.5320) acc:  0.62 ( 0.73)
test epoch 13 test loss:  0.5346 test acc:  0.75
epoch: 14, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.4782 ( 0.4782) acc:  0.78 ( 0.78)
epoch: 14, batch: 2/10 time:  0.0003 ( 0.0007) loss:  0.6254 ( 0.5518) acc:  0.66 ( 0.72)
epoch: 14, batch: 3/10 time:  0.0003 ( 0.0010) loss:  0.4978 ( 0.5338) acc:  0.69 ( 0.71)
epoch: 14, batch: 4/10 time:  0.0003 ( 0.0014) loss:  0.4828 ( 0.5211) acc:  0.75 ( 0.72)
epoch: 14, batch: 5/10 time:  0.0003 ( 0.0017) loss:  0.4660 ( 0.5100) acc:  0.75 ( 0.72)
epoch: 14, batch: 6/10 time:  0.0003 ( 0.0020) loss:  0.5557 ( 0.5177) acc:  0.66 ( 0.71)
epoch: 14, batch: 7/10 time:  0.0003 ( 0.0024) loss:  0.6824 ( 0.5412) acc:  0.69 ( 0.71)
epoch: 14, batch: 8/10 time:  0.0003 ( 0.0027) loss:  0.5535 ( 0.5427) acc:  0.75 ( 0.71)
epoch: 14, batch: 9/10 time:  0.0004 ( 0.0031) loss:  0.4105 ( 0.5280) acc:  0.84 ( 0.73)
epoch: 14, batch: 10/10 time:  0.0004 ( 0.0034) loss:  0.5604 ( 0.5305) acc:  0.71 ( 0.73)
test epoch 14 test loss:  0.5326 test acc:  0.75
epoch: 15, batch: 1/10 time:  0.0004 ( 0.0004) loss:  0.5007 ( 0.5007) acc:  0.66 ( 0.66)
epoch: 15, batch: 2/10 time:  0.0004 ( 0.0007) loss:  0.5091 ( 0.5049) acc:  0.75 ( 0.70)
epoch: 15, batch: 3/10 time:  0.0004 ( 0.0011) loss:  0.5234 ( 0.5110) acc:  0.78 ( 0.73)
epoch: 15, batch: 4/10 time:  0.0004 ( 0.0015) loss:  0.4904 ( 0.5059) acc:  0.72 ( 0.73)
epoch: 15, batch: 5/10 time:  0.0004 ( 0.0019) loss:  0.5878 ( 0.5223) acc:  0.69 ( 0.72)
epoch: 15, batch: 6/10 time:  0.0004 ( 0.0023) loss:  0.5294 ( 0.5235) acc:  0.72 ( 0.72)
epoch: 15, batch: 7/10 time:  0.0003 ( 0.0026) loss:  0.6132 ( 0.5363) acc:  0.66 ( 0.71)
epoch: 15, batch: 8/10 time:  0.0003 ( 0.0030) loss:  0.5018 ( 0.5320) acc:  0.75 ( 0.71)
epoch: 15, batch: 9/10 time:  0.0003 ( 0.0033) loss:  0.5385 ( 0.5327) acc:  0.78 ( 0.72)
epoch: 15, batch: 10/10 time:  0.0003 ( 0.0036) loss:  0.4491 ( 0.5263) acc:  0.79 ( 0.73)
test epoch 15 test loss:  0.5314 test acc:  0.75
epoch: 16, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.4985 ( 0.4985) acc:  0.75 ( 0.75)
epoch: 16, batch: 2/10 time:  0.0003 ( 0.0007) loss:  0.5024 ( 0.5005) acc:  0.75 ( 0.75)
epoch: 16, batch: 3/10 time:  0.0003 ( 0.0010) loss:  0.4824 ( 0.4944) acc:  0.75 ( 0.75)
epoch: 16, batch: 4/10 time:  0.0003 ( 0.0014) loss:  0.4030 ( 0.4716) acc:  0.88 ( 0.78)
epoch: 16, batch: 5/10 time:  0.0003 ( 0.0017) loss:  0.6148 ( 0.5002) acc:  0.59 ( 0.74)
epoch: 16, batch: 6/10 time:  0.0003 ( 0.0021) loss:  0.4196 ( 0.4868) acc:  0.84 ( 0.76)
epoch: 16, batch: 7/10 time:  0.0004 ( 0.0024) loss:  0.4747 ( 0.4851) acc:  0.78 ( 0.76)
epoch: 16, batch: 8/10 time:  0.0004 ( 0.0028) loss:  0.6089 ( 0.5005) acc:  0.59 ( 0.74)
epoch: 16, batch: 9/10 time:  0.0004 ( 0.0031) loss:  0.8235 ( 0.5364) acc:  0.44 ( 0.71)
epoch: 16, batch: 10/10 time:  0.0004 ( 0.0035) loss:  0.4114 ( 0.5268) acc:  0.88 ( 0.72)
test epoch 16 test loss:  0.5304 test acc:  0.73
epoch: 17, batch: 1/10 time:  0.0004 ( 0.0004) loss:  0.4973 ( 0.4973) acc:  0.75 ( 0.75)
epoch: 17, batch: 2/10 time:  0.0004 ( 0.0008) loss:  0.5441 ( 0.5207) acc:  0.72 ( 0.73)
epoch: 17, batch: 3/10 time:  0.0004 ( 0.0012) loss:  0.4486 ( 0.4967) acc:  0.88 ( 0.78)
epoch: 17, batch: 4/10 time:  0.0003 ( 0.0016) loss:  0.4460 ( 0.4840) acc:  0.84 ( 0.80)
epoch: 17, batch: 5/10 time:  0.0003 ( 0.0019) loss:  0.4837 ( 0.4839) acc:  0.75 ( 0.79)
epoch: 17, batch: 6/10 time:  0.0003 ( 0.0022) loss:  0.5008 ( 0.4867) acc:  0.72 ( 0.78)
epoch: 17, batch: 7/10 time:  0.0003 ( 0.0026) loss:  0.5384 ( 0.4941) acc:  0.75 ( 0.77)
epoch: 17, batch: 8/10 time:  0.0003 ( 0.0029) loss:  0.6589 ( 0.5147) acc:  0.66 ( 0.76)
epoch: 17, batch: 9/10 time:  0.0003 ( 0.0032) loss:  0.5555 ( 0.5192) acc:  0.69 ( 0.75)
epoch: 17, batch: 10/10 time:  0.0003 ( 0.0036) loss:  0.5750 ( 0.5235) acc:  0.62 ( 0.74)
test epoch 17 test loss:  0.5287 test acc:  0.75
epoch: 18, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5089 ( 0.5089) acc:  0.69 ( 0.69)
epoch: 18, batch: 2/10 time:  0.0003 ( 0.0007) loss:  0.5187 ( 0.5138) acc:  0.72 ( 0.70)
epoch: 18, batch: 3/10 time:  0.0004 ( 0.0010) loss:  0.4896 ( 0.5057) acc:  0.72 ( 0.71)
epoch: 18, batch: 4/10 time:  0.0004 ( 0.0014) loss:  0.5235 ( 0.5102) acc:  0.78 ( 0.73)
epoch: 18, batch: 5/10 time:  0.0004 ( 0.0018) loss:  0.4827 ( 0.5047) acc:  0.75 ( 0.73)
epoch: 18, batch: 6/10 time:  0.0004 ( 0.0021) loss:  0.5682 ( 0.5153) acc:  0.72 ( 0.73)
epoch: 18, batch: 7/10 time:  0.0003 ( 0.0025) loss:  0.5497 ( 0.5202) acc:  0.78 ( 0.74)
epoch: 18, batch: 8/10 time:  0.0004 ( 0.0028) loss:  0.5962 ( 0.5297) acc:  0.66 ( 0.73)
epoch: 18, batch: 9/10 time:  0.0004 ( 0.0033) loss:  0.5121 ( 0.5277) acc:  0.75 ( 0.73)
epoch: 18, batch: 10/10 time:  0.0003 ( 0.0036) loss:  0.4598 ( 0.5225) acc:  0.79 ( 0.73)
test epoch 18 test loss:  0.5275 test acc:  0.75
epoch: 19, batch: 1/10 time:  0.0006 ( 0.0006) loss:  0.6061 ( 0.6061) acc:  0.72 ( 0.72)
epoch: 19, batch: 2/10 time:  0.0003 ( 0.0009) loss:  0.4510 ( 0.5285) acc:  0.78 ( 0.75)
epoch: 19, batch: 3/10 time:  0.0003 ( 0.0012) loss:  0.5193 ( 0.5255) acc:  0.72 ( 0.74)
epoch: 19, batch: 4/10 time:  0.0003 ( 0.0016) loss:  0.4791 ( 0.5139) acc:  0.78 ( 0.75)
epoch: 19, batch: 5/10 time:  0.0003 ( 0.0019) loss:  0.4856 ( 0.5082) acc:  0.78 ( 0.76)
epoch: 19, batch: 6/10 time:  0.0003 ( 0.0022) loss:  0.6296 ( 0.5284) acc:  0.66 ( 0.74)
epoch: 19, batch: 7/10 time:  0.0003 ( 0.0026) loss:  0.4837 ( 0.5221) acc:  0.75 ( 0.74)
epoch: 19, batch: 8/10 time:  0.0003 ( 0.0029) loss:  0.5216 ( 0.5220) acc:  0.75 ( 0.74)
epoch: 19, batch: 9/10 time:  0.0003 ( 0.0032) loss:  0.6232 ( 0.5332) acc:  0.69 ( 0.74)
epoch: 19, batch: 10/10 time:  0.0003 ( 0.0036) loss:  0.4093 ( 0.5237) acc:  0.83 ( 0.74)
test epoch 19 test loss:  0.5287 test acc:  0.75
epoch: 20, batch: 1/10 time:  0.0004 ( 0.0004) loss:  0.6301 ( 0.6301) acc:  0.69 ( 0.69)
epoch: 20, batch: 2/10 time:  0.0004 ( 0.0007) loss:  0.4792 ( 0.5547) acc:  0.75 ( 0.72)
epoch: 20, batch: 3/10 time:  0.0004 ( 0.0011) loss:  0.5465 ( 0.5520) acc:  0.59 ( 0.68)
epoch: 20, batch: 4/10 time:  0.0004 ( 0.0014) loss:  0.5095 ( 0.5414) acc:  0.69 ( 0.68)
epoch: 20, batch: 5/10 time:  0.0004 ( 0.0018) loss:  0.4274 ( 0.5186) acc:  0.81 ( 0.71)
epoch: 20, batch: 6/10 time:  0.0004 ( 0.0022) loss:  0.4474 ( 0.5067) acc:  0.81 ( 0.72)
epoch: 20, batch: 7/10 time:  0.0005 ( 0.0026) loss:  0.5708 ( 0.5158) acc:  0.62 ( 0.71)
epoch: 20, batch: 8/10 time:  0.0004 ( 0.0030) loss:  0.5116 ( 0.5153) acc:  0.78 ( 0.72)
epoch: 20, batch: 9/10 time:  0.0003 ( 0.0033) loss:  0.5277 ( 0.5167) acc:  0.75 ( 0.72)
epoch: 20, batch: 10/10 time:  0.0004 ( 0.0037) loss:  0.5326 ( 0.5179) acc:  0.83 ( 0.73)
test epoch 20 test loss:  0.5275 test acc:  0.75
epoch: 21, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.4839 ( 0.4839) acc:  0.78 ( 0.78)
epoch: 21, batch: 2/10 time:  0.0003 ( 0.0007) loss:  0.4671 ( 0.4755) acc:  0.75 ( 0.77)
epoch: 21, batch: 3/10 time:  0.0003 ( 0.0010) loss:  0.4157 ( 0.4556) acc:  0.81 ( 0.78)
epoch: 21, batch: 4/10 time:  0.0003 ( 0.0013) loss:  0.4828 ( 0.4624) acc:  0.78 ( 0.78)
epoch: 21, batch: 5/10 time:  0.0003 ( 0.0017) loss:  0.5148 ( 0.4729) acc:  0.69 ( 0.76)
epoch: 21, batch: 6/10 time:  0.0003 ( 0.0020) loss:  0.5609 ( 0.4875) acc:  0.69 ( 0.75)
epoch: 21, batch: 7/10 time:  0.0003 ( 0.0023) loss:  0.5005 ( 0.4894) acc:  0.75 ( 0.75)
epoch: 21, batch: 8/10 time:  0.0003 ( 0.0027) loss:  0.5247 ( 0.4938) acc:  0.72 ( 0.75)
epoch: 21, batch: 9/10 time:  0.0004 ( 0.0030) loss:  0.6727 ( 0.5137) acc:  0.62 ( 0.73)
epoch: 21, batch: 10/10 time:  0.0004 ( 0.0034) loss:  0.5559 ( 0.5169) acc:  0.79 ( 0.74)
test epoch 21 test loss:  0.5274 test acc:  0.75
epoch: 22, batch: 1/10 time:  0.0004 ( 0.0004) loss:  0.5326 ( 0.5326) acc:  0.75 ( 0.75)
epoch: 22, batch: 2/10 time:  0.0004 ( 0.0007) loss:  0.4463 ( 0.4894) acc:  0.81 ( 0.78)
epoch: 22, batch: 3/10 time:  0.0003 ( 0.0011) loss:  0.5852 ( 0.5214) acc:  0.72 ( 0.76)
epoch: 22, batch: 4/10 time:  0.0004 ( 0.0014) loss:  0.5186 ( 0.5207) acc:  0.75 ( 0.76)
epoch: 22, batch: 5/10 time:  0.0006 ( 0.0020) loss:  0.5171 ( 0.5200) acc:  0.69 ( 0.74)
epoch: 22, batch: 6/10 time:  0.0004 ( 0.0024) loss:  0.4219 ( 0.5036) acc:  0.78 ( 0.75)
epoch: 22, batch: 7/10 time:  0.0004 ( 0.0028) loss:  0.5268 ( 0.5069) acc:  0.69 ( 0.74)
epoch: 22, batch: 8/10 time:  0.0003 ( 0.0031) loss:  0.5319 ( 0.5100) acc:  0.72 ( 0.74)
epoch: 22, batch: 9/10 time:  0.0003 ( 0.0035) loss:  0.5419 ( 0.5136) acc:  0.72 ( 0.74)
epoch: 22, batch: 10/10 time:  0.0003 ( 0.0038) loss:  0.5356 ( 0.5153) acc:  0.75 ( 0.74)
test epoch 22 test loss:  0.5271 test acc:  0.75
epoch: 23, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.4861 ( 0.4861) acc:  0.78 ( 0.78)
epoch: 23, batch: 2/10 time:  0.0003 ( 0.0007) loss:  0.4909 ( 0.4885) acc:  0.75 ( 0.77)
epoch: 23, batch: 3/10 time:  0.0003 ( 0.0010) loss:  0.5574 ( 0.5115) acc:  0.69 ( 0.74)
epoch: 23, batch: 4/10 time:  0.0003 ( 0.0014) loss:  0.4353 ( 0.4924) acc:  0.78 ( 0.75)
epoch: 23, batch: 5/10 time:  0.0003 ( 0.0017) loss:  0.6815 ( 0.5302) acc:  0.59 ( 0.72)
epoch: 23, batch: 6/10 time:  0.0003 ( 0.0020) loss:  0.5399 ( 0.5318) acc:  0.72 ( 0.72)
epoch: 23, batch: 7/10 time:  0.0004 ( 0.0024) loss:  0.4882 ( 0.5256) acc:  0.81 ( 0.73)
epoch: 23, batch: 8/10 time:  0.0004 ( 0.0028) loss:  0.4920 ( 0.5214) acc:  0.72 ( 0.73)
epoch: 23, batch: 9/10 time:  0.0004 ( 0.0031) loss:  0.4930 ( 0.5183) acc:  0.75 ( 0.73)
epoch: 23, batch: 10/10 time:  0.0004 ( 0.0035) loss:  0.4646 ( 0.5141) acc:  0.79 ( 0.74)
test epoch 23 test loss:  0.5267 test acc:  0.75
epoch: 24, batch: 1/10 time:  0.0008 ( 0.0008) loss:  0.6150 ( 0.6150) acc:  0.66 ( 0.66)
epoch: 24, batch: 2/10 time:  0.0005 ( 0.0013) loss:  0.5557 ( 0.5853) acc:  0.72 ( 0.69)
epoch: 24, batch: 3/10 time:  0.0003 ( 0.0016) loss:  0.4440 ( 0.5382) acc:  0.81 ( 0.73)
epoch: 24, batch: 4/10 time:  0.0003 ( 0.0019) loss:  0.4159 ( 0.5076) acc:  0.84 ( 0.76)
epoch: 24, batch: 5/10 time:  0.0003 ( 0.0022) loss:  0.5336 ( 0.5128) acc:  0.75 ( 0.76)
epoch: 24, batch: 6/10 time:  0.0003 ( 0.0025) loss:  0.6162 ( 0.5301) acc:  0.62 ( 0.73)
epoch: 24, batch: 7/10 time:  0.0003 ( 0.0027) loss:  0.5342 ( 0.5306) acc:  0.72 ( 0.73)
epoch: 24, batch: 8/10 time:  0.0003 ( 0.0030) loss:  0.4242 ( 0.5173) acc:  0.81 ( 0.74)
epoch: 24, batch: 9/10 time:  0.0003 ( 0.0033) loss:  0.4412 ( 0.5089) acc:  0.78 ( 0.75)
epoch: 24, batch: 10/10 time:  0.0003 ( 0.0036) loss:  0.5701 ( 0.5136) acc:  0.71 ( 0.74)
test epoch 24 test loss:  0.5239 test acc:  0.75
epoch: 25, batch: 1/10 time:  0.0003 ( 0.0003) loss:  0.5050 ( 0.5050) acc:  0.75 ( 0.75)
epoch: 25, batch: 2/10 time:  0.0003 ( 0.0006) loss:  0.6445 ( 0.5747) acc:  0.69 ( 0.72)
epoch: 25, batch: 3/10 time:  0.0003 ( 0.0009) loss:  0.6067 ( 0.5854) acc:  0.69 ( 0.71)
epoch: 25, batch: 4/10 time:  0.0003 ( 0.0012) loss:  0.3852 ( 0.5354) acc:  0.81 ( 0.73)
epoch: 25, batch: 5/10 time:  0.0003 ( 0.0015) loss:  0.5781 ( 0.5439) acc:  0.66 ( 0.72)
epoch: 25, batch: 6/10 time:  0.0003 ( 0.0018) loss:  0.4992 ( 0.5365) acc:  0.69 ( 0.71)
epoch: 25, batch: 7/10 time:  0.0003 ( 0.0021) loss:  0.5388 ( 0.5368) acc:  0.69 ( 0.71)
epoch: 25, batch: 8/10 time:  0.0003 ( 0.0024) loss:  0.5031 ( 0.5326) acc:  0.78 ( 0.72)
epoch: 25, batch: 9/10 time:  0.0003 ( 0.0027) loss:  0.3940 ( 0.5172) acc:  0.84 ( 0.73)
epoch: 25, batch: 10/10 time:  0.0003 ( 0.0030) loss:  0.4799 ( 0.5143) acc:  0.75 ( 0.73)
test epoch 25 test loss:  0.5234 test acc:  0.75
epoch: 26, batch: 1/10 time:  0.0004 ( 0.0004) loss:  0.4324 ( 0.4324) acc:  0.75 ( 0.75)
epoch: 26, batch: 2/10 time:  0.0004 ( 0.0007) loss:  0.5020 ( 0.4672) acc:  0.75 ( 0.75)
epoch: 26, batch: 3/10 time:  0.0004 ( 0.0011) loss:  0.4964 ( 0.4769) acc:  0.75 ( 0.75)
epoch: 26, batch: 4/10 time:  0.0004 ( 0.0015) loss:  0.4611 ( 0.4730) acc:  0.81 ( 0.77)
epoch: 26, batch: 5/10 time:  0.0003 ( 0.0018) loss:  0.4507 ( 0.4685) acc:  0.81 ( 0.78)
epoch: 26, batch: 6/10 time:  0.0003 ( 0.0022) loss:  0.6761 ( 0.5031) acc:  0.69 ( 0.76)
epoch: 26, batch: 7/10 time:  0.0003 ( 0.0025) loss:  0.5804 ( 0.5142) acc:  0.75 ( 0.76)
epoch: 26, batch: 8/10 time:  0.0003 ( 0.0028) loss:  0.5594 ( 0.5198) acc:  0.62 ( 0.74)
epoch: 26, batch: 9/10 time:  0.0003 ( 0.0032) loss:  0.5024 ( 0.5179) acc:  0.72 ( 0.74)
epoch: 26, batch: 10/10 time:  0.0003 ( 0.0035) loss:  0.4642 ( 0.5138) acc:  0.75 ( 0.74)
test epoch 26 test loss:  0.5230 test acc:  0.75
epoch: 27, batch: 1/10 time:  0.0004 ( 0.0004) loss:  0.3609 ( 0.3609) acc:  0.84 ( 0.84)
epoch: 27, batch: 2/10 time:  0.0003 ( 0.0007) loss:  0.5746 ( 0.4678) acc:  0.66 ( 0.75)
epoch: 27, batch: 3/10 time:  0.0004 ( 0.0011) loss:  0.5112 ( 0.4822) acc:  0.75 ( 0.75)
epoch: 27, batch: 4/10 time:  0.0004 ( 0.0014) loss:  0.6242 ( 0.5177) acc:  0.66 ( 0.73)
epoch: 27, batch: 5/10 time:  0.0004 ( 0.0018) loss:  0.4532 ( 0.5048) acc:  0.75 ( 0.73)
epoch: 27, batch: 6/10 time:  0.0004 ( 0.0022) loss:  0.5424 ( 0.5111) acc:  0.78 ( 0.74)
epoch: 27, batch: 7/10 time:  0.0004 ( 0.0025) loss:  0.5498 ( 0.5166) acc:  0.66 ( 0.73)
epoch: 27, batch: 8/10 time:  0.0004 ( 0.0029) loss:  0.4259 ( 0.5053) acc:  0.84 ( 0.74)
epoch: 27, batch: 9/10 time:  0.0004 ( 0.0033) loss:  0.4648 ( 0.5008) acc:  0.78 ( 0.75)
epoch: 27, batch: 10/10 time:  0.0006 ( 0.0039) loss:  0.6269 ( 0.5105) acc:  0.62 ( 0.74)
test epoch 27 test loss:  0.5243 test acc:  0.75
epoch: 28, batch: 1/10 time:  0.0006 ( 0.0006) loss:  0.4385 ( 0.4385) acc:  0.75 ( 0.75)
epoch: 28, batch: 2/10 time:  0.0006 ( 0.0013) loss:  0.5868 ( 0.5126) acc:  0.66 ( 0.70)
epoch: 28, batch: 3/10 time:  0.0006 ( 0.0019) loss:  0.4265 ( 0.4839) acc:  0.81 ( 0.74)
epoch: 28, batch: 4/10 time:  0.0005 ( 0.0024) loss:  0.4471 ( 0.4747) acc:  0.81 ( 0.76)
epoch: 28, batch: 5/10 time:  0.0006 ( 0.0030) loss:  0.4524 ( 0.4703) acc:  0.75 ( 0.76)
epoch: 28, batch: 6/10 time:  0.0005 ( 0.0035) loss:  0.5146 ( 0.4776) acc:  0.72 ( 0.75)
epoch: 28, batch: 7/10 time:  0.0005 ( 0.0040) loss:  0.6602 ( 0.5037) acc:  0.66 ( 0.74)
epoch: 28, batch: 8/10 time:  0.0005 ( 0.0046) loss:  0.5435 ( 0.5087) acc:  0.69 ( 0.73)
epoch: 28, batch: 9/10 time:  0.0004 ( 0.0050) loss:  0.5016 ( 0.5079) acc:  0.81 ( 0.74)
epoch: 28, batch: 10/10 time:  0.0005 ( 0.0055) loss:  0.5288 ( 0.5095) acc:  0.71 ( 0.74)
test epoch 28 test loss:  0.5265 test acc:  0.77
epoch: 29, batch: 1/10 time:  0.0008 ( 0.0008) loss:  0.3970 ( 0.3970) acc:  0.84 ( 0.84)
epoch: 29, batch: 2/10 time:  0.0012 ( 0.0020) loss:  0.5111 ( 0.4541) acc:  0.78 ( 0.81)
epoch: 29, batch: 3/10 time:  0.0005 ( 0.0025) loss:  0.4017 ( 0.4366) acc:  0.88 ( 0.83)
epoch: 29, batch: 4/10 time:  0.0004 ( 0.0029) loss:  0.5524 ( 0.4656) acc:  0.72 ( 0.80)
epoch: 29, batch: 5/10 time:  0.0004 ( 0.0033) loss:  0.6338 ( 0.4992) acc:  0.59 ( 0.76)
epoch: 29, batch: 6/10 time:  0.0004 ( 0.0037) loss:  0.5002 ( 0.4994) acc:  0.75 ( 0.76)
epoch: 29, batch: 7/10 time:  0.0004 ( 0.0042) loss:  0.4379 ( 0.4906) acc:  0.81 ( 0.77)
epoch: 29, batch: 8/10 time:  0.0005 ( 0.0047) loss:  0.5772 ( 0.5014) acc:  0.69 ( 0.76)
epoch: 29, batch: 9/10 time:  0.0005 ( 0.0052) loss:  0.5797 ( 0.5101) acc:  0.66 ( 0.75)
epoch: 29, batch: 10/10 time:  0.0004 ( 0.0056) loss:  0.5136 ( 0.5104) acc:  0.79 ( 0.75)
test epoch 29 test loss:  0.5270 test acc:  0.79</code></pre>
</div>
</details>
<details><summary>Click to view results</summary>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="intro_files/figure-html/cell-11-output-2.png" width="814" height="283" class="figure-img"></p>
</figure>
</div>
</div>
</details>
</div>
<!-- {{< include pytorch.qmd >}} -->
<!-- {{< include multi.qmd >}} -->
<!-- {{< include sklearn.qmd >}} -->
</section>
</section>
<section id="exercises-and-projects" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="exercises-and-projects"><span class="header-section-number">6.4</span> Exercises and Projects</h2>
<div id="exr-" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.1</strong></span> Please hand write a report about the details of the math formulas for Logistic regression.</p>
</div>
<!-- 

::: {#exr-}
CHOOSE ONE: Please use `sklearn` to apply the LogisticRegression to one of the following datasets. You may either use `LogisticRegression` or `SGDClassifier`.

- the `iris` dataset.
- the dating dataset.
- the `titanic` dataset.

Please in addition answer the following questions.

1. What is your accuracy score?
2. How many epochs do you use?
3. Plot the learning curve (accuracy vs training sizes).
::: -->
<div id="exr-" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.2</strong></span> CHOOSE ONE: Please use <code>PyTorch</code> to apply the LogisticRegression to one of the following datasets.</p>
<ul>
<li>the <code>iris</code> dataset.</li>
<li>the dating dataset.</li>
<li>the <code>titanic</code> dataset.</li>
</ul>
<p>Please in addition answer the following questions.</p>
<ol type="1">
<li>What is your accuracy score?</li>
<li>How many epochs do you use?</li>
<li>What is the batch size do you use?</li>
<li>Plot the learning curve (loss vs epochs, accuracy vs epochs).</li>
<li>Analyze the bias / variance status.</li>
</ol>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/5/intro.html" class="pagination-link" aria-label="Intro to Pytorch">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Intro to Pytorch</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/7/intro.html" class="pagination-link" aria-label="Netural networks">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Netural networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>