
## Neural network implement of Logistic regression
In the previous sections, we use gradient descent to run the Logistic regression model. We mentioned some important concepts, like epochs, mini-batch, etc.. We are going to use `PyTorch` to implement it. We will reuse many codes we wrote in the previous chapter.



### Example

We still use the horse colic dataset as an example. We first prepare the dataset.


```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

filepath = "assests/datasets/horse_colic_clean.csv"
df = pd.read_csv(filepath)
X = df.iloc[:, :22].to_numpy().astype(float)
y = (df.iloc[:, 22]<2).to_numpy().astype(int)

SEED = 42
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=SEED)

```


We need to perform normalization before throwing the data into the model. Here we use the `MinMaxScaler()` from `sklearn` package. 


```{python}
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_train = mms.fit_transform(X_train, y_train)
X_test = mms.transform(X_test)
```

Then we write a `Dataset` class to build the dataset and create the dataloaders. Since the set is already split, we don't need to `random_split` here.
```{python}
import torch
from torch.utils.data import Dataset, DataLoader

class MyData(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=float)
        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)

    def __getitem__(self, index):
        return (self.X[index], self.y[index])

    def __len__(self):
        return len(self.y)


train_set = MyData(X_train, y_train)
val_set = MyData(X_test, y_test)

train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
val_loader = DataLoader(val_set, batch_size=32)
```

In the following code, we first set up the original model.

```{python}
import torch.nn as nn
from torch.nn.modules import Linear

class LoR(nn.Module):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.linear = Linear(in_features=22, out_features=1, dtype=float)
        self.activation = nn.Sigmoid()

    def forward(self, X):
        # pred = self.activation(self.linear(X))
        pred = self.linear(X)
        # return (pred >= 0).float()
        return pred

```


Then we run our regular training loop.


```{python}
import time
import matplotlib.pyplot as plt
from torch.optim import SGD
from torch.nn import BCEWithLogitsLoss

model = LoR()
optim = SGD(model.parameters(), lr=0.2)
loss_fn = BCEWithLogitsLoss()
n_epochs = 30

class Meter:
    def __init__(self, total=0.0, count=0, value=0.0):
        self.total = total
        self.count = count
        self.value = value
        self.avg = self.total / self.count if self.count > 0 else 0.0

    def update(self, value, n=1):
        self.value = value
        self.total += value * n
        self.count += n
        self.avg = self.total / self.count if self.count > 0 else 0.0

history = {'loss': [], 'acc': [], 'loss_test': [], 'acc_test': []}

for epoch in range(n_epochs):
    monitor_loss = Meter()
    monitor_loss_test = Meter()
    monitor_acc = Meter()
    monitor_acc_test = Meter()
    monitor_time = Meter()

    for i, (X_batch, y_batch) in enumerate(train_loader):
        model.train()
        t0 = time.perf_counter()
        optim.zero_grad()
        p = model(X_batch)
        loss = loss_fn(p, y_batch)
        loss.backward()
        optim.step()
        t1 = time.perf_counter()

        with torch.no_grad():
            pred = (p>0.5).to(torch.long)
            acc = (pred == y_batch).to(torch.float).mean().item()
            monitor_acc.update(acc, n=X_batch.shape[0])
            monitor_loss.update(loss.item(), n=X_batch.shape[0])
            monitor_time.update(t1-t0, n=1)

        print(
            f'epoch: {epoch}, batch: {i+1}/{len(train_loader)} '
            f'time: {monitor_time.value: .4f} ({monitor_time.total: .4f}) '
            f'loss: {monitor_loss.value: .4f} ({monitor_loss.avg: .4f}) '
            f'acc: {monitor_acc.value: .2f} ({monitor_acc.avg: .2f})'
        )

    history['loss'].append(monitor_loss.avg)
    history['acc'].append(monitor_acc.avg)

    with torch.no_grad():
        model.eval()
        for X_batch_test, y_batch_test in val_loader:
            p = model(X_batch_test)
            loss_test = loss_fn(p, y_batch_test)
            monitor_loss_test.update(loss_test.item(), n=X_batch_test.shape[0])
            pred_test = (p>0.5).to(torch.int)
            acc_test = ( pred_test == y_batch_test).to(torch.float).mean().item()
            monitor_acc_test.update(acc_test, n=X_batch_test.shape[0])

        print(
            f'test epoch {epoch} '
            f'test loss: {monitor_loss_test.avg: .4f} '
            f'test acc: {monitor_acc_test.avg: .2f}'
        )
        history['loss_test'].append(monitor_loss_test.avg)
        history['acc_test'].append(monitor_acc_test.avg)

fig, axs = plt.subplots(1, 2)
fig.set_size_inches((10,3))
axs[0].plot(history['loss'], label='training_loss')
axs[0].plot(history['loss_test'], label='testing_loss')
axs[0].legend()
axs[1].plot(history['acc'], label='training_acc')
axs[1].plot(history['acc_test'], label='testing_acc')
axs[1].legend()
axs[0].set_title('Loss')
axs[1].set_title('Accuracy')
```






