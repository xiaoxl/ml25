## Example

Let us take some of our old dataset as an example. This is an continuation of the horse colic dataset from Logistic regression. Note that most of the codes are directly taken from logistic regression section, since MLP is just a generalization of logistic regression.



```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

filepath = "assests/datasets/horse_colic_clean.csv"
df = pd.read_csv(filepath)
X = df.iloc[:, :22].to_numpy().astype(float)
y = (df.iloc[:, 22]<2).to_numpy().astype(int)

SEED = 42
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=SEED)

from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
mms.fit(X_train)
X_train = mms.transform(X_train)
X_test = mms.transform(X_test)
```

The data is feed into the dataloader. Note that we change the batch size of the test dataloader to be the whole set, since I don't want to do batch evaluation. This can be modified accordingly.

```{python}
import torch
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        return (self.X[idx], self.y[idx])

train_loader = DataLoader(MyDataset(X_train, y_train), batch_size =32)
val_loader = DataLoader(MyDataset(X_test, y_test), batch_size=X_test.shape[0])
```


Now we build a neural network. This is a 2-layer model, with 1 hidden layer with 10 nodes. Since we are going to use BCEWithLogitsLoss, we don't add the final activation function here in the model, but leave it to the loss function.


```{python}
import torch.nn as nn
class MyModel(nn.Module):
    def __init__(self, num_inputs):
        super().__init__()
        self.linear1 = nn.Linear(num_inputs, 20)
        self.act1 = nn.ReLU()
        self.linear2 = nn.Linear(20, 1)
        # self.act2 = nn.Sigmoid()

    def forward(self, x):
        x = self.linear1(x)
        x = self.act1(x)
        x = self.linear2(x)
        # x = self.act2(x)
        return x

model = MyModel(22)
```

We could use the following code to look at the structure of the model.

```{python}
total = 0
for n, p in model.named_parameters():
    print(n, p.shape, p.numel())
    total += p.numel()
print("total params:", total)
```



Now we start to train the model and evaluate. Note that the majority part of the code is about evaluating the result. Since we are doing binary classification, our result can be computed by checking whether our model output (before the final sigmoid function) is positive or negative. This is where `(p>0)` comes from.


```{python}
#| output-fold: true
#| output-summary: "Click to view results"
import time
import matplotlib.pyplot as plt
from torch.optim import SGD
from torch.nn import BCEWithLogitsLoss

model = MyModel(22)
optim = SGD(model.parameters(), lr=0.1)
loss_fn = BCEWithLogitsLoss()
n_epochs = 30

class Meter:
    def __init__(self, total=0.0, count=0, value=0.0):
        self.total = total
        self.count = count
        self.value = value
        self.avg = self.total / self.count if self.count > 0 else 0.0

    def update(self, value, n=1):
        self.value = value
        self.total += value * n
        self.count += n
        self.avg = self.total / self.count if self.count > 0 else 0.0

history = {'loss': [], 'acc': [], 'loss_test': [], 'acc_test': []}

for epoch in range(n_epochs):
    monitor_loss = Meter()
    monitor_loss_test = Meter()
    monitor_acc = Meter()
    monitor_acc_test = Meter()
    monitor_time = Meter()

    for i, (X_batch, y_batch) in enumerate(train_loader):
        model.train()
        t0 = time.perf_counter()
        optim.zero_grad()
        p = model(X_batch)
        loss = loss_fn(p, y_batch)
        loss.backward()
        optim.step()
        t1 = time.perf_counter()

        with torch.no_grad():
            pred = (p>0).to(torch.long)
            acc = (pred == y_batch).to(torch.float).mean().item()
            monitor_acc.update(acc, n=X_batch.shape[0])
            monitor_loss.update(loss.item(), n=X_batch.shape[0])
            monitor_time.update(t1-t0, n=1)

        print(
            f'epoch: {epoch}, batch: {i+1}/{len(train_loader)} '
            f'time: {monitor_time.value: .4f} ({monitor_time.total: .4f}) '
            f'loss: {monitor_loss.value: .4f} ({monitor_loss.avg: .4f}) '
            f'acc: {monitor_acc.value: .2f} ({monitor_acc.avg: .2f})'
        )

    history['loss'].append(monitor_loss.avg)
    history['acc'].append(monitor_acc.avg)

    with torch.no_grad():
        model.eval()
        for X_batch_test, y_batch_test in val_loader:
            p = model(X_batch_test)
            loss_test = loss_fn(p, y_batch_test)
            monitor_loss_test.update(loss_test.item(), n=X_batch_test.shape[0])
            pred_test = (p>0).to(torch.int)
            acc_test = ( pred_test == y_batch_test).to(torch.float).mean().item()
            monitor_acc_test.update(acc_test, n=X_batch_test.shape[0])

        print(
            f'test epoch {epoch} '
            f'test loss: {monitor_loss_test.avg: .4f} '
            f'test acc: {monitor_acc_test.avg: .2f}'
        )
        history['loss_test'].append(monitor_loss_test.avg)
        history['acc_test'].append(monitor_acc_test.avg)

fig, axs = plt.subplots(1, 2)
fig.set_size_inches((10,3))
axs[0].plot(history['loss'], label='training_loss')
axs[0].plot(history['loss_test'], label='testing_loss')
axs[0].legend()
axs[1].plot(history['acc'], label='training_acc')
axs[1].plot(history['acc_test'], label='testing_acc')
axs[1].legend()
axs[0].set_title('Loss');
axs[1].set_title('Accuracy');
```


As you may see, to build a netural network model it requires many testing. There are many established models. When you build your own architecture, you may start from there and modify it to fit your data.