## Linear regression (`numpy`)


{{< include ../math.qmd >}}


We will translate everything from the previous sections into codes.

### Prepare the dataset
We first randomly generate a dataset `(X, y)` for the linear regression problem. 

```{python}
import numpy as np

np.random.seed(42)
X = np.random.rand(100)
y = 2.3 + 1.2 * X + np.random.randn(100) * 0.1
```
We set the seed to be 42 for reproducing the results. We will also split the dataset into training and test sets.


```{python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,
                                                    random_state=42)
```
We will only focus only on the training set in this Chapter. 


### Compute gradient
Recall @eq-cost_lr and @eq-gd_updating

$$
\begin{aligned}
J(b,w)&=\frac1N\sum_{i=1}^N(y_i-b-wx_i)^2,\\
\pdv{J}{b}&=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-1),\\
\pdv{J}{w}&=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-x_i).
\end{aligned}
$$


```{python}
def J(parameters, X, y):    
    b = parameters[0]
    w = parameters[1]
    return ((y-b-w*X)**2).mean().item()

def dJ(parameters, X, y):
    b = parameters[0]
    w = parameters[1]
    db = (2*(y-b-w*X)*(-1)).mean()
    dw = (2*(y-b-w*X)*(-X)).mean()
    return np.array([db, dw])
```

### Gradient descent {#sec-gradeientdescent_numpy_example}

In general we need to random select a starting point. Here for the purpose of comparing to what we get from previous section, we will use a manual selected starting point $(1, 1.5)$. We then follow the path and move for a few steps. Here we will use $\eta=0.2$ as the learning rate. 
```{python}
p = np.array([1.0, 1.5])
lr = 0.2

plist = []
for _ in range(10):
    J_i = J(p, X_train, y_train)
    dJ_i = dJ(p, X_train, y_train)
    p = p - lr * dJ_i
    plist.append([p[0], p[1]])

plist
```


You may compare the answer with the `PyTorch` implementation in @sec-gradientdescent_pytorch_example.

### Mini-batch and optimizers {#sec-minibatch_numpy}
Review the gradient formula @eq-gd_updating, the gradient is computed by looking at each given data point and putting the results together. Therefore it is possible to get the partial information of the gradient by just looking at part of the data. In other words, the updating process can be modify in the following way: divide the original dataset into several groups, run through each group to compute the gradient with the data in only one group and then update the parameters. In general there are three types:

- There is only 1 group: we update the parameters only once when we finish looking at all data points. This is the way we mentioned previously. It is called **batch gradient descent**. 
- Every single point forms a group: we update the parameters eachtime we look at one data point. This method is called **stocastic gradient descent** (SGD). Since we compute the gradient with only one data point, it is expected that the direction is far from perfect, and the descent process is expected to be more "random". 
- Multiple groups of the same size are formed, with a reasonable group size and group number. This is called **mini-batch gradient descent**. It is the middle point between the above two methods. The **batch size**, which is the size of each group, is a very important hyperparameter for trainning.


::: {.callout-note}
# Epochs
One **epoch** is the process that you see each data point exactly once, no matter what the batch size is. 
:::

Usually batch gradient descent is expected to have a more smooth trajection but move slowly, while SGD is expected to move faster to the minimal point but may never really get to it since the trajection is too jumpy. Mini-batch is meant to strike a balanced point by finding a good batch size. In the example below, we show the mini-batch gradient descent in the first 10 epochs.

```{python}
p = np.array([1.0, 1.5])
lr = 0.2
batchsize = 32
RANDOMSEED = 42

N = X_train.shape[0]
indx = np.arange(N)

np.random.seed(RANDOMSEED)
np.random.shuffle(indx)
batches = []

batch_num = int(np.ceil(N / batchsize))
for i in range(batch_num):
    last = np.minimum((i+1)*batchsize, N)
    batches.append(indx[i*batchsize: last])

plist = []
for epoch in range(10):
    for i in range(batch_num):
        dJ_i = dJ(p, X_train[batches[i]], y_train[batches[i]])
        p = p - lr * dJ_i
    plist.append([p[0], p[1]])
plist
```


::: {.callout-note collapse="true"}
# Non-shuffle version
Here is the result for the non-shuffle version. You could compare the results with what we do later.

```{python}
p = np.array([1.0, 1.5])
lr = 0.2
batchsize = 32

N = X_train.shape[0]
indx = np.arange(N)

batches = []
batch_num = int(np.ceil(N / batchsize))
for i in range(batch_num):
    last = np.minimum((i+1)*batchsize, N)
    batches.append(indx[i*batchsize: last])

plist = []
for epoch in range(10):
    for i in range(batch_num):
        dJ_i = dJ(p, X_train[batches[i]], y_train[batches[i]])
        p = p - lr * dJ_i
    plist.append([p[0], p[1]])
plist
```

:::