## Linear regression (math) {#sec-linearregression_math}


{{< include ../math.qmd >}}


We only consider the simplest case: simple linear regression (SLR). The idea is very simple. The dataset contains two variables (the independent variable $x$ and the response variable $y$.) The goal is to find the relation between $x$ and $y$ with the given dataset. We assume their relation is $y=b+wx$. How do we find $b$ and $w$?

Let us first see an example. We would like to find the red line (which is the best fitted curve) shown below.

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

sns.set_theme()
np.random.seed(42)
X = np.random.rand(100)
y = 2.3 + 1.2 * X + np.random.randn(100) * 0.1
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15,
                                                    random_state=42)

plt.scatter(X_train, y_train)
plt.plot(np.array([0,1]), np.array([2.3, 3.5]), 'r-')
plt.gca().set_ylim(2.2, 3.6)
```

### Parameter space

The key here is to understand the idea of "parameter space". Since we already know that the function we are looking for has a formula $y=b+wx$, we could use the pair $(b, w)$ to denote different candidates of our answer. For example, the following plot show some possibilities in green dashed lines, while each possiblity is denoted by $(b, w)$. Then the problem is reworded as to find the best pair $(b, w)$.

```{python}
#| echo: false

plt.scatter(X_train, y_train)
b = 2
plt.plot(np.array([0,1]), np.array([2.3, 2.3+b]), 'g--')
_ = plt.annotate(f'(2.3, {b})', [0.6, 3.4], color='green')

b = 3
plt.plot(np.array([0,1]), np.array([2.3, 2.3+b]), 'g--')
_ = plt.annotate(f'(2.3, {b})', [0.2, 3.5], color='green')

b = 1
plt.plot(np.array([0,1]), np.array([2.3, 2.3+b]), 'g--')
_ = plt.annotate(f'(2.3, {b})', [0.8, 3.0], color='green')

b = 0.5
plt.plot(np.array([0,1]), np.array([2.3, 2.3+b]), 'g--')
_ = plt.annotate(f'(2.3, {b})', [0.8, 2.6], color='green')

b = 1.2
plt.plot(np.array([0,1]), np.array([2.3, 2.3+b]), 'r-')
plt.gca().set_ylim(2.2, 3.6)
_ = plt.annotate(f'(2.3, {b})', [0.75, 3.4], color='red')
```


### Loss function
The "best" is defined in the following way. The dataset is given $\{(x_i, y_i)\}$. If we choose a pair of parameters $(b,w)$, we will have an estimated regression line, as well as a set of estimated $\hat{y_i}$. The idea is to let the difference between $y_i$ and $\hat{y_i}$ is as small as possible. In other words, a **loss function** $J$ is defined as follows:

$$
J_{\{(x_i,y_i)\}}(b,w)=\frac1N\sum_{i=1}^N(y_i-\hat{y_i})^2=\frac1N\sum_{i=1}^N(y_i-b-wx_i)^2
$$ {#eq-cost_lr}
and we are expected to find the $(b,w)$ such that the loss function is minimized. The contour map of $J$ is shown below. 


```{python}
#| echo: false

def f(b, w, x, y):
    m, n = b.shape
    res = np.zeros((m, n))

    for i in range(m):
        for j in range(n):
            res[i, j]= ((y-(b[i,j]+w[i,j]*x))**2).mean()

    return res


b = np.linspace(0, 3, 301)
w = np.linspace(0, 3, 301)
bg, wg = np.meshgrid(b, w)
z = f(bg, wg, X_train, y_train)

cs = plt.contour(bg, wg, z,
                 levels=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5]
                )
plt.colorbar()
ax = plt.gca()
_ = ax.set_xlabel('b')
_ = ax.set_ylabel('w')
```


### Gradient Descent

We use a technique called "gradient descent" to find the global minimal of $J$. We start from a random point. For example $(1.0, 1.5)$. Then we find a direction where the cost $J$ reduces the most, and move in that direction. This direction is computed by the gradient of the cost $J$, and this is the reason why the algorithm is called "gradient descent". After we get to a new point, we evaluate the new gradient and move in the new direction. The process is repeated and we are expected to get close to the minimal point after several iterations. Just like shown in the following plot.

```{python}
#| echo: false

def df(p, x, y):
    b, w = p
    resb = ((y-b-w*x)*(-1)).mean()
    resw = ((y-b-w*x)*(-x)).mean()
    return np.array([resb, resw])

cs = plt.contour(bg, wg, z,
                 levels=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5]
                )
plt.colorbar()
ax = plt.gca()
ax.set_xlabel('b')
ax.set_ylabel('w')

lr = 0.2
p = np.array([1, 1.5])

costJ = [f(np.array([[p[0]]]), np.array([[p[1]]]), X_train, y_train)[0,0]]

for _ in range(200):
    d = -df(p, X_train, y_train)*lr
    ax.plot(p[0], p[1], 'bo')
    ax.arrow(p[0], p[1], d[0]*.5, d[1]*.5,
            head_width=0.05, head_length=0.05, fc='k', ec='k')
    p = p + d
    costJ.append(f(np.array([[p[0]]]), np.array([[p[1]]]), X_train, y_train)[0,0])

bestp = p
```


The parameter updating rule is shown below. The $\eta$ is called the **learning rate**. It is a hyperparameter that is used to control the learning process. 

$$
\begin{aligned}
&\pdv{J}{b}=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-1),\quad &b_{new} = b_{old}-\eta*\pdv{J}{b},\\
&\pdv{J}{w}=\frac1N\sum_{i=1}^N2(y_i-b-wx_i)(-x_i),\quad &w_{new} = w_{old}-\eta*\pdv{J}{w},
\end{aligned}
$$ {#eq-gd_updating}

::: {.callout-note collapse="true"}
# Learning rate $\eta$
Generally speaking, larger $\eta$ will move faster to the global minimal, but might be jumpy which cause it harder to converge. On the other side, smaller $\eta$ moves in a more stable fashion, but may take a long time to converge. See the following examples.


```{python}
#| echo: false

fig, axs = plt.subplots(1, 2)
axs[0].contour(bg, wg, z, levels=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5])
axs[1].contour(bg, wg, z, levels=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5])

lr = 0.01
p = np.array([1, 1.5])

for _ in range(200):
    d = -df(p, X_train, y_train)*lr
    axs[0].plot(p[0], p[1], 'bo')
    axs[0].arrow(p[0], p[1], d[0]*.5, d[1]*.5,
            head_width=0.05, head_length=0.05, fc='k', ec='k')
    p = p + d

axs[0].set_title('learning rate $\\eta$ = 0.01')

lr = 1.6
p = np.array([1, 1.5])

for _ in range(200):
    d = -df(p, X_train, y_train)*lr
    axs[1].plot(p[0], p[1], 'bo')
    axs[1].arrow(p[0], p[1], d[0]*.9, d[1]*.9,
            head_width=0.05, head_length=0.05, fc='k', ec='k')
    p = p + d

_ = axs[1].set_title('learning rate $\\eta$ = 1.6')
```
In the first example, $\eta$ is too small, that after 200 iterations it is not very close to the minimal. In the second example, $\eta$ becomes large. Although it gets to somewhere near the minimal, the path is very jumpy. It is able to converge only because the problem is indeed an easy one.
:::

We may record the curve of the cost function.

```{python}
#| echo: false

costJ = np.array(costJ)
plt.plot(costJ)
_ = plt.gca().set_xlabel("iteration")
_ = plt.gca().set_ylabel("cost")

print(f"After 200 iterations, the parameters are ({bestp[0]}, {bestp[1]}).")
```
The cost is close to $0$ after 200 iterations and seems to be convergent. Therefore we believe that we are close to the minimal point. The point we get is `{python} (bestp[0], bestp[1])`.





### Summary

Let us summarize the example above and generalize it to the general case. 

1. Let $\{(X_i, y_i)\}$ be a given dataset. Assume that $y=f_{\Theta}(X)$ where $\Theta$ is the set of all parameters. 
2. The cost function $J_{\Theta, \{(X_i, y_i)\}}$ is defined. 
3. To find the minimal point of the cost function, the gradient descent is applied: 
   - Start from a random initial point $\theta_0$.
   - Compute the gradient $\nabla J$ and update $\theta_i=\theta_{i-1}- \eta \nabla J$ and repeat the process multiple times.
   - Draw the learning curve and determine when to stop. Then we get the estimated best parameters $\hat{\Theta}$.
4. Our model under this setting is sovled. We then turn to evaluation phase.


::: {.callout-note}
The above process can be further developped. We will discuss many of them in later sections.

1. The cost function is related to each concerte problem.
2. To compute the gradient of the cost function, chain rule is usually used. In the setting of MLP which we will discuss later, the gradient computations with chain rule are summarized as the so-called **Back propagation**. 
3. We go through the data points to compute the graident. How many points do we use? What is the frenqucy to update the gradient? This belongs to the topic of **mini-batch**.
4. Even when we know that the graident gives the best direction, sometimes we don't really want to go in that direction, but make some modifications for some reason. To modify the direction, as well as choosing the learning rate $\eta$, is the subject of **optimizers**.
:::