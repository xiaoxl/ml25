## Dataloader {#sec-dataloader}

### Convert the previous dataset using DataLoader
Usually we use a class to provide data. The class is based on `Dataset` class, and need to implement the constructor, `__getitem__` method and `__len__` method. Here is an example. 


::: {.callout-caution}
Note that we directly change `X` and `y` to be 2D tensors when we create the dataset.
:::


```{python}
from torch.utils.data import Dataset

class MyData(Dataset):
    def __init__(self, x, y):
        self.x = torch.tensor(x, dtype=float).reshape(-1, 1)
        self.y = torch.tensor(y, dtype=float).reshape(-1, 1)

    def __getitem__(self, index):
        return (self.x[index], self.y[index])

    def __len__(self):
        return len(self.y)

train_data = MyData(X_train, y_train)
train_data[1]
```


Then we use `Dataloader` to feed the data into our model.

```{python}
from torch.utils.data import DataLoader 
train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)
```

It is used in the following way.
```{python}
lr = 0.2
epoch_num = 10

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = BetterLR().to(device)
optimizer = SGD(model.parameters(), lr=lr)

plist = []
for epoch in range(epoch_num):
    for X_batch, y_batch in train_loader:
        yhat = model(X_batch)
        loss = MSELoss(reduction='mean')(yhat, y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    p = model.state_dict()
    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])
plist

```

When applying mini-batch, usually we will shuffle the dataset. If we disable the shuffle here as well as the shuffle in `numpy` case, you will see that we get exactly the same answer.


::: {.callout-note collapse="true"}
# Non-shuffle version
Here is the result for non-shuffle version.

```{python}
from torch.utils.data import DataLoader 

lr = 0.2
epoch_num = 10

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = BetterLR().to(device)
optimizer = SGD(model.parameters(), lr=lr)

train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=False)

plist = []
for epoch in range(epoch_num):
    for X_batch, y_batch in train_loader:
        yhat = model(X_batch)
        loss = MSELoss(reduction='mean')(yhat, y_batch)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    p = model.state_dict()
    plist.append([p['linear.bias'].item(), p['linear.weight'].item()])
plist

```
:::


You may notice that we use some not-very-elegent way to display the result. Don't worry about it. We will work on a better solution in the next section.

### Rewrite using `random_split`
It is possible to purely use `PyTorch` instead of going through `sklearn`. After we get the Dataset, we could use `random_split` to create training set and testing set. 


```{python}
from torch.utils.data import random_split
import numpy as np

dataset = MyData(X, y)
train_data, val_data = random_split(dataset, [.85, .15], generator=torch.Generator().manual_seed(42))

train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = DataLoader(val_data, batch_size=32)
```
