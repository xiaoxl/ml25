{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic setting for Machine learning problems\n",
        "\n",
        "\n",
        "::: {.callout-note}\n",
        "We by default assume that we are dealing with a **Supervised** **Classification** problem.\n",
        ":::\n",
        "\n",
        "\n",
        "### Input and output data structure\n",
        "Since we are dealing with Supervised Classification problems, the desired solutions are given. These desired solutions in Classification problems are also called *labels*. The properties that the data are used to describe are called *features*. Both features and labels are usually organized as row vectors. \n",
        "\n",
        "\n",
        ":::{#exm-} \n",
        "The example is extracted from @Har2012. There are some sample data shown in the following table. We would like to use these information to classify bird species.\n"
      ],
      "id": "cae2a563"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: tbl-table\n",
        "#| tbl-cap: Bird species classification based on four features\n",
        "#| echo: false\n",
        "\n",
        "import pandas as pd\n",
        "table = [[1000.1, 125.0, 'No', 'Brown', 'Buteo jamaicensis'],\n",
        "         [3000.7, 200.0, 'No', 'Gray', 'Sagittarius serpentarius'],\n",
        "         [3300.0, 220.3, 'No', 'Gray', 'Sagittarius serpentarius'],\n",
        "         [4100.0, 136.0, 'Yes', 'Black', 'Gavia immer'],\n",
        "         [3.0, 11.0, 'No', 'Green', 'Calothorax lucifer'],\n",
        "         [570.0, 75.0, 'No', 'Black', 'Campephilus principalis']]\n",
        "\n",
        "headers=['Weight (g)', 'Wingspan (cm)', 'Webbed feet?', 'Back color', 'Species']\n",
        "df = pd.DataFrame(table, columns=headers, index=None)\n",
        "df.style.set_properties(**{'text-align': 'right'}).set_table_styles([ dict(selector='th', props=[('text-align', 'right')] ) ]).hide(axis=\"index\")"
      ],
      "id": "tbl-table",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first four columns are features, and the last column is the label. The first two features are numeric and can take on decimal values. The third feature is binary that can only be $1$ (Yes) or $0$ (No). The fourth feature is an enumeration over the color palette. You may either treat it as categorical data or numeric data, depending on how you want to build the model and what you want to get out of the data. In this example we will use it as categorical data that we only choose it from a list of colors ($1$ --- Brown, $2$ --- Gray, $3$ --- Black, $4$ --- Green). \n",
        "\n",
        "Then we are able to transform the above data into the following form:\n",
        "\n",
        "| Features | Labels | \n",
        "|-----|-----|\n",
        "| $\\begin{bmatrix}1001.1 & 125.0 & 0 & 1 \\end{bmatrix}$   | $1$    |\n",
        "| $\\begin{bmatrix}3000.7 & 200.0 & 0 & 2 \\end{bmatrix}$    | $2$    |\n",
        "| $\\begin{bmatrix}3300.0 & 220.3 & 0 & 2 \\end{bmatrix}$    | $2$  |\n",
        "| $\\begin{bmatrix}4100.0 & 136.0 & 1 & 3 \\end{bmatrix}$    | $3$    |\n",
        "| $\\begin{bmatrix}3.0 & 11.0 & 0 & 4 \\end{bmatrix}$    | $4$    |\n",
        "| $\\begin{bmatrix}570.0 & 75.0 & 0 & 3 \\end{bmatrix}$    | $5$    |\n",
        "\n",
        ": Vectorized Bird species data {#tbl-vectorized}\n",
        "\n",
        "\n",
        "Then the Supervised Learning problem is stated as follows: Given the features and the labels, we would like to find a model that can classify future data.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "### Parameters and hyperparameters\n",
        "A model parameter is internal to the model and its value is learned from the data. \n",
        "\n",
        "A model hyperparameter is external to the model and its value is set by people.\n",
        "\n",
        "For example, assume that we would like to use Logistic regression to fit the data. We set the learning rate is `0.1` and the maximal iteration is `100`. After the computations are done, we get a the model \n",
        "\n",
        "$$\n",
        "y = \\sigma(0.8+0.7x).\n",
        "$$\n",
        "The two cofficients $0.8$ and $0.7$ are the parameters of the model. The model `Logistic regression`, the learning rate `0.1` and the maximal iteration `100` are all hyperparametrs. If we change to a different set of hyperparameters, we may get a different model, with a different set of parameters.\n",
        "\n",
        "The details of Logistic regression will be discussed later.\n",
        "\n",
        "### Evaluate a Machine Learning model\n",
        "Once the model is built, how do we know that it is good or not? The naive idea is to test the model on some brand new data and check whether it is able to get the desired results. The usual way to achieve it is to split the input dataset into three pieces: *training set*, *validation set* and *test set*.\n",
        "\n",
        "The model is initially fit on the training set, with some arbitrary selections of hyperparameters. Then hyperparameters will be changed, and new model is fitted over the training set. Which set of hyperparameters is better? We then test their performance over the validation set. We could run through a lot of different combinations of hyperparameters, and find the best performance over the validation set. After we get the best hyperparameters, the model is selcted, and we fit it over the training set to get our model to use.\n",
        "\n",
        "To compare our model with our models, either our own model using other algorithms, or models built by others, we need some new data. We can no longer use the training set and the validation set since all data in them are used, either for training or for hyperparameters tuning. We need to use the test set to evaluate the \"real performance\" of our data.\n",
        "\n",
        "To summarize: \n",
        "\n",
        "- Training set: used to fit the model;\n",
        "- Validation set: used to tune the hyperparameters;\n",
        "- Test set: used to check the overall performance of the model.\n",
        "\n",
        "The validation set is not always required. If we use cross-validation technique for hyperparameters tuning, like `sklearn.model_selection.GridSearchCV()`, we don't need a separated validation set. In this case, we will only need the training set and the test set, and run `GridSearchCV` over the training set. The cross-validation will be discussed in {numref}`Section %s<section-cross-validation>`. \n",
        "\n",
        "The sizes and strategies for dataset division depends on the problem and data available. It is often recommanded that more training data should be used. The typical distribution of training, validation and test is $(6:3:1)$, $(7:2:1)$ or $(8:1:1)$. Sometimes validation set is discarded and only training set and test set are used. In this case the distribution of training and test set is usually $(7:3)$, $(8:2)$ or $(9:1)$.\n",
        "\n",
        "\n",
        "### Workflow in developing a machine learning application\n",
        "\n",
        "The workflow described below is from @Har2012.\n",
        "\n",
        "1. Collect data.\n",
        "2. Prepare the input data.\n",
        "3. Analyze the input data.\n",
        "4. Train the algorithm.\n",
        "5. Test the algorithm.\n",
        "6. Use it.\n",
        "\n",
        "In this course, we will mainly focus on Step 4 as well Step 5. These two steps are where the \"core\" algorithms lie, depending on the algorithm. We will start from the next Chapter to talk about various Machine Learning algorithms and examples.\n",
        "\n",
        "\n",
        "<!-- \n",
        "## Output data structure\n",
        "\n",
        "### Binary Classification Problem\n",
        "When there are only one class, and all we care about is whether a data point belongs to this class or not, we call this type of problem **binary classification** problem. \n",
        "\n",
        "In this case, the desired output for each data point is either $1$ or $0$, where $1$ means \"belonging to this class\" and $0$ means \"not belonging to this class\".\n",
        "\n",
        "If there are two classes, we can still use the idea of binary classification to study the problem. We choose one class as our focus. When the data point belongs to the other class, we can simply say it does belong to the class we choose.\n",
        "\n",
        "### $0$ and $1$\n",
        "In many cases the desired output is either $0$ or $1$, while the output of the model is a real number between $0$ and $1$. In this case, the output of the model is interpreted as the probability for the data to be in the specific class. When we use this model, we simply choose the class that has the highest probability and claim that the data is belonging to this class. \n",
        "\n",
        "In the binary case, the above method can be stated in another way. We choose a threshold, and treat those whose probability are above the threshold to be in the class, and others not. The default value for the threshold is $0.5$, and in this case the method is just a special case for the previous method. \n",
        "\n",
        "### One-hot coding -->"
      ],
      "id": "caf6966f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Xinli\\miniforge3\\envs\\ds25\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}