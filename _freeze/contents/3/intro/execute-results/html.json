{
  "hash": "fd23c18c1df9e01d30e7fbd80fc326de",
  "result": {
    "engine": "jupyter",
    "markdown": "# Decision Trees\n\n\n<!-- ## Naive ideas -->\n\nGiven a dataset with labels, the decision tree algorithm firstly trys to split the whole dataset into two different groups, based on some speicific features. Choose which feature to use and set the threshold for the split are done.\n\n\n\n\n## Gini impurity\n\nTo split a dataset, we need a metric to tell whether the split is good or not. The two most popular metrics that are used are Gini impurity and Entropy. The two metrics don't have essential differences, that the results obtained by applying each metric are very similar to each other. Therefore we will only focus on Gini impurity since it is slightly easier to compute and slightly easier to explain.\n\n### Motivation and Definition\nAssume that we have a dataset of totally $n$ objects, and these objects are divided into $k$ classes. The $i$-th class has $n_i$ objects. Then if we randomly pick an object, the probability to get an object belonging to the $i$-th class is\n\n$$\np_i=\\frac{n_i}{n}\n$$\n\nIf we then guess the class of the object purely based on the distribution of each class, the probability that our guess is incorrect is \n\n$$\n1-p_i = 1-\\frac{n_i}{n}.\n$$\n\nTherefore, if we randomly pick an object that belongs to the $i$-th class and randomly guess its class purely based on the distribution but our guess is wrong, the probability that such a thing happens is \n\n$$\np_i(1-p_i).\n$$\n\nConsider all classes. The probability at which any object of the dataset will be mislabelled when it is randomly labeled is the sum of the probability described above:\n\n$$\n\\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2.\n$$\n\nThis is the definition formula for the *Gini impurity*. \n\n\n\n::: {#def-gini}\nThe **Gini impurity** is calculated using the following formula\n\n$$\nGini = \\sum_{i=1}^kp_i(1-p_i)=\\sum_{i=1}^kp_i-\\sum_{i=1}^kp_i^2=1-\\sum_{i=1}^kp_i^2,\n$$\nwhere $p_i$ is the probability of class $i$.\n:::\n\nThe way to understand Gini impurity is to consider some extreme examples. \n\n\n::: {#exm-}\n\nAssume that we only have one class. Therefore $k=1$, and $p_1=1$. Then the Gini impurity is\n\n$$\nGini = 1-1^2=0.\n$$\nThis is the minimum possible Gini impurity. It means that the dataset is **pure**: all the objects contained are of one unique class. In this case, we won't make any mistakes if we randomly guess the label.\n\n:::\n\n\n\n\n::: {#exm-}\nAssume that we have two classes. Therefore $k=2$. Consider the distribution $p_1$ and $p_2$. We know that $p_1+p_2=1$. Therefore $p_2=1-p_1$. Then the Gini impurity is\n\n$$\nGini(p_1) = 1-p_1^2-p_2^2=1-p_1^2-(1-p_1)^2=2p_1-2p_1^2.\n$$\nWhen $0\\leq p_1\\leq 1$, this function $Gini(p_1)$ is between $0$ and $0.5$. \n- It gets $0$ when $p_1=0$ or $1$. In these two cases, the dataset is still a one-class set since the size of one class is $0$. \n- It gets $0.5$ when $p_1=0.5$. This means that the Gini impurity is maximized when the size of different classes are balanced.\n:::\n\n### Algorithm\n\n\n::: {.callout-note}\n# Algorithm: Gini impurity\n\n**Inputs** A dataset $S=\\{data=[features, label]\\}$ with labels. \n\n**Outputs** The Gini impurity of the dataset.\n\n1. Get the size $n$ of the dataset.\n2. Go through the label list, and find all unique labels: $uniqueLabelList$.\n3. Go through each label $l$ in $uniqueLabelList$ and count how many elements belonging to the label, and record them as $n_l$.\n4. Use the formula to compute the Gini impurity:\n\n   $$\n    Gini = 1-\\sum_{l\\in uniqueLabelList}\\left(\\frac{n_l}{n}\\right)^2.\n   $$\n:::\n\n\n\n\n\n<details>\n<summary>The sample manual codes are optional. </summary>\n\n::: {#affcf929 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\ndef gini(S):\n    N = len(S)\n    y = S[:, -1].reshape(N)\n    gini = 1 - ((pd.Series(y).value_counts()/N)**2).sum()\n    return gini\n```\n:::\n\n\n</details>\n\n\n\n\n## CART Algorithms \n\n### Ideas\nConsider a labeled dataset $S$ with totally $m$ elements. We use a feature $k$ and a threshold $t_k$ to split it into two subsets: $S_l$ with $m_l$ elements and $S_r$ with $m_r$ elements. Then the cost function of this split is\n\n$$\nJ(k, t_k)=\\frac{m_l}mGini(S_l)+\\frac{m_r}{m}Gini(S_r).\n$$\nIt is not hard to see that the more pure the two subsets are the lower the cost function is. Therefore our goal is find a split that can minimize the cost function.\n\n:::{.callout-note}\n\n# Algorithm: Split the Dataset\n\n**Inputs** Given a labeled dataset $S=\\{[features, label]\\}$.\n\n**Outputs** A best split $(k, t_k)$.\n\n1. For each feature $k$:\n    1. For each value $t$ of the feature:\n        1. Split the dataset $S$ into two subsets, one with $k\\leq t$ and one with $k>t$.\n        2. Compute the cost function $J(k,t)$. \n        3. Compare it with the current smallest cost. If this split has smaller cost, replace the current smallest cost and pair with $(k, t)$.\n2. Return the pair $(k,t_k)$ that has the smallest cost function.\n:::\n\n\nWe then use this split algorithm recursively to get the decision tree.\n\n:::{.callout-note}\n# Classification and Regression Tree, CART\n\n**Inputs** Given a labeled dataset $S=\\{[features, label]\\}$ and a maximal depth `max_depth`.\n\n**Outputs** A decision tree.\n\n1. Starting from the original dataset $S$. Set the working dataset $G=S$.\n2. Consider a dataset $G$. If $Gini(G)\\neq0$, split $G$ into $G_l$ and $G_r$ to minimize the cost function. Record the split pair $(k, t_k)$.\n3. Now set the working dataset $G=G_l$ and $G=G_r$ respectively, and apply the above two steps to each of them.\n4. Repeat the above steps, until `max_depth` is reached.\n:::\n\n<details>\n<summary>The manual sample code is optional.</summary>\n\n::: {#fcf11b6c .cell execution_count=2}\n``` {.python .cell-code}\ndef split(G):\n    m = G.shape[0]\n    gmini = gini(G)\n    pair = None\n    if gini(G) != 0:\n        numOffeatures = G.shape[1] - 1\n        for k in range(numOffeatures):\n            for t in range(m):\n                Gl = G[G[:, k] <= G[t, k]]\n                Gr = G[G[:, k] > G[t, k]]\n                gl = gini(Gl)\n                gr = gini(Gr)\n                ml = Gl.shape[0]\n                mr = Gr.shape[0]\n                g = gl*ml/m + gr*mr/m\n                if g < gmini:\n                    gmini = g\n                    pair = (k, G[t, k])\n                    Glm = Gl\n                    Grm = Gr\n        res = {'split': True,\n               'pair': pair,\n               'sets': (Glm, Grm)}\n    else:\n        res = {'split': False,\n               'pair': pair,\n               'sets': G}\n    return res\n```\n:::\n\n\nFor the purpose of counting labels, we also write a code to do so.\n\n::: {#a693d603 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\ndef countlabels(S):\n    y = S[:, -1].reshape(S.shape[0])\n    labelCount = dict(pd.Series(y).value_counts())\n    return labelCount\n```\n:::\n\n\n</details>\n\n\n## Decision Tree Project 1: the `iris` dataset\n\nWe are going to use the Decision Tree model to study the `iris` dataset. This dataset has already studied previously using k-NN. Again we will only use the first two features for visualization purpose.\n\n### Initial setup\n\nSince the dataset will be splitted, we will put `X` and `y` together as a single variable `S`. In this case when we split the dataset by selecting rows, the features and the labels are still paired correctly. \n\nWe also print the labels and the feature names for our convenience.\n\n::: {#d980e9f3 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\niris = load_iris()\nX = iris.data[:, 2:]\ny = iris.target\ny = y.reshape((y.shape[0],1))\nS = np.concatenate([X,y], axis=1)\n\nprint(iris.target_names)\nprint(iris.feature_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['setosa' 'versicolor' 'virginica']\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n```\n:::\n:::\n\n\n### Use package `sklearn`\n\nNow we would like to use the decision tree package provided by `sklearn`. The process is straightforward. The parameter `random_state=40` will be discussed {ref}`later<note-random_state>`, and it is not necessary in most cases.\n\n::: {#8cdd6647 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(random_state=40, min_samples_split=2)\nclf.fit(X, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(random_state=40)</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n`sklearn` provide a way to automatically generate the tree view of the decision tree. The code is as follows. \n\n::: {#3c5679fb .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(3, 3), dpi=200)\nplot_tree(clf, filled=True, impurity=True, node_ids=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[Text(0.5, 0.9166666666666666, 'node #0\\nx[1] <= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n Text(0.4090909090909091, 0.75, 'node #1\\ngini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n Text(0.4545454545454546, 0.8333333333333333, 'True  '),\n Text(0.5909090909090909, 0.75, 'node #2\\nx[1] <= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n Text(0.5454545454545454, 0.8333333333333333, '  False'),\n Text(0.36363636363636365, 0.5833333333333334, 'node #3\\nx[0] <= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n Text(0.18181818181818182, 0.4166666666666667, 'node #4\\nx[1] <= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'),\n Text(0.09090909090909091, 0.25, 'node #5\\ngini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'),\n Text(0.2727272727272727, 0.25, 'node #6\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.5454545454545454, 0.4166666666666667, 'node #7\\nx[1] <= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'),\n Text(0.45454545454545453, 0.25, 'node #8\\ngini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'),\n Text(0.6363636363636364, 0.25, 'node #9\\nx[0] <= 5.45\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'),\n Text(0.5454545454545454, 0.08333333333333333, 'node #10\\ngini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'),\n Text(0.7272727272727273, 0.08333333333333333, 'node #11\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.8181818181818182, 0.5833333333333334, 'node #12\\nx[0] <= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'),\n Text(0.7272727272727273, 0.4166666666666667, 'node #13\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'),\n Text(0.9090909090909091, 0.4166666666666667, 'node #14\\ngini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-7-output-2.png){width=505 height=502}\n:::\n:::\n\n\n<details>\n<summary>Visualize decision boundary is optional.</summary>\n\n\nSimilar to k-NN, we may use `sklearn.inspection.DecisionBoundaryDisplay` to visualize the decision boundary of this decision tree.\n\n::: {#a344903f .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.inspection import DecisionBoundaryDisplay\nDecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    cmap='coolwarm',\n    response_method=\"predict\",\n    xlabel=iris.feature_names[0],\n    ylabel=iris.feature_names[1],\n)\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=15)\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-8-output-1.png){width=600 height=434}\n:::\n:::\n\n\n</details>\n\n\n### Tuning hyperparameters\n\n\nBuilding a decision tree is the same as splitting the training dataset. If we are alllowed to keep splitting it, it is possible to get to a case that each end node is pure: the Gini impurity is 0. This means two things:\n\n1. The tree learns too many (unnecessary) detailed info from the training set which might not be applied to the test set. This is called *overfitting*. We should prevent a model from overfitting.\n2. We could use the number of split to indicate the progress of the learning.\n\nIn other words, the finer the splits are, the further the learning is. We need to consider some early stop conditions to prevent the model from overfitting.\n\nSome possible early stop conditions:\n\n- `max_depth`: The max depth of the tree. The default is `none` which means no limits.\n- `min_samples_split`: The minimum number of samples required to split an internal node. The default is 2 which means that we will split a node with as few as 2 elements if needed.\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. The default is 1 which means that we will still split the node even if one subset only contains 1 element.\n\nIf we don't set them (which means that we use the default setting), the dataset will be split untill all subsets are pure.\n\n\n\n::: {#exm-}\n# ALMOST all end nodes are pure\n\nIn this example, we let the tree grow as further as possible. It only stops when (almost) all end nodes are pure, even if the end nodes only contain ONE element (like #6 and #11).\n\n::: {#fe4dd5b8 .cell execution_count=8}\n``` {.python .cell-code}\nclf = DecisionTreeClassifier(random_state=40)\nclf.fit(X, y)\nplt.figure(figsize=(3, 3), dpi=200)\nplot_tree(clf, filled=True, impurity=True, node_ids=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n[Text(0.5, 0.9166666666666666, 'node #0\\nx[1] <= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]'),\n Text(0.4090909090909091, 0.75, 'node #1\\ngini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]'),\n Text(0.4545454545454546, 0.8333333333333333, 'True  '),\n Text(0.5909090909090909, 0.75, 'node #2\\nx[1] <= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]'),\n Text(0.5454545454545454, 0.8333333333333333, '  False'),\n Text(0.36363636363636365, 0.5833333333333334, 'node #3\\nx[0] <= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]'),\n Text(0.18181818181818182, 0.4166666666666667, 'node #4\\nx[1] <= 1.65\\ngini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]'),\n Text(0.09090909090909091, 0.25, 'node #5\\ngini = 0.0\\nsamples = 47\\nvalue = [0, 47, 0]'),\n Text(0.2727272727272727, 0.25, 'node #6\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.5454545454545454, 0.4166666666666667, 'node #7\\nx[1] <= 1.55\\ngini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]'),\n Text(0.45454545454545453, 0.25, 'node #8\\ngini = 0.0\\nsamples = 3\\nvalue = [0, 0, 3]'),\n Text(0.6363636363636364, 0.25, 'node #9\\nx[0] <= 5.45\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 2, 1]'),\n Text(0.5454545454545454, 0.08333333333333333, 'node #10\\ngini = 0.0\\nsamples = 2\\nvalue = [0, 2, 0]'),\n Text(0.7272727272727273, 0.08333333333333333, 'node #11\\ngini = 0.0\\nsamples = 1\\nvalue = [0, 0, 1]'),\n Text(0.8181818181818182, 0.5833333333333334, 'node #12\\nx[0] <= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]'),\n Text(0.7272727272727273, 0.4166666666666667, 'node #13\\ngini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]'),\n Text(0.9090909090909091, 0.4166666666666667, 'node #14\\ngini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]')]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-9-output-2.png){width=505 height=502}\n:::\n:::\n\n\nIn this example there is one exception. #13 node is not pure. The reason is as follows.\n\n::: {#736c8ee3 .cell execution_count=9}\n``` {.python .cell-code}\nX[(X[:, 0]>0.8) & (X[:, 1] >1.75) & (X[:, 0]<=4.85)]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([[4.8, 1.8],\n       [4.8, 1.8],\n       [4.8, 1.8]])\n```\n:::\n:::\n\n\nAll the data points in this node has the same feature while their labels are different. They cannot be split further purely based on features.\n\n\n:::\n\n\nTherefore we could treat these hyperparameters as an indicator about how far the dataset is split. The further the dataset is split, the further the learning goes, the more details are learned. Since we don't want the model to be overfitting, we don't want to split the dataset that far. In this case, we could use the learning curve to help us make the decision.\n\nIn this example, let us choose `min_samples_leaf` as the indicator. When `min_samples_leaf` is big, the learning just starts. When `min_samples_leaf=1`, we reach the far most side of learning. We will see how the training and testing accuracy changes along the learning process.\n\n::: {#32e28628 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\nnum_leaf = list(range(1, 100))\ntrain_acc = []\ntest_acc = []\nfor i in num_leaf:\n    clf = DecisionTreeClassifier(random_state=42, min_samples_leaf=i)\n    clf.fit(X_train, y_train)\n    train_acc.append(clf.score(X_train, y_train))\n    test_acc.append(clf.score(X_test, y_test))\nplt.plot(num_leaf, train_acc, label='training')\nplt.plot(num_leaf, test_acc, label='testing')\nplt.gca().set_xlim((max(num_leaf)+1, min(num_leaf)-1))\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-11-output-1.png){width=575 height=411}\n:::\n:::\n\n\nFrom this plot, the accuracy has a big jump at `min_sample_leaf=41`. So we could choose this to be our hyperparameter.\n\n\n### Apply CART manually (optional)\n\n\n<details>\n<summary>The manual sample code is optional.</summary>\n\n\nWe apply `split` to the dataset `S`. \n\n::: {#7cdc2ab4 .cell execution_count=11}\n``` {.python .cell-code}\nfrom assests.codes.dt import gini, split, countlabels\nr = split(S)\nif r['split'] is True:\n    Gl, Gr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Gl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Gr)),\n          ' and its label counts is {d}'.format(d=countlabels(Gr)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(0, 1.9)\nThe left subset's Gini impurity is 0.00,  and its label counts is {0.0: 50}\nThe right subset's Gini impurity is 0.50,  and its label counts is {1.0: 50, 2.0: 50}\n```\n:::\n:::\n\n\nThe results shows that `S` is splitted into two subsets based on the `0`-th feature and the split value is `1.9`. \n\nThe left subset is already pure since its Gini impurity is `0`. All elements in the left subset is label `0` (which is `setosa`). The right one is mixed since its Gini impurity is `0.5`. Therefore we need to apply `split` again to the right subset.\n\n::: {#b1518359 .cell execution_count=12}\n``` {.python .cell-code}\nr = split(Gr)\nif r['split'] is True:\n    Grl, Grr = r['sets']\n    print(r['pair'])\n    print('The left subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grl)),\n          ' and its label counts is {d:}'.format(d=countlabels(Grl)))\n    print('The right subset\\'s Gini impurity is {g:.2f},'.format(g=gini(Grr)),\n          ' and its label counts is {d}'.format(d=countlabels(Grr)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1, 1.7)\nThe left subset's Gini impurity is 0.17,  and its label counts is {1.0: 49, 2.0: 5}\nThe right subset's Gini impurity is 0.04,  and its label counts is {2.0: 45, 1.0: 1}\n```\n:::\n:::\n\n\nThis time the subset is splitted into two more subsets based on the `1`-st feature and the split value is `1.7`. The total Gini impurity is minimized using this split. \n\nThe decision we created so far can be described as follows:\n\n1. Check the first feature `sepal length (cm)` to see whether it is smaller or equal to `1.9`.\n   1. If it is, classify it as lable `0` which is `setosa`.\n   2. If not, continue to the next stage.\n2. Check the second feature `sepal width (cm)` to see whether it is smaller or equal to `1.7`. \n   1. If it is, classify it as label `1` which is `versicolor`.\n   2. If not, classify it as label `2` which is `virginica`.\n\n\n#### Analyze the differences between the two methods\nThe tree generated by `sklearn` and the tree we got manually is a little bit different. Let us explore the differences here. \n\nTo make it easier to split the set, we could convert the `numpy.ndarray` to `pandas.DataFrame`.\n\n::: {#2b3767d1 .cell execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.DataFrame(X)\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow based on our tree, we would like to get all data points that the first feature (which is marked as `0`) is smaller or equal to `1.9`. We save it as `df1`. Similarly based on the tree gotten from `sklearn`, we would like to get all data points taht the second feature (which is marked as `1`) is smaller or equal to `0.8` and save it to `df2`. \n\n::: {#3df0c2fb .cell execution_count=14}\n``` {.python .cell-code}\ndf1 = df[df[0]<=1.9]\ndf2 = df[df[1]<=0.8]\n```\n:::\n\n\nThen we would like to compare these two dataframes. What we want is to see whether they are the same regardless the order. One way to do this is to sort the two dataframes and then compare them directly.\n\nTo sort the dataframe we use the method `DataFrame.sort_values`. The details can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html). Note that after `sort_values` we apply `reset_index` to reset the index just in case the index is massed by the sort operation.\n\nThen we use `DataFrame.equals` to check whether they are the same.\n\n::: {#bc8e7e51 .cell execution_count=15}\n``` {.python .cell-code}\ndf1sorted = df1.sort_values(by=df1.columns.tolist()).reset_index(drop=True)\ndf2sorted = df2.sort_values(by=df2.columns.tolist()).reset_index(drop=True)\nprint(df1sorted.equals(df2sorted))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n:::\n\n\nSo these two sets are really the same. The reason this happens can be seen from the following two graphs.\n\n\n:::{layout-ncol=2}\n\n\n![From our code](assests/img/20220809120531.png)\n\n![From `sklearn`](assests/img/20220809122643.png)\n\n:::\n\n\n\nSo you can see that either way can give us the same classification. This means that given one dataset the construction of the decision tree might be random at some points.\n\n::: {.callout-note}\n# note-random_state\nSince the split is random, when using `sklearn.DecisionTreeClassifier` to construct decision trees, sometimes we might get the same tree as what we get from our naive codes. \n\nTo illustrate this phenomenaon I need to set the random state in case it will generate the same tree as ours when I need it to generate a different tree. The parameter `random_state=40` mentioned before is for this purpose.\n:::\n\n\n\nAnother difference is the split value of the second branch. In our case it is `1.7` and in `sklearn` case it is `1.75`. So after we get the right subset from the first split (which is called `dfr`), we would split it into two sets based on whether the second feature is above or below `1.7`.\n\n::: {#bc50104e .cell execution_count=16}\n``` {.python .cell-code}\ndfr = df[df[0]>1.9]\ndf2a = dfr[dfr[1]>1.7]\ndf2b = dfr[dfr[1]<=1.7]\nprint(df2b[1].max())\nprint(df2a[1].min())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.7\n1.8\n```\n:::\n:::\n\n\nNow you can see where the split number comes from. In our code, when we found a split, we will directly use that number as the cut. In this case it is `1.7`. \n\nIn `sklearn`, when it finds a split, the algorithm will go for the middle of the gap as the cut. In this case it is `(1.7+1.8)/2=1.75`. \n\n</details>\n\n\n### Pruning\nOther than tuning hyperparameters, we also want to prune our tree in order to reduce overfitting further. The pruning algorithm introduced together with CART is called **Minimal-Cost-Complexity Pruning** (MCCP). \n\nThe idea is to use the cost-complexity measure to replace Gini impurity to grow the tree. The cost-complexity measure is gotten by adding the total number of end nodes with the coefficent `ccp_alpha` to the Gini impurity. The parameter `ccp_alpha>=0` is known as the **complexity parameter**, and it can help the tree to self-balance the number of end nodes and the Gini impurity.\n\n\nIn `sklearn`, we use `cost_complexity_pruning_path` to help us find the effective alphas and the corresponding impurities. \n\n::: {#11cda53c .cell execution_count=17}\n``` {.python .cell-code}\nclf = DecisionTreeClassifier(random_state=42)\nccp_path = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = ccp_path.ccp_alphas, ccp_path.impurities\n```\n:::\n\n\nThe list `ccp_alphas` contains all important `ccp_alpha`s. We could train a model for each of them. We could use the argument `ccp_alpha` to control the value when initialize the model. After we train all these tree models for each effective `ccp_alpha`, we evaluate the model and record the results.\n\n::: {#d4107166 .cell execution_count=18}\n``` {.python .cell-code}\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\ntrain_acc = [clf.score(X_train, y_train) for clf in clfs]\ntest_acc = [clf.score(X_test, y_test) for clf in clfs]\n```\n:::\n\n\nWe would like to visualize these info.\n\n::: {#ebf7d165 .cell execution_count=19}\n``` {.python .cell-code}\nfig, ax = plt.subplots(3, 1)\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[2].plot(ccp_alphas, train_acc, marker=\"o\", drawstyle=\"steps-post\", label='train')\nax[2].plot(ccp_alphas, test_acc, marker=\"o\", drawstyle=\"steps-post\", label='test')\nax[2].set_xlabel(\"alpha\")\nax[2].set_ylabel(\"accuracy\")\nax[2].legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-20-output-1.png){width=597 height=430}\n:::\n:::\n\n\nBased on the plot, it seems that the fourth dot 0.01534424 is a good fit. So this is the complexity parameter we are going to use.\n\n\n\n<details>\n<summary>We could combine all the code above into a function.</summary>\n\n::: {#f3a9dd95 .cell execution_count=20}\n``` {.python .cell-code}\nfrom sklearn.base import clone\ndef eval_ccp_alphas(clf, X, y):\n    ccp_path = clf.cost_complexity_pruning_path(X, y)\n    ccp_alphas, impurities = ccp_path.ccp_alphas, ccp_path.impurities\n    clfs = []\n    for ccp_alpha in ccp_alphas:\n        clf_tmp = clone(clf)\n        clf_tmp.set_params(ccp_alpha=ccp_alpha)\n        clf_tmp.fit(X, y)\n        clfs.append(clf_tmp)\n\n    node_counts = [clf.tree_.node_count for clf in clfs]\n    depth = [clf.tree_.max_depth for clf in clfs]\n    train_acc = [clf.score(X_train, y_train) for clf in clfs]\n    test_acc = [clf.score(X_test, y_test) for clf in clfs]\n\n    fig, ax = plt.subplots(3, 1)\n    ax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\n    ax[0].set_xlabel(\"alpha\")\n    ax[0].set_ylabel(\"number of nodes\")\n    ax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\n    ax[1].set_xlabel(\"alpha\")\n    ax[1].set_ylabel(\"depth of tree\")\n    ax[2].plot(ccp_alphas, train_acc, marker=\"o\", drawstyle=\"steps-post\", label='train')\n    ax[2].plot(ccp_alphas, test_acc, marker=\"o\", drawstyle=\"steps-post\", label='test')\n    ax[2].set_xlabel(\"alpha\")\n    ax[2].set_ylabel(\"accuracy\")\n    ax[2].legend()\n\n    return ccp_alphas\n```\n:::\n\n\n::: {#6ccc85d6 .cell execution_count=21}\n``` {.python .cell-code}\neval_ccp_alphas(DecisionTreeClassifier(random_state=42), X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\narray([0.        , 0.00485564, 0.01049869, 0.01534424, 0.03364964,\n       0.24888323, 0.33214852])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-22-output-2.png){width=597 height=430}\n:::\n:::\n\n\n</details>\n\n\n\n\n\n\n\n<!-- {{< include dt2.qmd >}} -->\n\n\n\n\n## Exercises and Projects\n\n\n::: {#exr-}\nThe dataset and its scattering plot is given below.\n\n1. Please calculate the Gini impurity of the whole set by hand.\n2. Please apply CART to create the decision tree by hand. \n3. Please use the tree you created to classify the following points:\n    - $(0.4, 1.0)$\n    - $(0.6, 1.0)$\n    - $(0.6, 0)$\n\nThe following code is for ploting. You may also get the precise data points by reading the code. You don't need to write codes to solve the problem.\n\n::: {#f0c8d457 .cell execution_count=22}\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-html/cell-23-output-1.png){width=581 height=564}\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n::: {#exr-}\nCHOOSE ONE: Please apply the Decision Tree to one of the following datasets. \n\n- dating dataset (in Chpater 2). \n- the `titanic` dataset.\n\nPlease answer the following questions.\n\n1. Please find the best hyperparameters of your choice.\n2. Please record the accuracy (or cross-validation score) of your model and compare it with the models you learned before (kNN). \n3. Please find the two most important features and explane your reason.\n:::\n\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}